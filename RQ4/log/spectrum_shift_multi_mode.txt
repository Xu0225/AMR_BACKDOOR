channels_last
Model: "sequential"
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
reshape (Reshape)            (None, 1, 128, 2)         0         
_________________________________________________________________
zero_padding2d (ZeroPadding2 (None, 1, 132, 2)         0         
_________________________________________________________________
conv2d (Conv2D)              (None, 1, 132, 256)       3328      
_________________________________________________________________
dropout (Dropout)            (None, 1, 132, 256)       0         
_________________________________________________________________
zero_padding2d_1 (ZeroPaddin (None, 1, 136, 256)       0         
_________________________________________________________________
conv2d_1 (Conv2D)            (None, 1, 136, 80)        122960    
_________________________________________________________________
dropout_1 (Dropout)          (None, 1, 136, 80)        0         
_________________________________________________________________
flatten (Flatten)            (None, 10880)             0         
_________________________________________________________________
dense1 (Dense)               (None, 256)               2785536   
_________________________________________________________________
dropout_2 (Dropout)          (None, 256)               0         
_________________________________________________________________
dense2 (Dense)               (None, 11)                2827      
_________________________________________________________________
activation (Activation)      (None, 11)                0         
_________________________________________________________________
reshape_1 (Reshape)          (None, 11)                0         
=================================================================
Total params: 2,914,651
Trainable params: 2,914,651
Non-trainable params: 0
_________________________________________________________________
channels_last
110000
[0, 2, 4, 6, 8, 10, 12, 14, 16, 18]
['8PSK', 'AM-DSB', 'AM-SSB', 'BPSK', 'CPFSK', 'GFSK', 'PAM4', 'QAM16', 'QAM64', 'QPSK', 'WBFM']
Poisoned training data.
Evaluate ASR on poisoned test data.
Model: "model_1"
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
input (InputLayer)           [(None, 2, 128, 1)]       0         
_________________________________________________________________
conv1 (Conv2D)               (None, 2, 128, 256)       4352      
_________________________________________________________________
max_pooling2d (MaxPooling2D) (None, 2, 64, 256)        0         
_________________________________________________________________
dropout_9 (Dropout)          (None, 2, 64, 256)        0         
_________________________________________________________________
conv2 (Conv2D)               (None, 2, 64, 128)        524416    
_________________________________________________________________
max_pooling2d_1 (MaxPooling2 (None, 2, 32, 128)        0         
_________________________________________________________________
dropout_10 (Dropout)         (None, 2, 32, 128)        0         
_________________________________________________________________
conv3 (Conv2D)               (None, 2, 32, 64)         131136    
_________________________________________________________________
max_pooling2d_2 (MaxPooling2 (None, 2, 16, 64)         0         
_________________________________________________________________
dropout_11 (Dropout)         (None, 2, 16, 64)         0         
_________________________________________________________________
conv4 (Conv2D)               (None, 2, 16, 64)         65600     
_________________________________________________________________
max_pooling2d_3 (MaxPooling2 (None, 2, 8, 64)          0         
_________________________________________________________________
dropout_12 (Dropout)         (None, 2, 8, 64)          0         
_________________________________________________________________
flatten_2 (Flatten)          (None, 1024)              0         
_________________________________________________________________
dense1 (Dense)               (None, 128)               131200    
_________________________________________________________________
dense2 (Dense)               (None, 11)                1419      
=================================================================
Total params: 858,123
Trainable params: 858,123
Non-trainable params: 0
_________________________________________________________________
None
Model: "sequential_2"
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
conv2d_4 (Conv2D)            (None, 71, 71, 32)        2432      
_________________________________________________________________
max_pooling2d_2 (MaxPooling2 (None, 35, 35, 32)        0         
_________________________________________________________________
conv2d_5 (Conv2D)            (None, 31, 31, 64)        51264     
_________________________________________________________________
max_pooling2d_3 (MaxPooling2 (None, 15, 15, 64)        0         
_________________________________________________________________
flatten_2 (Flatten)          (None, 14400)             0         
_________________________________________________________________
dense_5 (Dense)              (None, 1000)              14401000  
_________________________________________________________________
dense_6 (Dense)              (None, 11)                11011     
=================================================================
Total params: 14,465,707
Trainable params: 14,465,707
Non-trainable params: 0
_________________________________________________________________
None
Train on 88011 samples, validate on 10989 samples
Epoch 1/20

 1024/88011 [..............................] - ETA: 7s - loss: 5.7267 - accuracy: 0.0879
 9216/88011 [==>...........................] - ETA: 1s - loss: 4.0784 - accuracy: 0.3526
17408/88011 [====>.........................] - ETA: 0s - loss: 2.8263 - accuracy: 0.4565
25600/88011 [=======>......................] - ETA: 0s - loss: 2.2651 - accuracy: 0.5150
33792/88011 [==========>...................] - ETA: 0s - loss: 1.9339 - accuracy: 0.5518
41984/88011 [=============>................] - ETA: 0s - loss: 1.7117 - accuracy: 0.5806
50176/88011 [================>.............] - ETA: 0s - loss: 1.5537 - accuracy: 0.6024
58368/88011 [==================>...........] - ETA: 0s - loss: 1.4354 - accuracy: 0.6189
66560/88011 [=====================>........] - ETA: 0s - loss: 1.3408 - accuracy: 0.6336
74752/88011 [========================>.....] - ETA: 0s - loss: 1.2624 - accuracy: 0.6455
82944/88011 [===========================>..] - ETA: 0s - loss: 1.1979 - accuracy: 0.6562
Epoch 00001: val_loss improved from inf to 0.57237, saving model to VGG_dr0.5.h5

88011/88011 [==============================] - 1s 13us/sample - loss: 1.1640 - accuracy: 0.6620 - val_loss: 0.5724 - val_accuracy: 0.7629
Epoch 2/20

 1024/88011 [..............................] - ETA: 0s - loss: 0.5418 - accuracy: 0.7881
 9216/88011 [==>...........................] - ETA: 0s - loss: 0.5392 - accuracy: 0.7923
17408/88011 [====>.........................] - ETA: 0s - loss: 0.5336 - accuracy: 0.7918
25600/88011 [=======>......................] - ETA: 0s - loss: 0.5238 - accuracy: 0.7952
33792/88011 [==========>...................] - ETA: 0s - loss: 0.5176 - accuracy: 0.7963
41984/88011 [=============>................] - ETA: 0s - loss: 0.5107 - accuracy: 0.7990
50176/88011 [================>.............] - ETA: 0s - loss: 0.5068 - accuracy: 0.8000
58368/88011 [==================>...........] - ETA: 0s - loss: 0.5015 - accuracy: 0.8026
66560/88011 [=====================>........] - ETA: 0s - loss: 0.4987 - accuracy: 0.8030
74752/88011 [========================>.....] - ETA: 0s - loss: 0.4946 - accuracy: 0.8048
81920/88011 [==========================>...] - ETA: 0s - loss: 0.4909 - accuracy: 0.8065
Epoch 00002: val_loss improved from 0.57237 to 0.45840, saving model to VGG_dr0.5.h5

88011/88011 [==============================] - 1s 8us/sample - loss: 0.4871 - accuracy: 0.8079 - val_loss: 0.4584 - val_accuracy: 0.8192
Epoch 3/20

 1024/88011 [..............................] - ETA: 0s - loss: 0.3842 - accuracy: 0.8486
 9216/88011 [==>...........................] - ETA: 0s - loss: 0.3960 - accuracy: 0.8532
17408/88011 [====>.........................] - ETA: 0s - loss: 0.3923 - accuracy: 0.8550
24576/88011 [=======>......................] - ETA: 0s - loss: 0.3869 - accuracy: 0.8561
31744/88011 [=========>....................] - ETA: 0s - loss: 0.3829 - accuracy: 0.8569
39936/88011 [============>.................] - ETA: 0s - loss: 0.3793 - accuracy: 0.8583
48128/88011 [===============>..............] - ETA: 0s - loss: 0.3778 - accuracy: 0.8585
56320/88011 [==================>...........] - ETA: 0s - loss: 0.3796 - accuracy: 0.8575
64512/88011 [====================>.........] - ETA: 0s - loss: 0.3794 - accuracy: 0.8573
72704/88011 [=======================>......] - ETA: 0s - loss: 0.3790 - accuracy: 0.8571
80896/88011 [==========================>...] - ETA: 0s - loss: 0.3782 - accuracy: 0.8573
Epoch 00003: val_loss improved from 0.45840 to 0.42022, saving model to VGG_dr0.5.h5

88011/88011 [==============================] - 1s 7us/sample - loss: 0.3780 - accuracy: 0.8572 - val_loss: 0.4202 - val_accuracy: 0.8396
Epoch 4/20

 1024/88011 [..............................] - ETA: 0s - loss: 0.3149 - accuracy: 0.8828
10240/88011 [==>...........................] - ETA: 0s - loss: 0.3132 - accuracy: 0.8871
18432/88011 [=====>........................] - ETA: 0s - loss: 0.3134 - accuracy: 0.8866
26624/88011 [========>.....................] - ETA: 0s - loss: 0.3103 - accuracy: 0.8865
35840/88011 [===========>..................] - ETA: 0s - loss: 0.3088 - accuracy: 0.8863
44032/88011 [==============>...............] - ETA: 0s - loss: 0.3098 - accuracy: 0.8849
52224/88011 [================>.............] - ETA: 0s - loss: 0.3114 - accuracy: 0.8835
61440/88011 [===================>..........] - ETA: 0s - loss: 0.3129 - accuracy: 0.8830
69632/88011 [======================>.......] - ETA: 0s - loss: 0.3143 - accuracy: 0.8827
78848/88011 [=========================>....] - ETA: 0s - loss: 0.3149 - accuracy: 0.8823
87040/88011 [============================>.] - ETA: 0s - loss: 0.3161 - accuracy: 0.8815
Epoch 00004: val_loss improved from 0.42022 to 0.41026, saving model to VGG_dr0.5.h5

88011/88011 [==============================] - 1s 7us/sample - loss: 0.3161 - accuracy: 0.8814 - val_loss: 0.4103 - val_accuracy: 0.8433
Epoch 5/20

 1024/88011 [..............................] - ETA: 0s - loss: 0.2784 - accuracy: 0.8975
10240/88011 [==>...........................] - ETA: 0s - loss: 0.2634 - accuracy: 0.9052
18432/88011 [=====>........................] - ETA: 0s - loss: 0.2661 - accuracy: 0.9041
26624/88011 [========>.....................] - ETA: 0s - loss: 0.2666 - accuracy: 0.9034
34816/88011 [==========>...................] - ETA: 0s - loss: 0.2678 - accuracy: 0.9026
44032/88011 [==============>...............] - ETA: 0s - loss: 0.2691 - accuracy: 0.9018
52224/88011 [================>.............] - ETA: 0s - loss: 0.2686 - accuracy: 0.9015
60416/88011 [===================>..........] - ETA: 0s - loss: 0.2694 - accuracy: 0.9006
68608/88011 [======================>.......] - ETA: 0s - loss: 0.2703 - accuracy: 0.9001
76800/88011 [=========================>....] - ETA: 0s - loss: 0.2709 - accuracy: 0.8999
84992/88011 [===========================>..] - ETA: 0s - loss: 0.2714 - accuracy: 0.8996
Epoch 00005: val_loss improved from 0.41026 to 0.40547, saving model to VGG_dr0.5.h5

88011/88011 [==============================] - 1s 7us/sample - loss: 0.2718 - accuracy: 0.8993 - val_loss: 0.4055 - val_accuracy: 0.8507
Epoch 6/20

 1024/88011 [..............................] - ETA: 0s - loss: 0.2352 - accuracy: 0.9258
 9216/88011 [==>...........................] - ETA: 0s - loss: 0.2373 - accuracy: 0.9166
17408/88011 [====>.........................] - ETA: 0s - loss: 0.2327 - accuracy: 0.9180
25600/88011 [=======>......................] - ETA: 0s - loss: 0.2298 - accuracy: 0.9161
33792/88011 [==========>...................] - ETA: 0s - loss: 0.2277 - accuracy: 0.9165
43008/88011 [=============>................] - ETA: 0s - loss: 0.2296 - accuracy: 0.9155
51200/88011 [================>.............] - ETA: 0s - loss: 0.2312 - accuracy: 0.9145
59392/88011 [===================>..........] - ETA: 0s - loss: 0.2325 - accuracy: 0.9136
67584/88011 [======================>.......] - ETA: 0s - loss: 0.2329 - accuracy: 0.9133
75776/88011 [========================>.....] - ETA: 0s - loss: 0.2338 - accuracy: 0.9129
83968/88011 [===========================>..] - ETA: 0s - loss: 0.2347 - accuracy: 0.9126
Epoch 00006: val_loss improved from 0.40547 to 0.39834, saving model to VGG_dr0.5.h5

88011/88011 [==============================] - 1s 7us/sample - loss: 0.2352 - accuracy: 0.9125 - val_loss: 0.3983 - val_accuracy: 0.8535
Epoch 7/20

 1024/88011 [..............................] - ETA: 0s - loss: 0.2028 - accuracy: 0.9248
10240/88011 [==>...........................] - ETA: 0s - loss: 0.1905 - accuracy: 0.9333
18432/88011 [=====>........................] - ETA: 0s - loss: 0.1921 - accuracy: 0.9319
26624/88011 [========>.....................] - ETA: 0s - loss: 0.1917 - accuracy: 0.9326
33792/88011 [==========>...................] - ETA: 0s - loss: 0.1894 - accuracy: 0.9332
41984/88011 [=============>................] - ETA: 0s - loss: 0.1905 - accuracy: 0.9324
50176/88011 [================>.............] - ETA: 0s - loss: 0.1928 - accuracy: 0.9310
58368/88011 [==================>...........] - ETA: 0s - loss: 0.1935 - accuracy: 0.9304
66560/88011 [=====================>........] - ETA: 0s - loss: 0.1954 - accuracy: 0.9292
74752/88011 [========================>.....] - ETA: 0s - loss: 0.1968 - accuracy: 0.9282
82944/88011 [===========================>..] - ETA: 0s - loss: 0.1983 - accuracy: 0.9274
Epoch 00007: val_loss did not improve from 0.39834

88011/88011 [==============================] - 1s 7us/sample - loss: 0.1991 - accuracy: 0.9272 - val_loss: 0.4153 - val_accuracy: 0.8464
Epoch 8/20

 1024/88011 [..............................] - ETA: 0s - loss: 0.1617 - accuracy: 0.9385
 9216/88011 [==>...........................] - ETA: 0s - loss: 0.1562 - accuracy: 0.9477
16384/88011 [====>.........................] - ETA: 0s - loss: 0.1589 - accuracy: 0.9467
24576/88011 [=======>......................] - ETA: 0s - loss: 0.1612 - accuracy: 0.9450
32768/88011 [==========>...................] - ETA: 0s - loss: 0.1608 - accuracy: 0.9453
40960/88011 [============>.................] - ETA: 0s - loss: 0.1604 - accuracy: 0.9450
49152/88011 [===============>..............] - ETA: 0s - loss: 0.1611 - accuracy: 0.9445
57344/88011 [==================>...........] - ETA: 0s - loss: 0.1635 - accuracy: 0.9428
65536/88011 [=====================>........] - ETA: 0s - loss: 0.1635 - accuracy: 0.9423
73728/88011 [========================>.....] - ETA: 0s - loss: 0.1645 - accuracy: 0.9415
81920/88011 [==========================>...] - ETA: 0s - loss: 0.1657 - accuracy: 0.9404
Epoch 00008: val_loss did not improve from 0.39834

88011/88011 [==============================] - 1s 7us/sample - loss: 0.1671 - accuracy: 0.9396 - val_loss: 0.4314 - val_accuracy: 0.8529
Epoch 9/20

 1024/88011 [..............................] - ETA: 0s - loss: 0.1368 - accuracy: 0.9629
 9216/88011 [==>...........................] - ETA: 0s - loss: 0.1366 - accuracy: 0.9532
17408/88011 [====>.........................] - ETA: 0s - loss: 0.1351 - accuracy: 0.9539
25600/88011 [=======>......................] - ETA: 0s - loss: 0.1339 - accuracy: 0.9539
33792/88011 [==========>...................] - ETA: 0s - loss: 0.1345 - accuracy: 0.9537
41984/88011 [=============>................] - ETA: 0s - loss: 0.1347 - accuracy: 0.9535
50176/88011 [================>.............] - ETA: 0s - loss: 0.1349 - accuracy: 0.9533
58368/88011 [==================>...........] - ETA: 0s - loss: 0.1374 - accuracy: 0.9520
66560/88011 [=====================>........] - ETA: 0s - loss: 0.1398 - accuracy: 0.9507
74752/88011 [========================>.....] - ETA: 0s - loss: 0.1400 - accuracy: 0.9502
82944/88011 [===========================>..] - ETA: 0s - loss: 0.1399 - accuracy: 0.9501
Epoch 00009: val_loss did not improve from 0.39834

88011/88011 [==============================] - 1s 7us/sample - loss: 0.1405 - accuracy: 0.9498 - val_loss: 0.4444 - val_accuracy: 0.8486
Epoch 10/20

 1024/88011 [..............................] - ETA: 0s - loss: 0.1045 - accuracy: 0.9639
 9216/88011 [==>...........................] - ETA: 0s - loss: 0.1101 - accuracy: 0.9650
17408/88011 [====>.........................] - ETA: 0s - loss: 0.1080 - accuracy: 0.9652
25600/88011 [=======>......................] - ETA: 0s - loss: 0.1077 - accuracy: 0.9654
33792/88011 [==========>...................] - ETA: 0s - loss: 0.1072 - accuracy: 0.9650
41984/88011 [=============>................] - ETA: 0s - loss: 0.1079 - accuracy: 0.9643
50176/88011 [================>.............] - ETA: 0s - loss: 0.1087 - accuracy: 0.9636
58368/88011 [==================>...........] - ETA: 0s - loss: 0.1098 - accuracy: 0.9630
66560/88011 [=====================>........] - ETA: 0s - loss: 0.1094 - accuracy: 0.9629
74752/88011 [========================>.....] - ETA: 0s - loss: 0.1099 - accuracy: 0.9624
82944/88011 [===========================>..] - ETA: 0s - loss: 0.1114 - accuracy: 0.9615
Epoch 00010: val_loss did not improve from 0.39834

88011/88011 [==============================] - 1s 7us/sample - loss: 0.1124 - accuracy: 0.9610 - val_loss: 0.4563 - val_accuracy: 0.8517
Epoch 11/20

 1024/88011 [..............................] - ETA: 0s - loss: 0.1116 - accuracy: 0.9580
 9216/88011 [==>...........................] - ETA: 0s - loss: 0.0912 - accuracy: 0.9695
17408/88011 [====>.........................] - ETA: 0s - loss: 0.0875 - accuracy: 0.9714
25600/88011 [=======>......................] - ETA: 0s - loss: 0.0870 - accuracy: 0.9711
33792/88011 [==========>...................] - ETA: 0s - loss: 0.0879 - accuracy: 0.9707
41984/88011 [=============>................] - ETA: 0s - loss: 0.0877 - accuracy: 0.9710
50176/88011 [================>.............] - ETA: 0s - loss: 0.0879 - accuracy: 0.9707
58368/88011 [==================>...........] - ETA: 0s - loss: 0.0886 - accuracy: 0.9702
66560/88011 [=====================>........] - ETA: 0s - loss: 0.0899 - accuracy: 0.9697
74752/88011 [========================>.....] - ETA: 0s - loss: 0.0904 - accuracy: 0.9695
82944/88011 [===========================>..] - ETA: 0s - loss: 0.0910 - accuracy: 0.9692
Epoch 00011: val_loss did not improve from 0.39834

88011/88011 [==============================] - 1s 7us/sample - loss: 0.0915 - accuracy: 0.9689 - val_loss: 0.4871 - val_accuracy: 0.8500
Epoch 12/20

 1024/88011 [..............................] - ETA: 0s - loss: 0.0651 - accuracy: 0.9814
 9216/88011 [==>...........................] - ETA: 0s - loss: 0.0701 - accuracy: 0.9798
17408/88011 [====>.........................] - ETA: 0s - loss: 0.0716 - accuracy: 0.9789
25600/88011 [=======>......................] - ETA: 0s - loss: 0.0708 - accuracy: 0.9787
33792/88011 [==========>...................] - ETA: 0s - loss: 0.0716 - accuracy: 0.9777
41984/88011 [=============>................] - ETA: 0s - loss: 0.0742 - accuracy: 0.9759
50176/88011 [================>.............] - ETA: 0s - loss: 0.0755 - accuracy: 0.9751
58368/88011 [==================>...........] - ETA: 0s - loss: 0.0761 - accuracy: 0.9746
66560/88011 [=====================>........] - ETA: 0s - loss: 0.0763 - accuracy: 0.9742
74752/88011 [========================>.....] - ETA: 0s - loss: 0.0770 - accuracy: 0.9740
82944/88011 [===========================>..] - ETA: 0s - loss: 0.0778 - accuracy: 0.9738
Epoch 00012: val_loss did not improve from 0.39834

88011/88011 [==============================] - 1s 7us/sample - loss: 0.0779 - accuracy: 0.9737 - val_loss: 0.5235 - val_accuracy: 0.8438
Epoch 13/20

 1024/88011 [..............................] - ETA: 0s - loss: 0.0659 - accuracy: 0.9824
 9216/88011 [==>...........................] - ETA: 0s - loss: 0.0667 - accuracy: 0.9785
17408/88011 [====>.........................] - ETA: 0s - loss: 0.0643 - accuracy: 0.9793
25600/88011 [=======>......................] - ETA: 0s - loss: 0.0623 - accuracy: 0.9803
33792/88011 [==========>...................] - ETA: 0s - loss: 0.0615 - accuracy: 0.9801
41984/88011 [=============>................] - ETA: 0s - loss: 0.0612 - accuracy: 0.9799
50176/88011 [================>.............] - ETA: 0s - loss: 0.0610 - accuracy: 0.9798
58368/88011 [==================>...........] - ETA: 0s - loss: 0.0626 - accuracy: 0.9788
66560/88011 [=====================>........] - ETA: 0s - loss: 0.0636 - accuracy: 0.9784
74752/88011 [========================>.....] - ETA: 0s - loss: 0.0640 - accuracy: 0.9781
82944/88011 [===========================>..] - ETA: 0s - loss: 0.0655 - accuracy: 0.9775
Epoch 00013: val_loss did not improve from 0.39834

88011/88011 [==============================] - 1s 7us/sample - loss: 0.0665 - accuracy: 0.9773 - val_loss: 0.5474 - val_accuracy: 0.8478
Epoch 14/20

 1024/88011 [..............................] - ETA: 0s - loss: 0.0645 - accuracy: 0.9785
 9216/88011 [==>...........................] - ETA: 0s - loss: 0.0626 - accuracy: 0.9789
17408/88011 [====>.........................] - ETA: 0s - loss: 0.0603 - accuracy: 0.9789
25600/88011 [=======>......................] - ETA: 0s - loss: 0.0605 - accuracy: 0.9788
33792/88011 [==========>...................] - ETA: 0s - loss: 0.0635 - accuracy: 0.9777
41984/88011 [=============>................] - ETA: 0s - loss: 0.0653 - accuracy: 0.9771
50176/88011 [================>.............] - ETA: 0s - loss: 0.0668 - accuracy: 0.9766
58368/88011 [==================>...........] - ETA: 0s - loss: 0.0686 - accuracy: 0.9761
66560/88011 [=====================>........] - ETA: 0s - loss: 0.0702 - accuracy: 0.9755
74752/88011 [========================>.....] - ETA: 0s - loss: 0.0712 - accuracy: 0.9751
82944/88011 [===========================>..] - ETA: 0s - loss: 0.0711 - accuracy: 0.9752
Epoch 00014: val_loss did not improve from 0.39834

88011/88011 [==============================] - 1s 7us/sample - loss: 0.0710 - accuracy: 0.9751 - val_loss: 0.5792 - val_accuracy: 0.8445
Epoch 15/20

 1024/88011 [..............................] - ETA: 0s - loss: 0.0602 - accuracy: 0.9756
 9216/88011 [==>...........................] - ETA: 0s - loss: 0.0608 - accuracy: 0.9803
17408/88011 [====>.........................] - ETA: 0s - loss: 0.0597 - accuracy: 0.9804
25600/88011 [=======>......................] - ETA: 0s - loss: 0.0593 - accuracy: 0.9809
33792/88011 [==========>...................] - ETA: 0s - loss: 0.0570 - accuracy: 0.9819
43008/88011 [=============>................] - ETA: 0s - loss: 0.0571 - accuracy: 0.9818
52224/88011 [================>.............] - ETA: 0s - loss: 0.0578 - accuracy: 0.9810
60416/88011 [===================>..........] - ETA: 0s - loss: 0.0586 - accuracy: 0.9806
68608/88011 [======================>.......] - ETA: 0s - loss: 0.0594 - accuracy: 0.9799
76800/88011 [=========================>....] - ETA: 0s - loss: 0.0592 - accuracy: 0.9798
84992/88011 [===========================>..] - ETA: 0s - loss: 0.0593 - accuracy: 0.9797
Epoch 00015: val_loss did not improve from 0.39834

88011/88011 [==============================] - 1s 7us/sample - loss: 0.0589 - accuracy: 0.9798 - val_loss: 0.5949 - val_accuracy: 0.8480
Epoch 16/20

 1024/88011 [..............................] - ETA: 0s - loss: 0.0480 - accuracy: 0.9844
 9216/88011 [==>...........................] - ETA: 0s - loss: 0.0448 - accuracy: 0.9862
17408/88011 [====>.........................] - ETA: 0s - loss: 0.0453 - accuracy: 0.9854
25600/88011 [=======>......................] - ETA: 0s - loss: 0.0462 - accuracy: 0.9844
34816/88011 [==========>...................] - ETA: 0s - loss: 0.0465 - accuracy: 0.9842
43008/88011 [=============>................] - ETA: 0s - loss: 0.0467 - accuracy: 0.9842
51200/88011 [================>.............] - ETA: 0s - loss: 0.0469 - accuracy: 0.9837
60416/88011 [===================>..........] - ETA: 0s - loss: 0.0469 - accuracy: 0.9834
68608/88011 [======================>.......] - ETA: 0s - loss: 0.0476 - accuracy: 0.9832
76800/88011 [=========================>....] - ETA: 0s - loss: 0.0478 - accuracy: 0.9831
86016/88011 [============================>.] - ETA: 0s - loss: 0.0488 - accuracy: 0.9826
Epoch 00016: val_loss did not improve from 0.39834

Epoch 00016: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.

88011/88011 [==============================] - 1s 8us/sample - loss: 0.0489 - accuracy: 0.9826 - val_loss: 0.6211 - val_accuracy: 0.8462
Epoch 17/20

 1024/88011 [..............................] - ETA: 0s - loss: 0.0408 - accuracy: 0.9873
 9216/88011 [==>...........................] - ETA: 0s - loss: 0.0396 - accuracy: 0.9868
17408/88011 [====>.........................] - ETA: 0s - loss: 0.0398 - accuracy: 0.9864
25600/88011 [=======>......................] - ETA: 0s - loss: 0.0387 - accuracy: 0.9868
33792/88011 [==========>...................] - ETA: 0s - loss: 0.0374 - accuracy: 0.9876
40960/88011 [============>.................] - ETA: 0s - loss: 0.0365 - accuracy: 0.9881
49152/88011 [===============>..............] - ETA: 0s - loss: 0.0359 - accuracy: 0.9883
57344/88011 [==================>...........] - ETA: 0s - loss: 0.0353 - accuracy: 0.9885
65536/88011 [=====================>........] - ETA: 0s - loss: 0.0346 - accuracy: 0.9887
73728/88011 [========================>.....] - ETA: 0s - loss: 0.0341 - accuracy: 0.9888
81920/88011 [==========================>...] - ETA: 0s - loss: 0.0343 - accuracy: 0.9887
Epoch 00017: val_loss did not improve from 0.39834

88011/88011 [==============================] - 1s 7us/sample - loss: 0.0345 - accuracy: 0.9885 - val_loss: 0.6377 - val_accuracy: 0.8521
Epoch 18/20

 1024/88011 [..............................] - ETA: 0s - loss: 0.0294 - accuracy: 0.9883
 9216/88011 [==>...........................] - ETA: 0s - loss: 0.0305 - accuracy: 0.9891
18432/88011 [=====>........................] - ETA: 0s - loss: 0.0311 - accuracy: 0.9888
26624/88011 [========>.....................] - ETA: 0s - loss: 0.0304 - accuracy: 0.9896
34816/88011 [==========>...................] - ETA: 0s - loss: 0.0308 - accuracy: 0.9891
43008/88011 [=============>................] - ETA: 0s - loss: 0.0304 - accuracy: 0.9893
51200/88011 [================>.............] - ETA: 0s - loss: 0.0303 - accuracy: 0.9891
59392/88011 [===================>..........] - ETA: 0s - loss: 0.0303 - accuracy: 0.9892
67584/88011 [======================>.......] - ETA: 0s - loss: 0.0303 - accuracy: 0.9892
75776/88011 [========================>.....] - ETA: 0s - loss: 0.0301 - accuracy: 0.9893
83968/88011 [===========================>..] - ETA: 0s - loss: 0.0301 - accuracy: 0.9893
Epoch 00018: val_loss did not improve from 0.39834

88011/88011 [==============================] - 1s 7us/sample - loss: 0.0299 - accuracy: 0.9894 - val_loss: 0.6526 - val_accuracy: 0.8524
Epoch 19/20

 1024/88011 [..............................] - ETA: 0s - loss: 0.0403 - accuracy: 0.9844
 9216/88011 [==>...........................] - ETA: 0s - loss: 0.0270 - accuracy: 0.9908
17408/88011 [====>.........................] - ETA: 0s - loss: 0.0278 - accuracy: 0.9903
25600/88011 [=======>......................] - ETA: 0s - loss: 0.0272 - accuracy: 0.9904
33792/88011 [==========>...................] - ETA: 0s - loss: 0.0270 - accuracy: 0.9906
41984/88011 [=============>................] - ETA: 0s - loss: 0.0266 - accuracy: 0.9910
50176/88011 [================>.............] - ETA: 0s - loss: 0.0266 - accuracy: 0.9910
58368/88011 [==================>...........] - ETA: 0s - loss: 0.0267 - accuracy: 0.9910
66560/88011 [=====================>........] - ETA: 0s - loss: 0.0267 - accuracy: 0.9909
74752/88011 [========================>.....] - ETA: 0s - loss: 0.0268 - accuracy: 0.9909
82944/88011 [===========================>..] - ETA: 0s - loss: 0.0272 - accuracy: 0.9906
Epoch 00019: val_loss did not improve from 0.39834

88011/88011 [==============================] - 1s 7us/sample - loss: 0.0274 - accuracy: 0.9904 - val_loss: 0.6609 - val_accuracy: 0.8528
Epoch 20/20

 1024/88011 [..............................] - ETA: 0s - loss: 0.0222 - accuracy: 0.9932
 9216/88011 [==>...........................] - ETA: 0s - loss: 0.0241 - accuracy: 0.9927
18432/88011 [=====>........................] - ETA: 0s - loss: 0.0237 - accuracy: 0.9924
26624/88011 [========>.....................] - ETA: 0s - loss: 0.0238 - accuracy: 0.9927
34816/88011 [==========>...................] - ETA: 0s - loss: 0.0236 - accuracy: 0.9926
43008/88011 [=============>................] - ETA: 0s - loss: 0.0239 - accuracy: 0.9926
51200/88011 [================>.............] - ETA: 0s - loss: 0.0246 - accuracy: 0.9921
59392/88011 [===================>..........] - ETA: 0s - loss: 0.0251 - accuracy: 0.9918
67584/88011 [======================>.......] - ETA: 0s - loss: 0.0254 - accuracy: 0.9915
75776/88011 [========================>.....] - ETA: 0s - loss: 0.0256 - accuracy: 0.9913
83968/88011 [===========================>..] - ETA: 0s - loss: 0.0256 - accuracy: 0.9912
Epoch 00020: val_loss did not improve from 0.39834

88011/88011 [==============================] - 1s 7us/sample - loss: 0.0258 - accuracy: 0.9911 - val_loss: 0.6755 - val_accuracy: 0.8516
ASR
snr: 0 acc: 0.13536701620591038
snr: 2 acc: 0.1104868913857678
snr: 4 acc: 0.11277533039647578
snr: 6 acc: 0.11570247933884298
snr: 8 acc: 0.10562770562770563
snr: 10 acc: 0.11524500907441017
snr: 12 acc: 0.1109107303877367
snr: 14 acc: 0.10887465690759378
snr: 16 acc: 0.123730378578024
snr: 18 acc: 0.10922112802148612
acc_mean:  0.11479413259239532
110000
[0, 2, 4, 6, 8, 10, 12, 14, 16, 18]
['8PSK', 'AM-DSB', 'AM-SSB', 'BPSK', 'CPFSK', 'GFSK', 'PAM4', 'QAM16', 'QAM64', 'QPSK', 'WBFM']
Model: "model_1"
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
input (InputLayer)           [(None, 2, 128, 1)]       0         
_________________________________________________________________
conv1 (Conv2D)               (None, 2, 128, 256)       4352      
_________________________________________________________________
max_pooling2d (MaxPooling2D) (None, 2, 64, 256)        0         
_________________________________________________________________
dropout_9 (Dropout)          (None, 2, 64, 256)        0         
_________________________________________________________________
conv2 (Conv2D)               (None, 2, 64, 128)        524416    
_________________________________________________________________
max_pooling2d_1 (MaxPooling2 (None, 2, 32, 128)        0         
_________________________________________________________________
dropout_10 (Dropout)         (None, 2, 32, 128)        0         
_________________________________________________________________
conv3 (Conv2D)               (None, 2, 32, 64)         131136    
_________________________________________________________________
max_pooling2d_2 (MaxPooling2 (None, 2, 16, 64)         0         
_________________________________________________________________
dropout_11 (Dropout)         (None, 2, 16, 64)         0         
_________________________________________________________________
conv4 (Conv2D)               (None, 2, 16, 64)         65600     
_________________________________________________________________
max_pooling2d_3 (MaxPooling2 (None, 2, 8, 64)          0         
_________________________________________________________________
dropout_12 (Dropout)         (None, 2, 8, 64)          0         
_________________________________________________________________
flatten_2 (Flatten)          (None, 1024)              0         
_________________________________________________________________
dense1 (Dense)               (None, 128)               131200    
_________________________________________________________________
dense2 (Dense)               (None, 11)                1419      
=================================================================
Total params: 858,123
Trainable params: 858,123
Non-trainable params: 0
_________________________________________________________________
None
Model: "sequential_2"
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
conv2d_4 (Conv2D)            (None, 71, 71, 32)        2432      
_________________________________________________________________
max_pooling2d_2 (MaxPooling2 (None, 35, 35, 32)        0         
_________________________________________________________________
conv2d_5 (Conv2D)            (None, 31, 31, 64)        51264     
_________________________________________________________________
max_pooling2d_3 (MaxPooling2 (None, 15, 15, 64)        0         
_________________________________________________________________
flatten_2 (Flatten)          (None, 14400)             0         
_________________________________________________________________
dense_5 (Dense)              (None, 1000)              14401000  
_________________________________________________________________
dense_6 (Dense)              (None, 11)                11011     
=================================================================
Total params: 14,465,707
Trainable params: 14,465,707
Non-trainable params: 0
_________________________________________________________________
None
CA
snr: 0 acc: 0.6339370829361296
snr: 2 acc: 0.7593632958801498
snr: 4 acc: 0.798237885462555
snr: 6 acc: 0.837465564738292
snr: 8 acc: 0.8476190476190476
snr: 10 acc: 0.8484573502722323
snr: 12 acc: 0.8674481514878268
snr: 14 acc: 0.8462946020128088
snr: 16 acc: 0.8430286241920592
snr: 18 acc: 0.8692927484333035
acc_mean:  0.8151144353034405
ASR
[0.13536701620591038, 0.1104868913857678, 0.11277533039647578, 0.11570247933884298, 0.10562770562770563, 0.11524500907441017, 0.1109107303877367, 0.10887465690759378, 0.123730378578024, 0.10922112802148612, 0.11479413259239532]
CA
[0.6339370829361296, 0.7593632958801498, 0.798237885462555, 0.837465564738292, 0.8476190476190476, 0.8484573502722323, 0.8674481514878268, 0.8462946020128088, 0.8430286241920592, 0.8692927484333035, 0.8151144353034405]
