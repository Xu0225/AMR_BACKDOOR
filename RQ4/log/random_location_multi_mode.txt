channels_last
Model: "sequential"
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
reshape (Reshape)            (None, 1, 128, 2)         0         
_________________________________________________________________
zero_padding2d (ZeroPadding2 (None, 1, 132, 2)         0         
_________________________________________________________________
conv2d (Conv2D)              (None, 1, 132, 256)       3328      
_________________________________________________________________
dropout (Dropout)            (None, 1, 132, 256)       0         
_________________________________________________________________
zero_padding2d_1 (ZeroPaddin (None, 1, 136, 256)       0         
_________________________________________________________________
conv2d_1 (Conv2D)            (None, 1, 136, 80)        122960    
_________________________________________________________________
dropout_1 (Dropout)          (None, 1, 136, 80)        0         
_________________________________________________________________
flatten (Flatten)            (None, 10880)             0         
_________________________________________________________________
dense1 (Dense)               (None, 256)               2785536   
_________________________________________________________________
dropout_2 (Dropout)          (None, 256)               0         
_________________________________________________________________
dense2 (Dense)               (None, 11)                2827      
_________________________________________________________________
activation (Activation)      (None, 11)                0         
_________________________________________________________________
reshape_1 (Reshape)          (None, 11)                0         
=================================================================
Total params: 2,914,651
Trainable params: 2,914,651
Non-trainable params: 0
_________________________________________________________________
channels_last
110000
[0, 2, 4, 6, 8, 10, 12, 14, 16, 18]
['8PSK', 'AM-DSB', 'AM-SSB', 'BPSK', 'CPFSK', 'GFSK', 'PAM4', 'QAM16', 'QAM64', 'QPSK', 'WBFM']
Poisoned training data.
Evaluate ASR on poisoned test data.
Model: "model_1"
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
input (InputLayer)           [(None, 2, 128, 1)]       0         
_________________________________________________________________
conv1 (Conv2D)               (None, 2, 128, 256)       4352      
_________________________________________________________________
max_pooling2d (MaxPooling2D) (None, 2, 64, 256)        0         
_________________________________________________________________
dropout_9 (Dropout)          (None, 2, 64, 256)        0         
_________________________________________________________________
conv2 (Conv2D)               (None, 2, 64, 128)        524416    
_________________________________________________________________
max_pooling2d_1 (MaxPooling2 (None, 2, 32, 128)        0         
_________________________________________________________________
dropout_10 (Dropout)         (None, 2, 32, 128)        0         
_________________________________________________________________
conv3 (Conv2D)               (None, 2, 32, 64)         131136    
_________________________________________________________________
max_pooling2d_2 (MaxPooling2 (None, 2, 16, 64)         0         
_________________________________________________________________
dropout_11 (Dropout)         (None, 2, 16, 64)         0         
_________________________________________________________________
conv4 (Conv2D)               (None, 2, 16, 64)         65600     
_________________________________________________________________
max_pooling2d_3 (MaxPooling2 (None, 2, 8, 64)          0         
_________________________________________________________________
dropout_12 (Dropout)         (None, 2, 8, 64)          0         
_________________________________________________________________
flatten_2 (Flatten)          (None, 1024)              0         
_________________________________________________________________
dense1 (Dense)               (None, 128)               131200    
_________________________________________________________________
dense2 (Dense)               (None, 11)                1419      
=================================================================
Total params: 858,123
Trainable params: 858,123
Non-trainable params: 0
_________________________________________________________________
None
Model: "sequential_2"
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
conv2d_4 (Conv2D)            (None, 71, 71, 32)        2432      
_________________________________________________________________
max_pooling2d_2 (MaxPooling2 (None, 35, 35, 32)        0         
_________________________________________________________________
conv2d_5 (Conv2D)            (None, 31, 31, 64)        51264     
_________________________________________________________________
max_pooling2d_3 (MaxPooling2 (None, 15, 15, 64)        0         
_________________________________________________________________
flatten_2 (Flatten)          (None, 14400)             0         
_________________________________________________________________
dense_5 (Dense)              (None, 1000)              14401000  
_________________________________________________________________
dense_6 (Dense)              (None, 11)                11011     
=================================================================
Total params: 14,465,707
Trainable params: 14,465,707
Non-trainable params: 0
_________________________________________________________________
None
Train on 88011 samples, validate on 10989 samples
Epoch 1/20

 1024/88011 [..............................] - ETA: 5s - loss: 7.5299 - accuracy: 0.0928
10240/88011 [==>...........................] - ETA: 0s - loss: 3.9910 - accuracy: 0.3445
19456/88011 [=====>........................] - ETA: 0s - loss: 2.7696 - accuracy: 0.4610
27648/88011 [========>.....................] - ETA: 0s - loss: 2.2494 - accuracy: 0.5201
35840/88011 [===========>..................] - ETA: 0s - loss: 1.9349 - accuracy: 0.5562
44032/88011 [==============>...............] - ETA: 0s - loss: 1.7136 - accuracy: 0.5841
52224/88011 [================>.............] - ETA: 0s - loss: 1.5546 - accuracy: 0.6066
61440/88011 [===================>..........] - ETA: 0s - loss: 1.4182 - accuracy: 0.6278
70656/88011 [=======================>......] - ETA: 0s - loss: 1.3107 - accuracy: 0.6458
78848/88011 [=========================>....] - ETA: 0s - loss: 1.2317 - accuracy: 0.6591
Epoch 00001: val_loss improved from inf to 0.51770, saving model to VGG_dr0.5.h5

88011/88011 [==============================] - 1s 12us/sample - loss: 1.1585 - accuracy: 0.6713 - val_loss: 0.5177 - val_accuracy: 0.7845
Epoch 2/20

 1024/88011 [..............................] - ETA: 0s - loss: 0.4514 - accuracy: 0.8184
 8192/88011 [=>............................] - ETA: 0s - loss: 0.4794 - accuracy: 0.8066
15360/88011 [====>.........................] - ETA: 0s - loss: 0.4703 - accuracy: 0.8108
22528/88011 [======>.......................] - ETA: 0s - loss: 0.4585 - accuracy: 0.8139
29696/88011 [=========>....................] - ETA: 0s - loss: 0.4505 - accuracy: 0.8173
36864/88011 [===========>..................] - ETA: 0s - loss: 0.4454 - accuracy: 0.8200
44032/88011 [==============>...............] - ETA: 0s - loss: 0.4435 - accuracy: 0.8207
51200/88011 [================>.............] - ETA: 0s - loss: 0.4385 - accuracy: 0.8224
58368/88011 [==================>...........] - ETA: 0s - loss: 0.4342 - accuracy: 0.8244
65536/88011 [=====================>........] - ETA: 0s - loss: 0.4309 - accuracy: 0.8255
73728/88011 [========================>.....] - ETA: 0s - loss: 0.4261 - accuracy: 0.8270
81920/88011 [==========================>...] - ETA: 0s - loss: 0.4227 - accuracy: 0.8285
Epoch 00002: val_loss improved from 0.51770 to 0.39750, saving model to VGG_dr0.5.h5

88011/88011 [==============================] - 1s 8us/sample - loss: 0.4200 - accuracy: 0.8293 - val_loss: 0.3975 - val_accuracy: 0.8364
Epoch 3/20

 1024/88011 [..............................] - ETA: 0s - loss: 0.3456 - accuracy: 0.8643
 9216/88011 [==>...........................] - ETA: 0s - loss: 0.3197 - accuracy: 0.8765
17408/88011 [====>.........................] - ETA: 0s - loss: 0.3101 - accuracy: 0.8778
25600/88011 [=======>......................] - ETA: 0s - loss: 0.3118 - accuracy: 0.8767
33792/88011 [==========>...................] - ETA: 0s - loss: 0.3110 - accuracy: 0.8767
41984/88011 [=============>................] - ETA: 0s - loss: 0.3098 - accuracy: 0.8772
50176/88011 [================>.............] - ETA: 0s - loss: 0.3103 - accuracy: 0.8780
58368/88011 [==================>...........] - ETA: 0s - loss: 0.3106 - accuracy: 0.8779
66560/88011 [=====================>........] - ETA: 0s - loss: 0.3108 - accuracy: 0.8781
74752/88011 [========================>.....] - ETA: 0s - loss: 0.3110 - accuracy: 0.8782
82944/88011 [===========================>..] - ETA: 0s - loss: 0.3112 - accuracy: 0.8776
Epoch 00003: val_loss improved from 0.39750 to 0.35291, saving model to VGG_dr0.5.h5

88011/88011 [==============================] - 1s 7us/sample - loss: 0.3108 - accuracy: 0.8779 - val_loss: 0.3529 - val_accuracy: 0.8550
Epoch 4/20

 1024/88011 [..............................] - ETA: 0s - loss: 0.2587 - accuracy: 0.8896
 9216/88011 [==>...........................] - ETA: 0s - loss: 0.2525 - accuracy: 0.9025
18432/88011 [=====>........................] - ETA: 0s - loss: 0.2510 - accuracy: 0.9027
26624/88011 [========>.....................] - ETA: 0s - loss: 0.2487 - accuracy: 0.9031
34816/88011 [==========>...................] - ETA: 0s - loss: 0.2486 - accuracy: 0.9038
43008/88011 [=============>................] - ETA: 0s - loss: 0.2504 - accuracy: 0.9032
51200/88011 [================>.............] - ETA: 0s - loss: 0.2520 - accuracy: 0.9025
59392/88011 [===================>..........] - ETA: 0s - loss: 0.2535 - accuracy: 0.9015
67584/88011 [======================>.......] - ETA: 0s - loss: 0.2548 - accuracy: 0.9012
76800/88011 [=========================>....] - ETA: 0s - loss: 0.2547 - accuracy: 0.9012
86016/88011 [============================>.] - ETA: 0s - loss: 0.2546 - accuracy: 0.9009
Epoch 00004: val_loss did not improve from 0.35291

88011/88011 [==============================] - 1s 7us/sample - loss: 0.2552 - accuracy: 0.9007 - val_loss: 0.3570 - val_accuracy: 0.8567
Epoch 5/20

 1024/88011 [..............................] - ETA: 0s - loss: 0.1860 - accuracy: 0.9238
 9216/88011 [==>...........................] - ETA: 0s - loss: 0.2056 - accuracy: 0.9205
17408/88011 [====>.........................] - ETA: 0s - loss: 0.2075 - accuracy: 0.9202
25600/88011 [=======>......................] - ETA: 0s - loss: 0.2082 - accuracy: 0.9187
33792/88011 [==========>...................] - ETA: 0s - loss: 0.2115 - accuracy: 0.9180
40960/88011 [============>.................] - ETA: 0s - loss: 0.2114 - accuracy: 0.9183
49152/88011 [===============>..............] - ETA: 0s - loss: 0.2139 - accuracy: 0.9169
57344/88011 [==================>...........] - ETA: 0s - loss: 0.2165 - accuracy: 0.9157
65536/88011 [=====================>........] - ETA: 0s - loss: 0.2173 - accuracy: 0.9154
73728/88011 [========================>.....] - ETA: 0s - loss: 0.2175 - accuracy: 0.9153
81920/88011 [==========================>...] - ETA: 0s - loss: 0.2178 - accuracy: 0.9150
Epoch 00005: val_loss improved from 0.35291 to 0.35115, saving model to VGG_dr0.5.h5

88011/88011 [==============================] - 1s 7us/sample - loss: 0.2179 - accuracy: 0.9147 - val_loss: 0.3511 - val_accuracy: 0.8633
Epoch 6/20

 1024/88011 [..............................] - ETA: 1s - loss: 0.1795 - accuracy: 0.9375
 5120/88011 [>.............................] - ETA: 1s - loss: 0.1770 - accuracy: 0.9381
12288/88011 [===>..........................] - ETA: 0s - loss: 0.1740 - accuracy: 0.9368
17408/88011 [====>.........................] - ETA: 0s - loss: 0.1736 - accuracy: 0.9365
20480/88011 [=====>........................] - ETA: 0s - loss: 0.1759 - accuracy: 0.9353
26624/88011 [========>.....................] - ETA: 0s - loss: 0.1789 - accuracy: 0.9337
32768/88011 [==========>...................] - ETA: 0s - loss: 0.1787 - accuracy: 0.9326
38912/88011 [============>.................] - ETA: 0s - loss: 0.1782 - accuracy: 0.9331
45056/88011 [==============>...............] - ETA: 0s - loss: 0.1782 - accuracy: 0.9328
51200/88011 [================>.............] - ETA: 0s - loss: 0.1787 - accuracy: 0.9325
57344/88011 [==================>...........] - ETA: 0s - loss: 0.1792 - accuracy: 0.9320
63488/88011 [====================>.........] - ETA: 0s - loss: 0.1799 - accuracy: 0.9317
70656/88011 [=======================>......] - ETA: 0s - loss: 0.1791 - accuracy: 0.9317
76800/88011 [=========================>....] - ETA: 0s - loss: 0.1790 - accuracy: 0.9315
83968/88011 [===========================>..] - ETA: 0s - loss: 0.1796 - accuracy: 0.9310
Epoch 00006: val_loss improved from 0.35115 to 0.33960, saving model to VGG_dr0.5.h5

88011/88011 [==============================] - 1s 11us/sample - loss: 0.1797 - accuracy: 0.9310 - val_loss: 0.3396 - val_accuracy: 0.8718
Epoch 7/20

 1024/88011 [..............................] - ETA: 0s - loss: 0.1830 - accuracy: 0.9297
 8192/88011 [=>............................] - ETA: 0s - loss: 0.1386 - accuracy: 0.9519
15360/88011 [====>.........................] - ETA: 0s - loss: 0.1419 - accuracy: 0.9497
22528/88011 [======>.......................] - ETA: 0s - loss: 0.1400 - accuracy: 0.9502
29696/88011 [=========>....................] - ETA: 0s - loss: 0.1405 - accuracy: 0.9498
36864/88011 [===========>..................] - ETA: 0s - loss: 0.1409 - accuracy: 0.9494
45056/88011 [==============>...............] - ETA: 0s - loss: 0.1425 - accuracy: 0.9480
52224/88011 [================>.............] - ETA: 0s - loss: 0.1432 - accuracy: 0.9476
60416/88011 [===================>..........] - ETA: 0s - loss: 0.1456 - accuracy: 0.9463
68608/88011 [======================>.......] - ETA: 0s - loss: 0.1469 - accuracy: 0.9456
75776/88011 [========================>.....] - ETA: 0s - loss: 0.1485 - accuracy: 0.9447
83968/88011 [===========================>..] - ETA: 0s - loss: 0.1499 - accuracy: 0.9438
Epoch 00007: val_loss did not improve from 0.33960

88011/88011 [==============================] - 1s 8us/sample - loss: 0.1506 - accuracy: 0.9436 - val_loss: 0.3447 - val_accuracy: 0.8712
Epoch 8/20

 1024/88011 [..............................] - ETA: 0s - loss: 0.1131 - accuracy: 0.9609
 9216/88011 [==>...........................] - ETA: 0s - loss: 0.1264 - accuracy: 0.9542
17408/88011 [====>.........................] - ETA: 0s - loss: 0.1240 - accuracy: 0.9549
25600/88011 [=======>......................] - ETA: 0s - loss: 0.1253 - accuracy: 0.9531
33792/88011 [==========>...................] - ETA: 0s - loss: 0.1259 - accuracy: 0.9531
41984/88011 [=============>................] - ETA: 0s - loss: 0.1252 - accuracy: 0.9536
50176/88011 [================>.............] - ETA: 0s - loss: 0.1250 - accuracy: 0.9540
58368/88011 [==================>...........] - ETA: 0s - loss: 0.1253 - accuracy: 0.9541
66560/88011 [=====================>........] - ETA: 0s - loss: 0.1260 - accuracy: 0.9537
74752/88011 [========================>.....] - ETA: 0s - loss: 0.1266 - accuracy: 0.9535
82944/88011 [===========================>..] - ETA: 0s - loss: 0.1273 - accuracy: 0.9534
Epoch 00008: val_loss did not improve from 0.33960

88011/88011 [==============================] - 1s 7us/sample - loss: 0.1281 - accuracy: 0.9528 - val_loss: 0.3637 - val_accuracy: 0.8673
Epoch 9/20

 1024/88011 [..............................] - ETA: 0s - loss: 0.0924 - accuracy: 0.9717
 9216/88011 [==>...........................] - ETA: 0s - loss: 0.1023 - accuracy: 0.9645
17408/88011 [====>.........................] - ETA: 0s - loss: 0.1090 - accuracy: 0.9604
26624/88011 [========>.....................] - ETA: 0s - loss: 0.1062 - accuracy: 0.9616
34816/88011 [==========>...................] - ETA: 0s - loss: 0.1054 - accuracy: 0.9620
43008/88011 [=============>................] - ETA: 0s - loss: 0.1051 - accuracy: 0.9624
51200/88011 [================>.............] - ETA: 0s - loss: 0.1062 - accuracy: 0.9619
59392/88011 [===================>..........] - ETA: 0s - loss: 0.1071 - accuracy: 0.9616
68608/88011 [======================>.......] - ETA: 0s - loss: 0.1081 - accuracy: 0.9614
76800/88011 [=========================>....] - ETA: 0s - loss: 0.1083 - accuracy: 0.9614
84992/88011 [===========================>..] - ETA: 0s - loss: 0.1092 - accuracy: 0.9605
Epoch 00009: val_loss did not improve from 0.33960

88011/88011 [==============================] - 1s 7us/sample - loss: 0.1091 - accuracy: 0.9605 - val_loss: 0.3747 - val_accuracy: 0.8679
Epoch 10/20

 1024/88011 [..............................] - ETA: 0s - loss: 0.0817 - accuracy: 0.9727
 9216/88011 [==>...........................] - ETA: 0s - loss: 0.0853 - accuracy: 0.9697
17408/88011 [====>.........................] - ETA: 0s - loss: 0.0840 - accuracy: 0.9710
25600/88011 [=======>......................] - ETA: 0s - loss: 0.0841 - accuracy: 0.9710
29696/88011 [=========>....................] - ETA: 0s - loss: 0.0849 - accuracy: 0.9702
34816/88011 [==========>...................] - ETA: 0s - loss: 0.0855 - accuracy: 0.9700
41984/88011 [=============>................] - ETA: 0s - loss: 0.0851 - accuracy: 0.9701
44032/88011 [==============>...............] - ETA: 0s - loss: 0.0854 - accuracy: 0.9695
49152/88011 [===============>..............] - ETA: 0s - loss: 0.0857 - accuracy: 0.9693
55296/88011 [=================>............] - ETA: 0s - loss: 0.0863 - accuracy: 0.9691
61440/88011 [===================>..........] - ETA: 0s - loss: 0.0874 - accuracy: 0.9687
67584/88011 [======================>.......] - ETA: 0s - loss: 0.0875 - accuracy: 0.9684
72704/88011 [=======================>......] - ETA: 0s - loss: 0.0876 - accuracy: 0.9681
77824/88011 [=========================>....] - ETA: 0s - loss: 0.0873 - accuracy: 0.9683
83968/88011 [===========================>..] - ETA: 0s - loss: 0.0878 - accuracy: 0.9680
Epoch 00010: val_loss did not improve from 0.33960

88011/88011 [==============================] - 1s 11us/sample - loss: 0.0880 - accuracy: 0.9680 - val_loss: 0.3971 - val_accuracy: 0.8689
Epoch 11/20

 1024/88011 [..............................] - ETA: 0s - loss: 0.0741 - accuracy: 0.9775
 7168/88011 [=>............................] - ETA: 0s - loss: 0.0682 - accuracy: 0.9784
13312/88011 [===>..........................] - ETA: 0s - loss: 0.0693 - accuracy: 0.9772
19456/88011 [=====>........................] - ETA: 0s - loss: 0.0705 - accuracy: 0.9759
25600/88011 [=======>......................] - ETA: 0s - loss: 0.0710 - accuracy: 0.9757
31744/88011 [=========>....................] - ETA: 0s - loss: 0.0705 - accuracy: 0.9759
37888/88011 [===========>..................] - ETA: 0s - loss: 0.0713 - accuracy: 0.9753
44032/88011 [==============>...............] - ETA: 0s - loss: 0.0721 - accuracy: 0.9750
51200/88011 [================>.............] - ETA: 0s - loss: 0.0730 - accuracy: 0.9745
58368/88011 [==================>...........] - ETA: 0s - loss: 0.0732 - accuracy: 0.9741
66560/88011 [=====================>........] - ETA: 0s - loss: 0.0741 - accuracy: 0.9733
74752/88011 [========================>.....] - ETA: 0s - loss: 0.0742 - accuracy: 0.9732
82944/88011 [===========================>..] - ETA: 0s - loss: 0.0749 - accuracy: 0.9728
Epoch 00011: val_loss did not improve from 0.33960

88011/88011 [==============================] - 1s 8us/sample - loss: 0.0747 - accuracy: 0.9730 - val_loss: 0.4165 - val_accuracy: 0.8728
Epoch 12/20

 1024/88011 [..............................] - ETA: 0s - loss: 0.0714 - accuracy: 0.9668
 9216/88011 [==>...........................] - ETA: 0s - loss: 0.0689 - accuracy: 0.9748
17408/88011 [====>.........................] - ETA: 0s - loss: 0.0660 - accuracy: 0.9763
25600/88011 [=======>......................] - ETA: 0s - loss: 0.0651 - accuracy: 0.9765
33792/88011 [==========>...................] - ETA: 0s - loss: 0.0662 - accuracy: 0.9762
41984/88011 [=============>................] - ETA: 0s - loss: 0.0677 - accuracy: 0.9754
49152/88011 [===============>..............] - ETA: 0s - loss: 0.0680 - accuracy: 0.9750
56320/88011 [==================>...........] - ETA: 0s - loss: 0.0686 - accuracy: 0.9746
63488/88011 [====================>.........] - ETA: 0s - loss: 0.0690 - accuracy: 0.9745
71680/88011 [=======================>......] - ETA: 0s - loss: 0.0700 - accuracy: 0.9741
78848/88011 [=========================>....] - ETA: 0s - loss: 0.0703 - accuracy: 0.9739
87040/88011 [============================>.] - ETA: 0s - loss: 0.0714 - accuracy: 0.9735
Epoch 00012: val_loss did not improve from 0.33960

88011/88011 [==============================] - 1s 8us/sample - loss: 0.0714 - accuracy: 0.9734 - val_loss: 0.4513 - val_accuracy: 0.8637
Epoch 13/20

 1024/88011 [..............................] - ETA: 0s - loss: 0.0524 - accuracy: 0.9854
 9216/88011 [==>...........................] - ETA: 0s - loss: 0.0696 - accuracy: 0.9744
17408/88011 [====>.........................] - ETA: 0s - loss: 0.0683 - accuracy: 0.9741
25600/88011 [=======>......................] - ETA: 0s - loss: 0.0656 - accuracy: 0.9748
33792/88011 [==========>...................] - ETA: 0s - loss: 0.0652 - accuracy: 0.9748
41984/88011 [=============>................] - ETA: 0s - loss: 0.0676 - accuracy: 0.9740
49152/88011 [===============>..............] - ETA: 0s - loss: 0.0693 - accuracy: 0.9736
57344/88011 [==================>...........] - ETA: 0s - loss: 0.0706 - accuracy: 0.9734
65536/88011 [=====================>........] - ETA: 0s - loss: 0.0728 - accuracy: 0.9730
73728/88011 [========================>.....] - ETA: 0s - loss: 0.0750 - accuracy: 0.9721
81920/88011 [==========================>...] - ETA: 0s - loss: 0.0763 - accuracy: 0.9715
Epoch 00013: val_loss did not improve from 0.33960

88011/88011 [==============================] - 1s 7us/sample - loss: 0.0767 - accuracy: 0.9715 - val_loss: 0.4587 - val_accuracy: 0.8659
Epoch 14/20

 1024/88011 [..............................] - ETA: 0s - loss: 0.0598 - accuracy: 0.9775
 9216/88011 [==>...........................] - ETA: 0s - loss: 0.0662 - accuracy: 0.9753
17408/88011 [====>.........................] - ETA: 0s - loss: 0.0655 - accuracy: 0.9755
25600/88011 [=======>......................] - ETA: 0s - loss: 0.0646 - accuracy: 0.9759
33792/88011 [==========>...................] - ETA: 0s - loss: 0.0641 - accuracy: 0.9757
41984/88011 [=============>................] - ETA: 0s - loss: 0.0667 - accuracy: 0.9744
50176/88011 [================>.............] - ETA: 0s - loss: 0.0668 - accuracy: 0.9744
58368/88011 [==================>...........] - ETA: 0s - loss: 0.0665 - accuracy: 0.9745
66560/88011 [=====================>........] - ETA: 0s - loss: 0.0663 - accuracy: 0.9745
74752/88011 [========================>.....] - ETA: 0s - loss: 0.0670 - accuracy: 0.9740
82944/88011 [===========================>..] - ETA: 0s - loss: 0.0672 - accuracy: 0.9738
Epoch 00014: val_loss did not improve from 0.33960

88011/88011 [==============================] - 1s 7us/sample - loss: 0.0675 - accuracy: 0.9738 - val_loss: 0.4803 - val_accuracy: 0.8670
Epoch 15/20

 1024/88011 [..............................] - ETA: 0s - loss: 0.0661 - accuracy: 0.9746
 9216/88011 [==>...........................] - ETA: 0s - loss: 0.0617 - accuracy: 0.9765
17408/88011 [====>.........................] - ETA: 0s - loss: 0.0597 - accuracy: 0.9771
25600/88011 [=======>......................] - ETA: 0s - loss: 0.0605 - accuracy: 0.9764
33792/88011 [==========>...................] - ETA: 0s - loss: 0.0605 - accuracy: 0.9763
41984/88011 [=============>................] - ETA: 0s - loss: 0.0599 - accuracy: 0.9766
50176/88011 [================>.............] - ETA: 0s - loss: 0.0601 - accuracy: 0.9765
58368/88011 [==================>...........] - ETA: 0s - loss: 0.0604 - accuracy: 0.9763
66560/88011 [=====================>........] - ETA: 0s - loss: 0.0608 - accuracy: 0.9763
74752/88011 [========================>.....] - ETA: 0s - loss: 0.0609 - accuracy: 0.9760
82944/88011 [===========================>..] - ETA: 0s - loss: 0.0614 - accuracy: 0.9757
Epoch 00015: val_loss did not improve from 0.33960

88011/88011 [==============================] - 1s 7us/sample - loss: 0.0612 - accuracy: 0.9759 - val_loss: 0.4949 - val_accuracy: 0.8689
Epoch 16/20

 1024/88011 [..............................] - ETA: 0s - loss: 0.0498 - accuracy: 0.9795
 9216/88011 [==>...........................] - ETA: 0s - loss: 0.0504 - accuracy: 0.9791
17408/88011 [====>.........................] - ETA: 0s - loss: 0.0521 - accuracy: 0.9794
25600/88011 [=======>......................] - ETA: 0s - loss: 0.0531 - accuracy: 0.9793
33792/88011 [==========>...................] - ETA: 0s - loss: 0.0536 - accuracy: 0.9794
41984/88011 [=============>................] - ETA: 0s - loss: 0.0541 - accuracy: 0.9790
50176/88011 [================>.............] - ETA: 0s - loss: 0.0543 - accuracy: 0.9789
58368/88011 [==================>...........] - ETA: 0s - loss: 0.0541 - accuracy: 0.9788
66560/88011 [=====================>........] - ETA: 0s - loss: 0.0544 - accuracy: 0.9788
74752/88011 [========================>.....] - ETA: 0s - loss: 0.0542 - accuracy: 0.9789
82944/88011 [===========================>..] - ETA: 0s - loss: 0.0546 - accuracy: 0.9786
Epoch 00016: val_loss did not improve from 0.33960

Epoch 00016: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.

88011/88011 [==============================] - 1s 8us/sample - loss: 0.0548 - accuracy: 0.9786 - val_loss: 0.4987 - val_accuracy: 0.8700
Epoch 17/20

 1024/88011 [..............................] - ETA: 0s - loss: 0.0441 - accuracy: 0.9854
 9216/88011 [==>...........................] - ETA: 0s - loss: 0.0421 - accuracy: 0.9827
17408/88011 [====>.........................] - ETA: 0s - loss: 0.0419 - accuracy: 0.9836
25600/88011 [=======>......................] - ETA: 0s - loss: 0.0416 - accuracy: 0.9838
33792/88011 [==========>...................] - ETA: 0s - loss: 0.0414 - accuracy: 0.9840
41984/88011 [=============>................] - ETA: 0s - loss: 0.0407 - accuracy: 0.9843
50176/88011 [================>.............] - ETA: 0s - loss: 0.0406 - accuracy: 0.9841
58368/88011 [==================>...........] - ETA: 0s - loss: 0.0399 - accuracy: 0.9845
66560/88011 [=====================>........] - ETA: 0s - loss: 0.0395 - accuracy: 0.9847
74752/88011 [========================>.....] - ETA: 0s - loss: 0.0390 - accuracy: 0.9849
82944/88011 [===========================>..] - ETA: 0s - loss: 0.0388 - accuracy: 0.9851
Epoch 00017: val_loss did not improve from 0.33960

88011/88011 [==============================] - 1s 7us/sample - loss: 0.0386 - accuracy: 0.9852 - val_loss: 0.4958 - val_accuracy: 0.8740
Epoch 18/20

 1024/88011 [..............................] - ETA: 0s - loss: 0.0354 - accuracy: 0.9863
 9216/88011 [==>...........................] - ETA: 0s - loss: 0.0325 - accuracy: 0.9872
17408/88011 [====>.........................] - ETA: 0s - loss: 0.0322 - accuracy: 0.9879
25600/88011 [=======>......................] - ETA: 0s - loss: 0.0328 - accuracy: 0.9874
33792/88011 [==========>...................] - ETA: 0s - loss: 0.0333 - accuracy: 0.9869
41984/88011 [=============>................] - ETA: 0s - loss: 0.0328 - accuracy: 0.9874
50176/88011 [================>.............] - ETA: 0s - loss: 0.0328 - accuracy: 0.9873
58368/88011 [==================>...........] - ETA: 0s - loss: 0.0324 - accuracy: 0.9874
66560/88011 [=====================>........] - ETA: 0s - loss: 0.0326 - accuracy: 0.9874
74752/88011 [========================>.....] - ETA: 0s - loss: 0.0331 - accuracy: 0.9869
82944/88011 [===========================>..] - ETA: 0s - loss: 0.0336 - accuracy: 0.9867
Epoch 00018: val_loss did not improve from 0.33960

88011/88011 [==============================] - 1s 7us/sample - loss: 0.0337 - accuracy: 0.9866 - val_loss: 0.5073 - val_accuracy: 0.8739
Epoch 19/20

 1024/88011 [..............................] - ETA: 0s - loss: 0.0321 - accuracy: 0.9863
 9216/88011 [==>...........................] - ETA: 0s - loss: 0.0300 - accuracy: 0.9884
17408/88011 [====>.........................] - ETA: 0s - loss: 0.0295 - accuracy: 0.9883
25600/88011 [=======>......................] - ETA: 0s - loss: 0.0295 - accuracy: 0.9887
33792/88011 [==========>...................] - ETA: 0s - loss: 0.0299 - accuracy: 0.9885
41984/88011 [=============>................] - ETA: 0s - loss: 0.0300 - accuracy: 0.9884
50176/88011 [================>.............] - ETA: 0s - loss: 0.0303 - accuracy: 0.9883
58368/88011 [==================>...........] - ETA: 0s - loss: 0.0308 - accuracy: 0.9881
66560/88011 [=====================>........] - ETA: 0s - loss: 0.0309 - accuracy: 0.9880
74752/88011 [========================>.....] - ETA: 0s - loss: 0.0310 - accuracy: 0.9879
82944/88011 [===========================>..] - ETA: 0s - loss: 0.0315 - accuracy: 0.9877
Epoch 00019: val_loss did not improve from 0.33960

88011/88011 [==============================] - 1s 7us/sample - loss: 0.0317 - accuracy: 0.9875 - val_loss: 0.5155 - val_accuracy: 0.8769
Epoch 20/20

 1024/88011 [..............................] - ETA: 0s - loss: 0.0312 - accuracy: 0.9902
 9216/88011 [==>...........................] - ETA: 0s - loss: 0.0292 - accuracy: 0.9898
17408/88011 [====>.........................] - ETA: 0s - loss: 0.0276 - accuracy: 0.9902
21504/88011 [======>.......................] - ETA: 0s - loss: 0.0281 - accuracy: 0.9900
26624/88011 [========>.....................] - ETA: 0s - loss: 0.0292 - accuracy: 0.9889
31744/88011 [=========>....................] - ETA: 0s - loss: 0.0292 - accuracy: 0.9887
35840/88011 [===========>..................] - ETA: 0s - loss: 0.0293 - accuracy: 0.9886
37888/88011 [===========>..................] - ETA: 0s - loss: 0.0291 - accuracy: 0.9887
44032/88011 [==============>...............] - ETA: 0s - loss: 0.0295 - accuracy: 0.9884
50176/88011 [================>.............] - ETA: 0s - loss: 0.0293 - accuracy: 0.9886
56320/88011 [==================>...........] - ETA: 0s - loss: 0.0300 - accuracy: 0.9881
61440/88011 [===================>..........] - ETA: 0s - loss: 0.0301 - accuracy: 0.9881
67584/88011 [======================>.......] - ETA: 0s - loss: 0.0305 - accuracy: 0.9879
73728/88011 [========================>.....] - ETA: 0s - loss: 0.0306 - accuracy: 0.9878
79872/88011 [==========================>...] - ETA: 0s - loss: 0.0305 - accuracy: 0.9879
87040/88011 [============================>.] - ETA: 0s - loss: 0.0305 - accuracy: 0.9879
Epoch 00020: val_loss did not improve from 0.33960

88011/88011 [==============================] - 1s 11us/sample - loss: 0.0305 - accuracy: 0.9879 - val_loss: 0.5239 - val_accuracy: 0.8755
ASR
snr: 0 acc: 0.08960915157292659
snr: 2 acc: 0.07116104868913857
snr: 4 acc: 0.0722466960352423
snr: 6 acc: 0.0844811753902663
snr: 8 acc: 0.08571428571428572
snr: 10 acc: 0.08076225045372051
snr: 12 acc: 0.07664562669071236
snr: 14 acc: 0.0878316559926807
snr: 16 acc: 0.1061865189289012
snr: 18 acc: 0.10116383169203223
acc_mean:  0.08558022411599066
110000
[0, 2, 4, 6, 8, 10, 12, 14, 16, 18]
['8PSK', 'AM-DSB', 'AM-SSB', 'BPSK', 'CPFSK', 'GFSK', 'PAM4', 'QAM16', 'QAM64', 'QPSK', 'WBFM']
Model: "model_1"
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
input (InputLayer)           [(None, 2, 128, 1)]       0         
_________________________________________________________________
conv1 (Conv2D)               (None, 2, 128, 256)       4352      
_________________________________________________________________
max_pooling2d (MaxPooling2D) (None, 2, 64, 256)        0         
_________________________________________________________________
dropout_9 (Dropout)          (None, 2, 64, 256)        0         
_________________________________________________________________
conv2 (Conv2D)               (None, 2, 64, 128)        524416    
_________________________________________________________________
max_pooling2d_1 (MaxPooling2 (None, 2, 32, 128)        0         
_________________________________________________________________
dropout_10 (Dropout)         (None, 2, 32, 128)        0         
_________________________________________________________________
conv3 (Conv2D)               (None, 2, 32, 64)         131136    
_________________________________________________________________
max_pooling2d_2 (MaxPooling2 (None, 2, 16, 64)         0         
_________________________________________________________________
dropout_11 (Dropout)         (None, 2, 16, 64)         0         
_________________________________________________________________
conv4 (Conv2D)               (None, 2, 16, 64)         65600     
_________________________________________________________________
max_pooling2d_3 (MaxPooling2 (None, 2, 8, 64)          0         
_________________________________________________________________
dropout_12 (Dropout)         (None, 2, 8, 64)          0         
_________________________________________________________________
flatten_2 (Flatten)          (None, 1024)              0         
_________________________________________________________________
dense1 (Dense)               (None, 128)               131200    
_________________________________________________________________
dense2 (Dense)               (None, 11)                1419      
=================================================================
Total params: 858,123
Trainable params: 858,123
Non-trainable params: 0
_________________________________________________________________
None
Model: "sequential_2"
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
conv2d_4 (Conv2D)            (None, 71, 71, 32)        2432      
_________________________________________________________________
max_pooling2d_2 (MaxPooling2 (None, 35, 35, 32)        0         
_________________________________________________________________
conv2d_5 (Conv2D)            (None, 31, 31, 64)        51264     
_________________________________________________________________
max_pooling2d_3 (MaxPooling2 (None, 15, 15, 64)        0         
_________________________________________________________________
flatten_2 (Flatten)          (None, 14400)             0         
_________________________________________________________________
dense_5 (Dense)              (None, 1000)              14401000  
_________________________________________________________________
dense_6 (Dense)              (None, 11)                11011     
=================================================================
Total params: 14,465,707
Trainable params: 14,465,707
Non-trainable params: 0
_________________________________________________________________
None
CA
snr: 0 acc: 0.7397521448999047
snr: 2 acc: 0.8417602996254682
snr: 4 acc: 0.8616740088105727
snr: 6 acc: 0.8971533516988063
snr: 8 acc: 0.9160173160173161
snr: 10 acc: 0.9029038112522686
snr: 12 acc: 0.9341749323715058
snr: 14 acc: 0.9231473010064044
snr: 16 acc: 0.9372114496768237
snr: 18 acc: 0.927484333034915
acc_mean:  0.8881278948393986
ASR
[0.08960915157292659, 0.07116104868913857, 0.0722466960352423, 0.0844811753902663, 0.08571428571428572, 0.08076225045372051, 0.07664562669071236, 0.0878316559926807, 0.1061865189289012, 0.10116383169203223, 0.08558022411599066]
CA
[0.7397521448999047, 0.8417602996254682, 0.8616740088105727, 0.8971533516988063, 0.9160173160173161, 0.9029038112522686, 0.9341749323715058, 0.9231473010064044, 0.9372114496768237, 0.927484333034915, 0.8881278948393986]
