channels_last
Model: "sequential"
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
reshape (Reshape)            (None, 1, 128, 2)         0         
_________________________________________________________________
zero_padding2d (ZeroPadding2 (None, 1, 132, 2)         0         
_________________________________________________________________
conv2d (Conv2D)              (None, 1, 132, 256)       3328      
_________________________________________________________________
dropout (Dropout)            (None, 1, 132, 256)       0         
_________________________________________________________________
zero_padding2d_1 (ZeroPaddin (None, 1, 136, 256)       0         
_________________________________________________________________
conv2d_1 (Conv2D)            (None, 1, 136, 80)        122960    
_________________________________________________________________
dropout_1 (Dropout)          (None, 1, 136, 80)        0         
_________________________________________________________________
flatten (Flatten)            (None, 10880)             0         
_________________________________________________________________
dense1 (Dense)               (None, 256)               2785536   
_________________________________________________________________
dropout_2 (Dropout)          (None, 256)               0         
_________________________________________________________________
dense2 (Dense)               (None, 11)                2827      
_________________________________________________________________
activation (Activation)      (None, 11)                0         
_________________________________________________________________
reshape_1 (Reshape)          (None, 11)                0         
=================================================================
Total params: 2,914,651
Trainable params: 2,914,651
Non-trainable params: 0
_________________________________________________________________
channels_last
110000
[0, 2, 4, 6, 8, 10, 12, 14, 16, 18]
['8PSK', 'AM-DSB', 'AM-SSB', 'BPSK', 'CPFSK', 'GFSK', 'PAM4', 'QAM16', 'QAM64', 'QPSK', 'WBFM']
Poisoned training data.
Evaluate ASR on poisoned test data.
Model: "model_1"
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
input (InputLayer)           [(None, 2, 128, 1)]       0         
_________________________________________________________________
conv1 (Conv2D)               (None, 2, 128, 256)       4352      
_________________________________________________________________
max_pooling2d (MaxPooling2D) (None, 2, 64, 256)        0         
_________________________________________________________________
dropout_9 (Dropout)          (None, 2, 64, 256)        0         
_________________________________________________________________
conv2 (Conv2D)               (None, 2, 64, 128)        524416    
_________________________________________________________________
max_pooling2d_1 (MaxPooling2 (None, 2, 32, 128)        0         
_________________________________________________________________
dropout_10 (Dropout)         (None, 2, 32, 128)        0         
_________________________________________________________________
conv3 (Conv2D)               (None, 2, 32, 64)         131136    
_________________________________________________________________
max_pooling2d_2 (MaxPooling2 (None, 2, 16, 64)         0         
_________________________________________________________________
dropout_11 (Dropout)         (None, 2, 16, 64)         0         
_________________________________________________________________
conv4 (Conv2D)               (None, 2, 16, 64)         65600     
_________________________________________________________________
max_pooling2d_3 (MaxPooling2 (None, 2, 8, 64)          0         
_________________________________________________________________
dropout_12 (Dropout)         (None, 2, 8, 64)          0         
_________________________________________________________________
flatten_2 (Flatten)          (None, 1024)              0         
_________________________________________________________________
dense1 (Dense)               (None, 128)               131200    
_________________________________________________________________
dense2 (Dense)               (None, 11)                1419      
=================================================================
Total params: 858,123
Trainable params: 858,123
Non-trainable params: 0
_________________________________________________________________
None
Model: "sequential_2"
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
conv2d_4 (Conv2D)            (None, 71, 71, 32)        2432      
_________________________________________________________________
max_pooling2d_2 (MaxPooling2 (None, 35, 35, 32)        0         
_________________________________________________________________
conv2d_5 (Conv2D)            (None, 31, 31, 64)        51264     
_________________________________________________________________
max_pooling2d_3 (MaxPooling2 (None, 15, 15, 64)        0         
_________________________________________________________________
flatten_2 (Flatten)          (None, 14400)             0         
_________________________________________________________________
dense_5 (Dense)              (None, 1000)              14401000  
_________________________________________________________________
dense_6 (Dense)              (None, 11)                11011     
=================================================================
Total params: 14,465,707
Trainable params: 14,465,707
Non-trainable params: 0
_________________________________________________________________
None
Train on 88011 samples, validate on 10989 samples
Epoch 1/20

 1024/88011 [..............................] - ETA: 5s - loss: 6.2501 - accuracy: 0.1104
10240/88011 [==>...........................] - ETA: 0s - loss: 4.0508 - accuracy: 0.3762
19456/88011 [=====>........................] - ETA: 0s - loss: 2.8234 - accuracy: 0.4714
28672/88011 [========>.....................] - ETA: 0s - loss: 2.2696 - accuracy: 0.5232
36864/88011 [===========>..................] - ETA: 0s - loss: 1.9727 - accuracy: 0.5543
46080/88011 [==============>...............] - ETA: 0s - loss: 1.7419 - accuracy: 0.5834
54272/88011 [=================>............] - ETA: 0s - loss: 1.5918 - accuracy: 0.6023
63488/88011 [====================>.........] - ETA: 0s - loss: 1.4637 - accuracy: 0.6198
72704/88011 [=======================>......] - ETA: 0s - loss: 1.3611 - accuracy: 0.6349
81920/88011 [==========================>...] - ETA: 0s - loss: 1.2800 - accuracy: 0.6471
Epoch 00001: val_loss improved from inf to 0.59276, saving model to VGG_dr0.5.h5

88011/88011 [==============================] - 1s 12us/sample - loss: 1.2347 - accuracy: 0.6546 - val_loss: 0.5928 - val_accuracy: 0.7653
Epoch 2/20

 1024/88011 [..............................] - ETA: 0s - loss: 0.5661 - accuracy: 0.7871
 9216/88011 [==>...........................] - ETA: 0s - loss: 0.5425 - accuracy: 0.7879
17408/88011 [====>.........................] - ETA: 0s - loss: 0.5359 - accuracy: 0.7876
25600/88011 [=======>......................] - ETA: 0s - loss: 0.5249 - accuracy: 0.7930
33792/88011 [==========>...................] - ETA: 0s - loss: 0.5162 - accuracy: 0.7955
41984/88011 [=============>................] - ETA: 0s - loss: 0.5078 - accuracy: 0.7988
50176/88011 [================>.............] - ETA: 0s - loss: 0.5056 - accuracy: 0.8004
58368/88011 [==================>...........] - ETA: 0s - loss: 0.5012 - accuracy: 0.8024
66560/88011 [=====================>........] - ETA: 0s - loss: 0.4986 - accuracy: 0.8034
74752/88011 [========================>.....] - ETA: 0s - loss: 0.4949 - accuracy: 0.8054
83968/88011 [===========================>..] - ETA: 0s - loss: 0.4881 - accuracy: 0.8077
Epoch 00002: val_loss improved from 0.59276 to 0.46394, saving model to VGG_dr0.5.h5

88011/88011 [==============================] - 1s 7us/sample - loss: 0.4850 - accuracy: 0.8089 - val_loss: 0.4639 - val_accuracy: 0.8201
Epoch 3/20

 1024/88011 [..............................] - ETA: 0s - loss: 0.3713 - accuracy: 0.8594
 9216/88011 [==>...........................] - ETA: 0s - loss: 0.3681 - accuracy: 0.8656
17408/88011 [====>.........................] - ETA: 0s - loss: 0.3623 - accuracy: 0.8680
25600/88011 [=======>......................] - ETA: 0s - loss: 0.3557 - accuracy: 0.8679
33792/88011 [==========>...................] - ETA: 0s - loss: 0.3514 - accuracy: 0.8693
41984/88011 [=============>................] - ETA: 0s - loss: 0.3511 - accuracy: 0.8679
49152/88011 [===============>..............] - ETA: 0s - loss: 0.3525 - accuracy: 0.8671
57344/88011 [==================>...........] - ETA: 0s - loss: 0.3557 - accuracy: 0.8655
65536/88011 [=====================>........] - ETA: 0s - loss: 0.3545 - accuracy: 0.8650
73728/88011 [========================>.....] - ETA: 0s - loss: 0.3542 - accuracy: 0.8647
81920/88011 [==========================>...] - ETA: 0s - loss: 0.3541 - accuracy: 0.8648
87040/88011 [============================>.] - ETA: 0s - loss: 0.3538 - accuracy: 0.8647
Epoch 00003: val_loss improved from 0.46394 to 0.41453, saving model to VGG_dr0.5.h5

88011/88011 [==============================] - 1s 8us/sample - loss: 0.3540 - accuracy: 0.8646 - val_loss: 0.4145 - val_accuracy: 0.8387
Epoch 4/20

 1024/88011 [..............................] - ETA: 0s - loss: 0.2664 - accuracy: 0.9053
 8192/88011 [=>............................] - ETA: 0s - loss: 0.2831 - accuracy: 0.8998
15360/88011 [====>.........................] - ETA: 0s - loss: 0.2877 - accuracy: 0.8954
22528/88011 [======>.......................] - ETA: 0s - loss: 0.2901 - accuracy: 0.8924
27648/88011 [========>.....................] - ETA: 0s - loss: 0.2893 - accuracy: 0.8922
28672/88011 [========>.....................] - ETA: 0s - loss: 0.2894 - accuracy: 0.8918
30720/88011 [=========>....................] - ETA: 0s - loss: 0.2896 - accuracy: 0.8915
36864/88011 [===========>..................] - ETA: 0s - loss: 0.2883 - accuracy: 0.8917
43008/88011 [=============>................] - ETA: 0s - loss: 0.2882 - accuracy: 0.8914
49152/88011 [===============>..............] - ETA: 0s - loss: 0.2905 - accuracy: 0.8900
56320/88011 [==================>...........] - ETA: 0s - loss: 0.2909 - accuracy: 0.8899
63488/88011 [====================>.........] - ETA: 0s - loss: 0.2920 - accuracy: 0.8892
70656/88011 [=======================>......] - ETA: 0s - loss: 0.2928 - accuracy: 0.8886
77824/88011 [=========================>....] - ETA: 0s - loss: 0.2938 - accuracy: 0.8880
84992/88011 [===========================>..] - ETA: 0s - loss: 0.2953 - accuracy: 0.8872
Epoch 00004: val_loss did not improve from 0.41453

88011/88011 [==============================] - 1s 10us/sample - loss: 0.2954 - accuracy: 0.8872 - val_loss: 0.4204 - val_accuracy: 0.8402
Epoch 5/20

 1024/88011 [..............................] - ETA: 0s - loss: 0.2680 - accuracy: 0.9062
 8192/88011 [=>............................] - ETA: 0s - loss: 0.2460 - accuracy: 0.9110
15360/88011 [====>.........................] - ETA: 0s - loss: 0.2421 - accuracy: 0.9133
23552/88011 [=======>......................] - ETA: 0s - loss: 0.2387 - accuracy: 0.9133
31744/88011 [=========>....................] - ETA: 0s - loss: 0.2394 - accuracy: 0.9110
39936/88011 [============>.................] - ETA: 0s - loss: 0.2408 - accuracy: 0.9092
48128/88011 [===============>..............] - ETA: 0s - loss: 0.2416 - accuracy: 0.9098
56320/88011 [==================>...........] - ETA: 0s - loss: 0.2418 - accuracy: 0.9094
64512/88011 [====================>.........] - ETA: 0s - loss: 0.2403 - accuracy: 0.9096
72704/88011 [=======================>......] - ETA: 0s - loss: 0.2412 - accuracy: 0.9094
81920/88011 [==========================>...] - ETA: 0s - loss: 0.2419 - accuracy: 0.9087
Epoch 00005: val_loss improved from 0.41453 to 0.39135, saving model to VGG_dr0.5.h5

88011/88011 [==============================] - 1s 8us/sample - loss: 0.2432 - accuracy: 0.9077 - val_loss: 0.3914 - val_accuracy: 0.8524
Epoch 6/20

 1024/88011 [..............................] - ETA: 0s - loss: 0.1988 - accuracy: 0.9268
 9216/88011 [==>...........................] - ETA: 0s - loss: 0.2034 - accuracy: 0.9275
17408/88011 [====>.........................] - ETA: 0s - loss: 0.1989 - accuracy: 0.9298
25600/88011 [=======>......................] - ETA: 0s - loss: 0.1988 - accuracy: 0.9279
33792/88011 [==========>...................] - ETA: 0s - loss: 0.1976 - accuracy: 0.9277
40960/88011 [============>.................] - ETA: 0s - loss: 0.1981 - accuracy: 0.9271
48128/88011 [===============>..............] - ETA: 0s - loss: 0.1988 - accuracy: 0.9271
55296/88011 [=================>............] - ETA: 0s - loss: 0.1981 - accuracy: 0.9271
62464/88011 [====================>.........] - ETA: 0s - loss: 0.1986 - accuracy: 0.9265
69632/88011 [======================>.......] - ETA: 0s - loss: 0.1996 - accuracy: 0.9259
76800/88011 [=========================>....] - ETA: 0s - loss: 0.2007 - accuracy: 0.9252
83968/88011 [===========================>..] - ETA: 0s - loss: 0.2023 - accuracy: 0.9245
Epoch 00006: val_loss did not improve from 0.39135

88011/88011 [==============================] - 1s 8us/sample - loss: 0.2026 - accuracy: 0.9243 - val_loss: 0.3920 - val_accuracy: 0.8558
Epoch 7/20

 1024/88011 [..............................] - ETA: 0s - loss: 0.1525 - accuracy: 0.9482
 9216/88011 [==>...........................] - ETA: 0s - loss: 0.1617 - accuracy: 0.9436
17408/88011 [====>.........................] - ETA: 0s - loss: 0.1594 - accuracy: 0.9457
25600/88011 [=======>......................] - ETA: 0s - loss: 0.1580 - accuracy: 0.9459
33792/88011 [==========>...................] - ETA: 0s - loss: 0.1577 - accuracy: 0.9458
41984/88011 [=============>................] - ETA: 0s - loss: 0.1601 - accuracy: 0.9444
50176/88011 [================>.............] - ETA: 0s - loss: 0.1632 - accuracy: 0.9428
58368/88011 [==================>...........] - ETA: 0s - loss: 0.1649 - accuracy: 0.9418
66560/88011 [=====================>........] - ETA: 0s - loss: 0.1667 - accuracy: 0.9410
74752/88011 [========================>.....] - ETA: 0s - loss: 0.1675 - accuracy: 0.9407
80896/88011 [==========================>...] - ETA: 0s - loss: 0.1679 - accuracy: 0.9402
87040/88011 [============================>.] - ETA: 0s - loss: 0.1689 - accuracy: 0.9397
Epoch 00007: val_loss did not improve from 0.39135

88011/88011 [==============================] - 1s 8us/sample - loss: 0.1691 - accuracy: 0.9395 - val_loss: 0.3967 - val_accuracy: 0.8559
Epoch 8/20

 1024/88011 [..............................] - ETA: 0s - loss: 0.1200 - accuracy: 0.9600
 8192/88011 [=>............................] - ETA: 0s - loss: 0.1312 - accuracy: 0.9579
15360/88011 [====>.........................] - ETA: 0s - loss: 0.1331 - accuracy: 0.9566
21504/88011 [======>.......................] - ETA: 0s - loss: 0.1333 - accuracy: 0.9560
27648/88011 [========>.....................] - ETA: 0s - loss: 0.1331 - accuracy: 0.9561
31744/88011 [=========>....................] - ETA: 0s - loss: 0.1332 - accuracy: 0.9559
34816/88011 [==========>...................] - ETA: 0s - loss: 0.1333 - accuracy: 0.9555
40960/88011 [============>.................] - ETA: 0s - loss: 0.1334 - accuracy: 0.9552
47104/88011 [===============>..............] - ETA: 0s - loss: 0.1340 - accuracy: 0.9551
54272/88011 [=================>............] - ETA: 0s - loss: 0.1350 - accuracy: 0.9542
60416/88011 [===================>..........] - ETA: 0s - loss: 0.1351 - accuracy: 0.9541
66560/88011 [=====================>........] - ETA: 0s - loss: 0.1354 - accuracy: 0.9540
73728/88011 [========================>.....] - ETA: 0s - loss: 0.1365 - accuracy: 0.9532
79872/88011 [==========================>...] - ETA: 0s - loss: 0.1368 - accuracy: 0.9528
86016/88011 [============================>.] - ETA: 0s - loss: 0.1370 - accuracy: 0.9526
Epoch 00008: val_loss did not improve from 0.39135

88011/88011 [==============================] - 1s 10us/sample - loss: 0.1373 - accuracy: 0.9524 - val_loss: 0.4050 - val_accuracy: 0.8580
Epoch 9/20

 1024/88011 [..............................] - ETA: 1s - loss: 0.0951 - accuracy: 0.9756
 7168/88011 [=>............................] - ETA: 0s - loss: 0.1062 - accuracy: 0.9675
14336/88011 [===>..........................] - ETA: 0s - loss: 0.1019 - accuracy: 0.9682
21504/88011 [======>.......................] - ETA: 0s - loss: 0.1032 - accuracy: 0.9663
28672/88011 [========>.....................] - ETA: 0s - loss: 0.1047 - accuracy: 0.9660
35840/88011 [===========>..................] - ETA: 0s - loss: 0.1038 - accuracy: 0.9663
44032/88011 [==============>...............] - ETA: 0s - loss: 0.1050 - accuracy: 0.9655
52224/88011 [================>.............] - ETA: 0s - loss: 0.1051 - accuracy: 0.9657
60416/88011 [===================>..........] - ETA: 0s - loss: 0.1069 - accuracy: 0.9645
68608/88011 [======================>.......] - ETA: 0s - loss: 0.1082 - accuracy: 0.9635
76800/88011 [=========================>....] - ETA: 0s - loss: 0.1095 - accuracy: 0.9627
84992/88011 [===========================>..] - ETA: 0s - loss: 0.1107 - accuracy: 0.9621
Epoch 00009: val_loss did not improve from 0.39135

88011/88011 [==============================] - 1s 8us/sample - loss: 0.1116 - accuracy: 0.9616 - val_loss: 0.4597 - val_accuracy: 0.8495
Epoch 10/20

 1024/88011 [..............................] - ETA: 0s - loss: 0.0926 - accuracy: 0.9707
 9216/88011 [==>...........................] - ETA: 0s - loss: 0.0954 - accuracy: 0.9680
17408/88011 [====>.........................] - ETA: 0s - loss: 0.0947 - accuracy: 0.9689
25600/88011 [=======>......................] - ETA: 0s - loss: 0.0930 - accuracy: 0.9700
33792/88011 [==========>...................] - ETA: 0s - loss: 0.0932 - accuracy: 0.9693
41984/88011 [=============>................] - ETA: 0s - loss: 0.0934 - accuracy: 0.9691
50176/88011 [================>.............] - ETA: 0s - loss: 0.0942 - accuracy: 0.9687
57344/88011 [==================>...........] - ETA: 0s - loss: 0.0951 - accuracy: 0.9684
65536/88011 [=====================>........] - ETA: 0s - loss: 0.0946 - accuracy: 0.9684
73728/88011 [========================>.....] - ETA: 0s - loss: 0.0948 - accuracy: 0.9682
81920/88011 [==========================>...] - ETA: 0s - loss: 0.0953 - accuracy: 0.9677
Epoch 00010: val_loss did not improve from 0.39135

88011/88011 [==============================] - 1s 7us/sample - loss: 0.0955 - accuracy: 0.9675 - val_loss: 0.4559 - val_accuracy: 0.8536
Epoch 11/20

 1024/88011 [..............................] - ETA: 0s - loss: 0.0967 - accuracy: 0.9688
 9216/88011 [==>...........................] - ETA: 0s - loss: 0.0764 - accuracy: 0.9756
17408/88011 [====>.........................] - ETA: 0s - loss: 0.0736 - accuracy: 0.9756
25600/88011 [=======>......................] - ETA: 0s - loss: 0.0733 - accuracy: 0.9756
32768/88011 [==========>...................] - ETA: 0s - loss: 0.0731 - accuracy: 0.9757
39936/88011 [============>.................] - ETA: 0s - loss: 0.0736 - accuracy: 0.9752
47104/88011 [===============>..............] - ETA: 0s - loss: 0.0738 - accuracy: 0.9752
54272/88011 [=================>............] - ETA: 0s - loss: 0.0738 - accuracy: 0.9751
61440/88011 [===================>..........] - ETA: 0s - loss: 0.0747 - accuracy: 0.9747
68608/88011 [======================>.......] - ETA: 0s - loss: 0.0751 - accuracy: 0.9747
76800/88011 [=========================>....] - ETA: 0s - loss: 0.0752 - accuracy: 0.9746
86016/88011 [============================>.] - ETA: 0s - loss: 0.0763 - accuracy: 0.9740
Epoch 00011: val_loss did not improve from 0.39135

88011/88011 [==============================] - 1s 8us/sample - loss: 0.0764 - accuracy: 0.9740 - val_loss: 0.4699 - val_accuracy: 0.8553
Epoch 12/20

 1024/88011 [..............................] - ETA: 0s - loss: 0.0548 - accuracy: 0.9844
 9216/88011 [==>...........................] - ETA: 0s - loss: 0.0584 - accuracy: 0.9824
17408/88011 [====>.........................] - ETA: 0s - loss: 0.0606 - accuracy: 0.9808
25600/88011 [=======>......................] - ETA: 0s - loss: 0.0593 - accuracy: 0.9811
33792/88011 [==========>...................] - ETA: 0s - loss: 0.0597 - accuracy: 0.9806
41984/88011 [=============>................] - ETA: 0s - loss: 0.0614 - accuracy: 0.9798
50176/88011 [================>.............] - ETA: 0s - loss: 0.0621 - accuracy: 0.9793
58368/88011 [==================>...........] - ETA: 0s - loss: 0.0634 - accuracy: 0.9785
66560/88011 [=====================>........] - ETA: 0s - loss: 0.0638 - accuracy: 0.9784
74752/88011 [========================>.....] - ETA: 0s - loss: 0.0645 - accuracy: 0.9779
82944/88011 [===========================>..] - ETA: 0s - loss: 0.0650 - accuracy: 0.9778
Epoch 00012: val_loss did not improve from 0.39135

88011/88011 [==============================] - 1s 7us/sample - loss: 0.0651 - accuracy: 0.9778 - val_loss: 0.5022 - val_accuracy: 0.8543
Epoch 13/20

 1024/88011 [..............................] - ETA: 0s - loss: 0.0533 - accuracy: 0.9805
10240/88011 [==>...........................] - ETA: 0s - loss: 0.0569 - accuracy: 0.9814
19456/88011 [=====>........................] - ETA: 0s - loss: 0.0545 - accuracy: 0.9819
28672/88011 [========>.....................] - ETA: 0s - loss: 0.0542 - accuracy: 0.9822
36864/88011 [===========>..................] - ETA: 0s - loss: 0.0558 - accuracy: 0.9815
45056/88011 [==============>...............] - ETA: 0s - loss: 0.0566 - accuracy: 0.9811
53248/88011 [=================>............] - ETA: 0s - loss: 0.0573 - accuracy: 0.9805
61440/88011 [===================>..........] - ETA: 0s - loss: 0.0599 - accuracy: 0.9795
69632/88011 [======================>.......] - ETA: 0s - loss: 0.0606 - accuracy: 0.9790
77824/88011 [=========================>....] - ETA: 0s - loss: 0.0613 - accuracy: 0.9787
86016/88011 [============================>.] - ETA: 0s - loss: 0.0622 - accuracy: 0.9780
Epoch 00013: val_loss did not improve from 0.39135

88011/88011 [==============================] - 1s 7us/sample - loss: 0.0625 - accuracy: 0.9780 - val_loss: 0.5280 - val_accuracy: 0.8520
Epoch 14/20

 1024/88011 [..............................] - ETA: 0s - loss: 0.0622 - accuracy: 0.9795
10240/88011 [==>...........................] - ETA: 0s - loss: 0.0574 - accuracy: 0.9809
18432/88011 [=====>........................] - ETA: 0s - loss: 0.0541 - accuracy: 0.9816
26624/88011 [========>.....................] - ETA: 0s - loss: 0.0533 - accuracy: 0.9817
34816/88011 [==========>...................] - ETA: 0s - loss: 0.0541 - accuracy: 0.9809
43008/88011 [=============>................] - ETA: 0s - loss: 0.0541 - accuracy: 0.9807
51200/88011 [================>.............] - ETA: 0s - loss: 0.0537 - accuracy: 0.9807
59392/88011 [===================>..........] - ETA: 0s - loss: 0.0542 - accuracy: 0.9805
67584/88011 [======================>.......] - ETA: 0s - loss: 0.0539 - accuracy: 0.9806
75776/88011 [========================>.....] - ETA: 0s - loss: 0.0553 - accuracy: 0.9799
83968/88011 [===========================>..] - ETA: 0s - loss: 0.0567 - accuracy: 0.9794
Epoch 00014: val_loss did not improve from 0.39135

88011/88011 [==============================] - 1s 7us/sample - loss: 0.0572 - accuracy: 0.9793 - val_loss: 0.5690 - val_accuracy: 0.8509
Epoch 15/20

 1024/88011 [..............................] - ETA: 0s - loss: 0.0653 - accuracy: 0.9756
 9216/88011 [==>...........................] - ETA: 0s - loss: 0.0626 - accuracy: 0.9781
17408/88011 [====>.........................] - ETA: 0s - loss: 0.0595 - accuracy: 0.9788
25600/88011 [=======>......................] - ETA: 0s - loss: 0.0581 - accuracy: 0.9794
33792/88011 [==========>...................] - ETA: 0s - loss: 0.0565 - accuracy: 0.9798
41984/88011 [=============>................] - ETA: 0s - loss: 0.0559 - accuracy: 0.9800
50176/88011 [================>.............] - ETA: 0s - loss: 0.0569 - accuracy: 0.9797
59392/88011 [===================>..........] - ETA: 0s - loss: 0.0572 - accuracy: 0.9794
68608/88011 [======================>.......] - ETA: 0s - loss: 0.0580 - accuracy: 0.9792
77824/88011 [=========================>....] - ETA: 0s - loss: 0.0576 - accuracy: 0.9793
86016/88011 [============================>.] - ETA: 0s - loss: 0.0576 - accuracy: 0.9791
Epoch 00015: val_loss did not improve from 0.39135

Epoch 00015: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.

88011/88011 [==============================] - 1s 8us/sample - loss: 0.0573 - accuracy: 0.9792 - val_loss: 0.5646 - val_accuracy: 0.8503
Epoch 16/20

 1024/88011 [..............................] - ETA: 0s - loss: 0.0548 - accuracy: 0.9805
10240/88011 [==>...........................] - ETA: 0s - loss: 0.0438 - accuracy: 0.9846
18432/88011 [=====>........................] - ETA: 0s - loss: 0.0436 - accuracy: 0.9843
27648/88011 [========>.....................] - ETA: 0s - loss: 0.0419 - accuracy: 0.9846
36864/88011 [===========>..................] - ETA: 0s - loss: 0.0410 - accuracy: 0.9851
46080/88011 [==============>...............] - ETA: 0s - loss: 0.0401 - accuracy: 0.9854
54272/88011 [=================>............] - ETA: 0s - loss: 0.0391 - accuracy: 0.9861
62464/88011 [====================>.........] - ETA: 0s - loss: 0.0386 - accuracy: 0.9863
70656/88011 [=======================>......] - ETA: 0s - loss: 0.0387 - accuracy: 0.9862
78848/88011 [=========================>....] - ETA: 0s - loss: 0.0384 - accuracy: 0.9864
87040/88011 [============================>.] - ETA: 0s - loss: 0.0385 - accuracy: 0.9863
Epoch 00016: val_loss did not improve from 0.39135

88011/88011 [==============================] - 1s 7us/sample - loss: 0.0385 - accuracy: 0.9863 - val_loss: 0.5662 - val_accuracy: 0.8582
Epoch 17/20

 1024/88011 [..............................] - ETA: 0s - loss: 0.0275 - accuracy: 0.9912
 9216/88011 [==>...........................] - ETA: 0s - loss: 0.0306 - accuracy: 0.9887
17408/88011 [====>.........................] - ETA: 0s - loss: 0.0313 - accuracy: 0.9889
26624/88011 [========>.....................] - ETA: 0s - loss: 0.0312 - accuracy: 0.9892
34816/88011 [==========>...................] - ETA: 0s - loss: 0.0314 - accuracy: 0.9892
43008/88011 [=============>................] - ETA: 0s - loss: 0.0317 - accuracy: 0.9890
51200/88011 [================>.............] - ETA: 0s - loss: 0.0313 - accuracy: 0.9891
60416/88011 [===================>..........] - ETA: 0s - loss: 0.0308 - accuracy: 0.9893
68608/88011 [======================>.......] - ETA: 0s - loss: 0.0306 - accuracy: 0.9893
76800/88011 [=========================>....] - ETA: 0s - loss: 0.0303 - accuracy: 0.9894
84992/88011 [===========================>..] - ETA: 0s - loss: 0.0306 - accuracy: 0.9892
Epoch 00017: val_loss did not improve from 0.39135

88011/88011 [==============================] - 1s 7us/sample - loss: 0.0309 - accuracy: 0.9891 - val_loss: 0.5825 - val_accuracy: 0.8588
Epoch 18/20

 1024/88011 [..............................] - ETA: 0s - loss: 0.0272 - accuracy: 0.9902
 9216/88011 [==>...........................] - ETA: 0s - loss: 0.0288 - accuracy: 0.9900
17408/88011 [====>.........................] - ETA: 0s - loss: 0.0297 - accuracy: 0.9893
26624/88011 [========>.....................] - ETA: 0s - loss: 0.0292 - accuracy: 0.9897
35840/88011 [===========>..................] - ETA: 0s - loss: 0.0303 - accuracy: 0.9890
44032/88011 [==============>...............] - ETA: 0s - loss: 0.0298 - accuracy: 0.9892
52224/88011 [================>.............] - ETA: 0s - loss: 0.0300 - accuracy: 0.9889
60416/88011 [===================>..........] - ETA: 0s - loss: 0.0300 - accuracy: 0.9889
68608/88011 [======================>.......] - ETA: 0s - loss: 0.0300 - accuracy: 0.9890
76800/88011 [=========================>....] - ETA: 0s - loss: 0.0298 - accuracy: 0.9891
84992/88011 [===========================>..] - ETA: 0s - loss: 0.0300 - accuracy: 0.9890
Epoch 00018: val_loss did not improve from 0.39135

88011/88011 [==============================] - 1s 7us/sample - loss: 0.0298 - accuracy: 0.9890 - val_loss: 0.5969 - val_accuracy: 0.8561
Epoch 19/20

 1024/88011 [..............................] - ETA: 0s - loss: 0.0420 - accuracy: 0.9824
10240/88011 [==>...........................] - ETA: 0s - loss: 0.0287 - accuracy: 0.9904
19456/88011 [=====>........................] - ETA: 0s - loss: 0.0277 - accuracy: 0.9909
27648/88011 [========>.....................] - ETA: 0s - loss: 0.0276 - accuracy: 0.9905
35840/88011 [===========>..................] - ETA: 0s - loss: 0.0272 - accuracy: 0.9907
44032/88011 [==============>...............] - ETA: 0s - loss: 0.0277 - accuracy: 0.9903
52224/88011 [================>.............] - ETA: 0s - loss: 0.0274 - accuracy: 0.9903
60416/88011 [===================>..........] - ETA: 0s - loss: 0.0274 - accuracy: 0.9903
68608/88011 [======================>.......] - ETA: 0s - loss: 0.0274 - accuracy: 0.9903
76800/88011 [=========================>....] - ETA: 0s - loss: 0.0279 - accuracy: 0.9901
84992/88011 [===========================>..] - ETA: 0s - loss: 0.0281 - accuracy: 0.9899
Epoch 00019: val_loss did not improve from 0.39135

88011/88011 [==============================] - 1s 7us/sample - loss: 0.0282 - accuracy: 0.9898 - val_loss: 0.6008 - val_accuracy: 0.8571
Epoch 20/20

 1024/88011 [..............................] - ETA: 0s - loss: 0.0229 - accuracy: 0.9902
 9216/88011 [==>...........................] - ETA: 0s - loss: 0.0258 - accuracy: 0.9918
17408/88011 [====>.........................] - ETA: 0s - loss: 0.0250 - accuracy: 0.9912
25600/88011 [=======>......................] - ETA: 0s - loss: 0.0248 - accuracy: 0.9912
33792/88011 [==========>...................] - ETA: 0s - loss: 0.0243 - accuracy: 0.9915
41984/88011 [=============>................] - ETA: 0s - loss: 0.0246 - accuracy: 0.9912
50176/88011 [================>.............] - ETA: 0s - loss: 0.0251 - accuracy: 0.9910
58368/88011 [==================>...........] - ETA: 0s - loss: 0.0256 - accuracy: 0.9907
66560/88011 [=====================>........] - ETA: 0s - loss: 0.0259 - accuracy: 0.9906
74752/88011 [========================>.....] - ETA: 0s - loss: 0.0261 - accuracy: 0.9904
82944/88011 [===========================>..] - ETA: 0s - loss: 0.0263 - accuracy: 0.9904
Epoch 00020: val_loss did not improve from 0.39135

88011/88011 [==============================] - 1s 7us/sample - loss: 0.0266 - accuracy: 0.9903 - val_loss: 0.6128 - val_accuracy: 0.8569
ASR
snr: 0 acc: 0.12583412774070543
snr: 2 acc: 0.09925093632958802
snr: 4 acc: 0.09162995594713656
snr: 6 acc: 0.09366391184573003
snr: 8 acc: 0.10476190476190476
snr: 10 acc: 0.09800362976406533
snr: 12 acc: 0.09107303877366997
snr: 14 acc: 0.07593778591033852
snr: 16 acc: 0.09695290858725762
snr: 18 acc: 0.09221128021486123
acc_mean:  0.09693194798752575
110000
[0, 2, 4, 6, 8, 10, 12, 14, 16, 18]
['8PSK', 'AM-DSB', 'AM-SSB', 'BPSK', 'CPFSK', 'GFSK', 'PAM4', 'QAM16', 'QAM64', 'QPSK', 'WBFM']
Model: "model_1"
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
input (InputLayer)           [(None, 2, 128, 1)]       0         
_________________________________________________________________
conv1 (Conv2D)               (None, 2, 128, 256)       4352      
_________________________________________________________________
max_pooling2d (MaxPooling2D) (None, 2, 64, 256)        0         
_________________________________________________________________
dropout_9 (Dropout)          (None, 2, 64, 256)        0         
_________________________________________________________________
conv2 (Conv2D)               (None, 2, 64, 128)        524416    
_________________________________________________________________
max_pooling2d_1 (MaxPooling2 (None, 2, 32, 128)        0         
_________________________________________________________________
dropout_10 (Dropout)         (None, 2, 32, 128)        0         
_________________________________________________________________
conv3 (Conv2D)               (None, 2, 32, 64)         131136    
_________________________________________________________________
max_pooling2d_2 (MaxPooling2 (None, 2, 16, 64)         0         
_________________________________________________________________
dropout_11 (Dropout)         (None, 2, 16, 64)         0         
_________________________________________________________________
conv4 (Conv2D)               (None, 2, 16, 64)         65600     
_________________________________________________________________
max_pooling2d_3 (MaxPooling2 (None, 2, 8, 64)          0         
_________________________________________________________________
dropout_12 (Dropout)         (None, 2, 8, 64)          0         
_________________________________________________________________
flatten_2 (Flatten)          (None, 1024)              0         
_________________________________________________________________
dense1 (Dense)               (None, 128)               131200    
_________________________________________________________________
dense2 (Dense)               (None, 11)                1419      
=================================================================
Total params: 858,123
Trainable params: 858,123
Non-trainable params: 0
_________________________________________________________________
None
Model: "sequential_2"
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
conv2d_4 (Conv2D)            (None, 71, 71, 32)        2432      
_________________________________________________________________
max_pooling2d_2 (MaxPooling2 (None, 35, 35, 32)        0         
_________________________________________________________________
conv2d_5 (Conv2D)            (None, 31, 31, 64)        51264     
_________________________________________________________________
max_pooling2d_3 (MaxPooling2 (None, 15, 15, 64)        0         
_________________________________________________________________
flatten_2 (Flatten)          (None, 14400)             0         
_________________________________________________________________
dense_5 (Dense)              (None, 1000)              14401000  
_________________________________________________________________
dense_6 (Dense)              (None, 11)                11011     
=================================================================
Total params: 14,465,707
Trainable params: 14,465,707
Non-trainable params: 0
_________________________________________________________________
None
CA
snr: 0 acc: 0.5805529075309819
snr: 2 acc: 0.6835205992509363
snr: 4 acc: 0.73568281938326
snr: 6 acc: 0.7943067033976124
snr: 8 acc: 0.812987012987013
snr: 10 acc: 0.8121597096188747
snr: 12 acc: 0.8674481514878268
snr: 14 acc: 0.8472095150960659
snr: 16 acc: 0.8457987072945522
snr: 18 acc: 0.8487018800358102
acc_mean:  0.7828368006082933
ASR
[0.12583412774070543, 0.09925093632958802, 0.09162995594713656, 0.09366391184573003, 0.10476190476190476, 0.09800362976406533, 0.09107303877366997, 0.07593778591033852, 0.09695290858725762, 0.09221128021486123, 0.09693194798752575]
CA
[0.5805529075309819, 0.6835205992509363, 0.73568281938326, 0.7943067033976124, 0.812987012987013, 0.8121597096188747, 0.8674481514878268, 0.8472095150960659, 0.8457987072945522, 0.8487018800358102, 0.7828368006082933]
