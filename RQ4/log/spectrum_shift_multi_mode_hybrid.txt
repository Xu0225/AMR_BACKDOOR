channels_last
Model: "sequential"
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
reshape (Reshape)            (None, 1, 128, 2)         0         
_________________________________________________________________
zero_padding2d (ZeroPadding2 (None, 1, 132, 2)         0         
_________________________________________________________________
conv2d (Conv2D)              (None, 1, 132, 256)       3328      
_________________________________________________________________
dropout (Dropout)            (None, 1, 132, 256)       0         
_________________________________________________________________
zero_padding2d_1 (ZeroPaddin (None, 1, 136, 256)       0         
_________________________________________________________________
conv2d_1 (Conv2D)            (None, 1, 136, 80)        122960    
_________________________________________________________________
dropout_1 (Dropout)          (None, 1, 136, 80)        0         
_________________________________________________________________
flatten (Flatten)            (None, 10880)             0         
_________________________________________________________________
dense1 (Dense)               (None, 256)               2785536   
_________________________________________________________________
dropout_2 (Dropout)          (None, 256)               0         
_________________________________________________________________
dense2 (Dense)               (None, 11)                2827      
_________________________________________________________________
activation (Activation)      (None, 11)                0         
_________________________________________________________________
reshape_1 (Reshape)          (None, 11)                0         
=================================================================
Total params: 2,914,651
Trainable params: 2,914,651
Non-trainable params: 0
_________________________________________________________________
channels_last
110000
[0, 2, 4, 6, 8, 10, 12, 14, 16, 18]
['8PSK', 'AM-DSB', 'AM-SSB', 'BPSK', 'CPFSK', 'GFSK', 'PAM4', 'QAM16', 'QAM64', 'QPSK', 'WBFM']
Poisoned training data.
Evaluate ASR on poisoned test data.
Model: "model_1"
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
input (InputLayer)           [(None, 2, 128, 1)]       0         
_________________________________________________________________
conv1 (Conv2D)               (None, 2, 128, 256)       4352      
_________________________________________________________________
max_pooling2d (MaxPooling2D) (None, 2, 64, 256)        0         
_________________________________________________________________
dropout_9 (Dropout)          (None, 2, 64, 256)        0         
_________________________________________________________________
conv2 (Conv2D)               (None, 2, 64, 128)        524416    
_________________________________________________________________
max_pooling2d_1 (MaxPooling2 (None, 2, 32, 128)        0         
_________________________________________________________________
dropout_10 (Dropout)         (None, 2, 32, 128)        0         
_________________________________________________________________
conv3 (Conv2D)               (None, 2, 32, 64)         131136    
_________________________________________________________________
max_pooling2d_2 (MaxPooling2 (None, 2, 16, 64)         0         
_________________________________________________________________
dropout_11 (Dropout)         (None, 2, 16, 64)         0         
_________________________________________________________________
conv4 (Conv2D)               (None, 2, 16, 64)         65600     
_________________________________________________________________
max_pooling2d_3 (MaxPooling2 (None, 2, 8, 64)          0         
_________________________________________________________________
dropout_12 (Dropout)         (None, 2, 8, 64)          0         
_________________________________________________________________
flatten_2 (Flatten)          (None, 1024)              0         
_________________________________________________________________
dense1 (Dense)               (None, 128)               131200    
_________________________________________________________________
dense2 (Dense)               (None, 11)                1419      
=================================================================
Total params: 858,123
Trainable params: 858,123
Non-trainable params: 0
_________________________________________________________________
None
Model: "sequential_2"
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
conv2d_4 (Conv2D)            (None, 71, 71, 32)        2432      
_________________________________________________________________
max_pooling2d_2 (MaxPooling2 (None, 35, 35, 32)        0         
_________________________________________________________________
conv2d_5 (Conv2D)            (None, 31, 31, 64)        51264     
_________________________________________________________________
max_pooling2d_3 (MaxPooling2 (None, 15, 15, 64)        0         
_________________________________________________________________
flatten_2 (Flatten)          (None, 14400)             0         
_________________________________________________________________
dense_5 (Dense)              (None, 1000)              14401000  
_________________________________________________________________
dense_6 (Dense)              (None, 11)                11011     
=================================================================
Total params: 14,465,707
Trainable params: 14,465,707
Non-trainable params: 0
_________________________________________________________________
None
Train on 88011 samples, validate on 10989 samples
Epoch 1/20

 1024/88011 [..............................] - ETA: 5s - loss: 10.5241 - accuracy: 0.0801
10240/88011 [==>...........................] - ETA: 0s - loss: 5.3902 - accuracy: 0.3249 
19456/88011 [=====>........................] - ETA: 0s - loss: 3.5484 - accuracy: 0.4400
28672/88011 [========>.....................] - ETA: 0s - loss: 2.7603 - accuracy: 0.5012
37888/88011 [===========>..................] - ETA: 0s - loss: 2.3036 - accuracy: 0.5459
47104/88011 [===============>..............] - ETA: 0s - loss: 2.0038 - accuracy: 0.5799
56320/88011 [==================>...........] - ETA: 0s - loss: 1.7893 - accuracy: 0.6058
65536/88011 [=====================>........] - ETA: 0s - loss: 1.6293 - accuracy: 0.6266
74752/88011 [========================>.....] - ETA: 0s - loss: 1.5011 - accuracy: 0.6458
82944/88011 [===========================>..] - ETA: 0s - loss: 1.4091 - accuracy: 0.6600
Epoch 00001: val_loss improved from inf to 0.56661, saving model to VGG_dr0.5.h5

88011/88011 [==============================] - 1s 11us/sample - loss: 1.3601 - accuracy: 0.6681 - val_loss: 0.5666 - val_accuracy: 0.7747
Epoch 2/20

 1024/88011 [..............................] - ETA: 0s - loss: 0.5056 - accuracy: 0.8223
10240/88011 [==>...........................] - ETA: 0s - loss: 0.4857 - accuracy: 0.8302
18432/88011 [=====>........................] - ETA: 0s - loss: 0.4768 - accuracy: 0.8343
27648/88011 [========>.....................] - ETA: 0s - loss: 0.4596 - accuracy: 0.8413
35840/88011 [===========>..................] - ETA: 0s - loss: 0.4504 - accuracy: 0.8447
45056/88011 [==============>...............] - ETA: 0s - loss: 0.4431 - accuracy: 0.8477
53248/88011 [=================>............] - ETA: 0s - loss: 0.4352 - accuracy: 0.8512
61440/88011 [===================>..........] - ETA: 0s - loss: 0.4261 - accuracy: 0.8548
69632/88011 [======================>.......] - ETA: 0s - loss: 0.4194 - accuracy: 0.8575
77824/88011 [=========================>....] - ETA: 0s - loss: 0.4119 - accuracy: 0.8604
86016/88011 [============================>.] - ETA: 0s - loss: 0.4035 - accuracy: 0.8636
Epoch 00002: val_loss improved from 0.56661 to 0.47545, saving model to VGG_dr0.5.h5

88011/88011 [==============================] - 1s 7us/sample - loss: 0.4024 - accuracy: 0.8641 - val_loss: 0.4755 - val_accuracy: 0.8215
Epoch 3/20

 1024/88011 [..............................] - ETA: 0s - loss: 0.2652 - accuracy: 0.9092
 9216/88011 [==>...........................] - ETA: 0s - loss: 0.2832 - accuracy: 0.9081
17408/88011 [====>.........................] - ETA: 0s - loss: 0.2794 - accuracy: 0.9118
25600/88011 [=======>......................] - ETA: 0s - loss: 0.2743 - accuracy: 0.9133
33792/88011 [==========>...................] - ETA: 0s - loss: 0.2699 - accuracy: 0.9147
41984/88011 [=============>................] - ETA: 0s - loss: 0.2669 - accuracy: 0.9151
50176/88011 [================>.............] - ETA: 0s - loss: 0.2652 - accuracy: 0.9157
58368/88011 [==================>...........] - ETA: 0s - loss: 0.2642 - accuracy: 0.9159
66560/88011 [=====================>........] - ETA: 0s - loss: 0.2617 - accuracy: 0.9169
75776/88011 [========================>.....] - ETA: 0s - loss: 0.2592 - accuracy: 0.9175
84992/88011 [===========================>..] - ETA: 0s - loss: 0.2565 - accuracy: 0.9181
Epoch 00003: val_loss did not improve from 0.47545

88011/88011 [==============================] - 1s 7us/sample - loss: 0.2561 - accuracy: 0.9182 - val_loss: 0.5042 - val_accuracy: 0.8239
Epoch 4/20

 1024/88011 [..............................] - ETA: 0s - loss: 0.1722 - accuracy: 0.9482
 9216/88011 [==>...........................] - ETA: 0s - loss: 0.1871 - accuracy: 0.9443
17408/88011 [====>.........................] - ETA: 0s - loss: 0.1850 - accuracy: 0.9442
25600/88011 [=======>......................] - ETA: 0s - loss: 0.1812 - accuracy: 0.9450
33792/88011 [==========>...................] - ETA: 0s - loss: 0.1801 - accuracy: 0.9447
41984/88011 [=============>................] - ETA: 0s - loss: 0.1808 - accuracy: 0.9442
50176/88011 [================>.............] - ETA: 0s - loss: 0.1809 - accuracy: 0.9437
58368/88011 [==================>...........] - ETA: 0s - loss: 0.1823 - accuracy: 0.9430
66560/88011 [=====================>........] - ETA: 0s - loss: 0.1840 - accuracy: 0.9424
74752/88011 [========================>.....] - ETA: 0s - loss: 0.1862 - accuracy: 0.9413
82944/88011 [===========================>..] - ETA: 0s - loss: 0.1859 - accuracy: 0.9412
Epoch 00004: val_loss did not improve from 0.47545

88011/88011 [==============================] - 1s 7us/sample - loss: 0.1857 - accuracy: 0.9412 - val_loss: 0.5414 - val_accuracy: 0.8239
Epoch 5/20

 1024/88011 [..............................] - ETA: 0s - loss: 0.1418 - accuracy: 0.9609
 9216/88011 [==>...........................] - ETA: 0s - loss: 0.1443 - accuracy: 0.9524
17408/88011 [====>.........................] - ETA: 0s - loss: 0.1376 - accuracy: 0.9555
25600/88011 [=======>......................] - ETA: 0s - loss: 0.1358 - accuracy: 0.9562
33792/88011 [==========>...................] - ETA: 0s - loss: 0.1370 - accuracy: 0.9564
41984/88011 [=============>................] - ETA: 0s - loss: 0.1368 - accuracy: 0.9566
51200/88011 [================>.............] - ETA: 0s - loss: 0.1358 - accuracy: 0.9571
59392/88011 [===================>..........] - ETA: 0s - loss: 0.1341 - accuracy: 0.9576
67584/88011 [======================>.......] - ETA: 0s - loss: 0.1351 - accuracy: 0.9572
75776/88011 [========================>.....] - ETA: 0s - loss: 0.1361 - accuracy: 0.9567
83968/88011 [===========================>..] - ETA: 0s - loss: 0.1364 - accuracy: 0.9567
Epoch 00005: val_loss did not improve from 0.47545

88011/88011 [==============================] - 1s 7us/sample - loss: 0.1368 - accuracy: 0.9564 - val_loss: 0.6236 - val_accuracy: 0.8122
Epoch 6/20

 1024/88011 [..............................] - ETA: 0s - loss: 0.1226 - accuracy: 0.9658
 9216/88011 [==>...........................] - ETA: 0s - loss: 0.1133 - accuracy: 0.9654
17408/88011 [====>.........................] - ETA: 0s - loss: 0.1100 - accuracy: 0.9655
25600/88011 [=======>......................] - ETA: 0s - loss: 0.1080 - accuracy: 0.9657
33792/88011 [==========>...................] - ETA: 0s - loss: 0.1054 - accuracy: 0.9665
41984/88011 [=============>................] - ETA: 0s - loss: 0.1068 - accuracy: 0.9659
50176/88011 [================>.............] - ETA: 0s - loss: 0.1078 - accuracy: 0.9658
58368/88011 [==================>...........] - ETA: 0s - loss: 0.1075 - accuracy: 0.9656
66560/88011 [=====================>........] - ETA: 0s - loss: 0.1074 - accuracy: 0.9656
75776/88011 [========================>.....] - ETA: 0s - loss: 0.1084 - accuracy: 0.9652
83968/88011 [===========================>..] - ETA: 0s - loss: 0.1085 - accuracy: 0.9649
Epoch 00006: val_loss did not improve from 0.47545

88011/88011 [==============================] - 1s 7us/sample - loss: 0.1084 - accuracy: 0.9649 - val_loss: 0.6773 - val_accuracy: 0.8112
Epoch 7/20

 1024/88011 [..............................] - ETA: 0s - loss: 0.0899 - accuracy: 0.9736
 9216/88011 [==>...........................] - ETA: 0s - loss: 0.0741 - accuracy: 0.9797
18432/88011 [=====>........................] - ETA: 0s - loss: 0.0754 - accuracy: 0.9772
27648/88011 [========>.....................] - ETA: 0s - loss: 0.0765 - accuracy: 0.9767
35840/88011 [===========>..................] - ETA: 0s - loss: 0.0769 - accuracy: 0.9764
44032/88011 [==============>...............] - ETA: 0s - loss: 0.0789 - accuracy: 0.9754
52224/88011 [================>.............] - ETA: 0s - loss: 0.0796 - accuracy: 0.9750
60416/88011 [===================>..........] - ETA: 0s - loss: 0.0795 - accuracy: 0.9751
68608/88011 [======================>.......] - ETA: 0s - loss: 0.0801 - accuracy: 0.9747
76800/88011 [=========================>....] - ETA: 0s - loss: 0.0808 - accuracy: 0.9745
84992/88011 [===========================>..] - ETA: 0s - loss: 0.0810 - accuracy: 0.9744
Epoch 00007: val_loss did not improve from 0.47545

88011/88011 [==============================] - 1s 7us/sample - loss: 0.0809 - accuracy: 0.9745 - val_loss: 0.7140 - val_accuracy: 0.8110
Epoch 8/20

 1024/88011 [..............................] - ETA: 0s - loss: 0.0706 - accuracy: 0.9775
 9216/88011 [==>...........................] - ETA: 0s - loss: 0.0567 - accuracy: 0.9837
17408/88011 [====>.........................] - ETA: 0s - loss: 0.0583 - accuracy: 0.9831
25600/88011 [=======>......................] - ETA: 0s - loss: 0.0578 - accuracy: 0.9834
33792/88011 [==========>...................] - ETA: 0s - loss: 0.0581 - accuracy: 0.9830
41984/88011 [=============>................] - ETA: 0s - loss: 0.0582 - accuracy: 0.9830
50176/88011 [================>.............] - ETA: 0s - loss: 0.0585 - accuracy: 0.9828
58368/88011 [==================>...........] - ETA: 0s - loss: 0.0590 - accuracy: 0.9826
66560/88011 [=====================>........] - ETA: 0s - loss: 0.0591 - accuracy: 0.9824
74752/88011 [========================>.....] - ETA: 0s - loss: 0.0586 - accuracy: 0.9824
82944/88011 [===========================>..] - ETA: 0s - loss: 0.0582 - accuracy: 0.9823
Epoch 00008: val_loss did not improve from 0.47545

88011/88011 [==============================] - 1s 7us/sample - loss: 0.0588 - accuracy: 0.9821 - val_loss: 0.7808 - val_accuracy: 0.8065
Epoch 9/20

 1024/88011 [..............................] - ETA: 0s - loss: 0.0450 - accuracy: 0.9893
 9216/88011 [==>...........................] - ETA: 0s - loss: 0.0463 - accuracy: 0.9886
17408/88011 [====>.........................] - ETA: 0s - loss: 0.0444 - accuracy: 0.9881
25600/88011 [=======>......................] - ETA: 0s - loss: 0.0450 - accuracy: 0.9880
33792/88011 [==========>...................] - ETA: 0s - loss: 0.0455 - accuracy: 0.9874
41984/88011 [=============>................] - ETA: 0s - loss: 0.0473 - accuracy: 0.9868
50176/88011 [================>.............] - ETA: 0s - loss: 0.0479 - accuracy: 0.9863
58368/88011 [==================>...........] - ETA: 0s - loss: 0.0488 - accuracy: 0.9861
67584/88011 [======================>.......] - ETA: 0s - loss: 0.0503 - accuracy: 0.9856
75776/88011 [========================>.....] - ETA: 0s - loss: 0.0502 - accuracy: 0.9856
83968/88011 [===========================>..] - ETA: 0s - loss: 0.0498 - accuracy: 0.9856
Epoch 00009: val_loss did not improve from 0.47545

88011/88011 [==============================] - 1s 7us/sample - loss: 0.0494 - accuracy: 0.9858 - val_loss: 0.8310 - val_accuracy: 0.8095
Epoch 10/20

 1024/88011 [..............................] - ETA: 0s - loss: 0.0348 - accuracy: 0.9941
 9216/88011 [==>...........................] - ETA: 0s - loss: 0.0307 - accuracy: 0.9937
17408/88011 [====>.........................] - ETA: 0s - loss: 0.0313 - accuracy: 0.9925
25600/88011 [=======>......................] - ETA: 0s - loss: 0.0308 - accuracy: 0.9925
33792/88011 [==========>...................] - ETA: 0s - loss: 0.0305 - accuracy: 0.9922
41984/88011 [=============>................] - ETA: 0s - loss: 0.0308 - accuracy: 0.9922
50176/88011 [================>.............] - ETA: 0s - loss: 0.0316 - accuracy: 0.9919
58368/88011 [==================>...........] - ETA: 0s - loss: 0.0318 - accuracy: 0.9918
66560/88011 [=====================>........] - ETA: 0s - loss: 0.0325 - accuracy: 0.9914
74752/88011 [========================>.....] - ETA: 0s - loss: 0.0327 - accuracy: 0.9915
82944/88011 [===========================>..] - ETA: 0s - loss: 0.0332 - accuracy: 0.9913
Epoch 00010: val_loss did not improve from 0.47545

88011/88011 [==============================] - 1s 7us/sample - loss: 0.0333 - accuracy: 0.9913 - val_loss: 0.8954 - val_accuracy: 0.8102
Epoch 11/20

 1024/88011 [..............................] - ETA: 0s - loss: 0.0294 - accuracy: 0.9932
 9216/88011 [==>...........................] - ETA: 0s - loss: 0.0240 - accuracy: 0.9953
17408/88011 [====>.........................] - ETA: 0s - loss: 0.0236 - accuracy: 0.9948
25600/88011 [=======>......................] - ETA: 0s - loss: 0.0244 - accuracy: 0.9946
33792/88011 [==========>...................] - ETA: 0s - loss: 0.0245 - accuracy: 0.9943
41984/88011 [=============>................] - ETA: 0s - loss: 0.0249 - accuracy: 0.9942
50176/88011 [================>.............] - ETA: 0s - loss: 0.0246 - accuracy: 0.9941
58368/88011 [==================>...........] - ETA: 0s - loss: 0.0249 - accuracy: 0.9940
66560/88011 [=====================>........] - ETA: 0s - loss: 0.0256 - accuracy: 0.9938
74752/88011 [========================>.....] - ETA: 0s - loss: 0.0256 - accuracy: 0.9936
82944/88011 [===========================>..] - ETA: 0s - loss: 0.0257 - accuracy: 0.9935
Epoch 00011: val_loss did not improve from 0.47545

88011/88011 [==============================] - 1s 7us/sample - loss: 0.0258 - accuracy: 0.9934 - val_loss: 0.9802 - val_accuracy: 0.8044
Epoch 12/20

 1024/88011 [..............................] - ETA: 0s - loss: 0.0133 - accuracy: 0.9951
 9216/88011 [==>...........................] - ETA: 0s - loss: 0.0191 - accuracy: 0.9950
17408/88011 [====>.........................] - ETA: 0s - loss: 0.0195 - accuracy: 0.9953
25600/88011 [=======>......................] - ETA: 0s - loss: 0.0185 - accuracy: 0.9957
33792/88011 [==========>...................] - ETA: 0s - loss: 0.0180 - accuracy: 0.9959
41984/88011 [=============>................] - ETA: 0s - loss: 0.0178 - accuracy: 0.9959
50176/88011 [================>.............] - ETA: 0s - loss: 0.0178 - accuracy: 0.9958
58368/88011 [==================>...........] - ETA: 0s - loss: 0.0183 - accuracy: 0.9956
66560/88011 [=====================>........] - ETA: 0s - loss: 0.0186 - accuracy: 0.9955
74752/88011 [========================>.....] - ETA: 0s - loss: 0.0189 - accuracy: 0.9954
82944/88011 [===========================>..] - ETA: 0s - loss: 0.0197 - accuracy: 0.9951
Epoch 00012: val_loss did not improve from 0.47545

Epoch 00012: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.

88011/88011 [==============================] - 1s 8us/sample - loss: 0.0201 - accuracy: 0.9949 - val_loss: 1.0202 - val_accuracy: 0.8031
Epoch 13/20

 1024/88011 [..............................] - ETA: 0s - loss: 0.0191 - accuracy: 0.9941
 9216/88011 [==>...........................] - ETA: 0s - loss: 0.0168 - accuracy: 0.9958
17408/88011 [====>.........................] - ETA: 0s - loss: 0.0152 - accuracy: 0.9965
25600/88011 [=======>......................] - ETA: 0s - loss: 0.0149 - accuracy: 0.9966
33792/88011 [==========>...................] - ETA: 0s - loss: 0.0140 - accuracy: 0.9969
41984/88011 [=============>................] - ETA: 0s - loss: 0.0135 - accuracy: 0.9970
50176/88011 [================>.............] - ETA: 0s - loss: 0.0129 - accuracy: 0.9971
58368/88011 [==================>...........] - ETA: 0s - loss: 0.0128 - accuracy: 0.9970
66560/88011 [=====================>........] - ETA: 0s - loss: 0.0125 - accuracy: 0.9971
74752/88011 [========================>.....] - ETA: 0s - loss: 0.0122 - accuracy: 0.9972
82944/88011 [===========================>..] - ETA: 0s - loss: 0.0121 - accuracy: 0.9972
Epoch 00013: val_loss did not improve from 0.47545

88011/88011 [==============================] - 1s 7us/sample - loss: 0.0122 - accuracy: 0.9972 - val_loss: 1.0342 - val_accuracy: 0.8080
Epoch 14/20

 1024/88011 [..............................] - ETA: 0s - loss: 0.0059 - accuracy: 0.9990
 9216/88011 [==>...........................] - ETA: 0s - loss: 0.0082 - accuracy: 0.9982
17408/88011 [====>.........................] - ETA: 0s - loss: 0.0089 - accuracy: 0.9980
25600/88011 [=======>......................] - ETA: 0s - loss: 0.0089 - accuracy: 0.9978
33792/88011 [==========>...................] - ETA: 0s - loss: 0.0091 - accuracy: 0.9977
41984/88011 [=============>................] - ETA: 0s - loss: 0.0090 - accuracy: 0.9977
50176/88011 [================>.............] - ETA: 0s - loss: 0.0089 - accuracy: 0.9979
58368/88011 [==================>...........] - ETA: 0s - loss: 0.0090 - accuracy: 0.9978
66560/88011 [=====================>........] - ETA: 0s - loss: 0.0090 - accuracy: 0.9978
74752/88011 [========================>.....] - ETA: 0s - loss: 0.0090 - accuracy: 0.9978
82944/88011 [===========================>..] - ETA: 0s - loss: 0.0089 - accuracy: 0.9979
Epoch 00014: val_loss did not improve from 0.47545

88011/88011 [==============================] - 1s 7us/sample - loss: 0.0088 - accuracy: 0.9979 - val_loss: 1.0541 - val_accuracy: 0.8088
Epoch 15/20

 1024/88011 [..............................] - ETA: 0s - loss: 0.0047 - accuracy: 1.0000
 9216/88011 [==>...........................] - ETA: 0s - loss: 0.0065 - accuracy: 0.9987
17408/88011 [====>.........................] - ETA: 0s - loss: 0.0062 - accuracy: 0.9990
25600/88011 [=======>......................] - ETA: 0s - loss: 0.0060 - accuracy: 0.9992
33792/88011 [==========>...................] - ETA: 0s - loss: 0.0061 - accuracy: 0.9991
41984/88011 [=============>................] - ETA: 0s - loss: 0.0063 - accuracy: 0.9990
50176/88011 [================>.............] - ETA: 0s - loss: 0.0065 - accuracy: 0.9989
58368/88011 [==================>...........] - ETA: 0s - loss: 0.0067 - accuracy: 0.9988
66560/88011 [=====================>........] - ETA: 0s - loss: 0.0069 - accuracy: 0.9988
74752/88011 [========================>.....] - ETA: 0s - loss: 0.0070 - accuracy: 0.9987
82944/88011 [===========================>..] - ETA: 0s - loss: 0.0069 - accuracy: 0.9987
Epoch 00015: val_loss did not improve from 0.47545

88011/88011 [==============================] - 1s 7us/sample - loss: 0.0069 - accuracy: 0.9987 - val_loss: 1.0795 - val_accuracy: 0.8088
Epoch 16/20

 1024/88011 [..............................] - ETA: 0s - loss: 0.0084 - accuracy: 0.9990
 9216/88011 [==>...........................] - ETA: 0s - loss: 0.0061 - accuracy: 0.9990
17408/88011 [====>.........................] - ETA: 0s - loss: 0.0060 - accuracy: 0.9990
25600/88011 [=======>......................] - ETA: 0s - loss: 0.0060 - accuracy: 0.9991
33792/88011 [==========>...................] - ETA: 0s - loss: 0.0059 - accuracy: 0.9991
41984/88011 [=============>................] - ETA: 0s - loss: 0.0058 - accuracy: 0.9991
50176/88011 [================>.............] - ETA: 0s - loss: 0.0060 - accuracy: 0.9991
58368/88011 [==================>...........] - ETA: 0s - loss: 0.0060 - accuracy: 0.9990
66560/88011 [=====================>........] - ETA: 0s - loss: 0.0062 - accuracy: 0.9989
74752/88011 [========================>.....] - ETA: 0s - loss: 0.0063 - accuracy: 0.9989
82944/88011 [===========================>..] - ETA: 0s - loss: 0.0065 - accuracy: 0.9988
Epoch 00016: val_loss did not improve from 0.47545

88011/88011 [==============================] - 1s 7us/sample - loss: 0.0065 - accuracy: 0.9988 - val_loss: 1.1022 - val_accuracy: 0.8075
Epoch 17/20

 1024/88011 [..............................] - ETA: 0s - loss: 0.0061 - accuracy: 0.9990
 9216/88011 [==>...........................] - ETA: 0s - loss: 0.0054 - accuracy: 0.9990
17408/88011 [====>.........................] - ETA: 0s - loss: 0.0055 - accuracy: 0.9991
25600/88011 [=======>......................] - ETA: 0s - loss: 0.0056 - accuracy: 0.9992
33792/88011 [==========>...................] - ETA: 0s - loss: 0.0057 - accuracy: 0.9990
41984/88011 [=============>................] - ETA: 0s - loss: 0.0054 - accuracy: 0.9990
50176/88011 [================>.............] - ETA: 0s - loss: 0.0054 - accuracy: 0.9991
58368/88011 [==================>...........] - ETA: 0s - loss: 0.0056 - accuracy: 0.9990
66560/88011 [=====================>........] - ETA: 0s - loss: 0.0056 - accuracy: 0.9989
74752/88011 [========================>.....] - ETA: 0s - loss: 0.0057 - accuracy: 0.9989
82944/88011 [===========================>..] - ETA: 0s - loss: 0.0060 - accuracy: 0.9988
Epoch 00017: val_loss did not improve from 0.47545

88011/88011 [==============================] - 1s 7us/sample - loss: 0.0061 - accuracy: 0.9988 - val_loss: 1.1308 - val_accuracy: 0.8082
Epoch 18/20

 1024/88011 [..............................] - ETA: 0s - loss: 0.0034 - accuracy: 1.0000
 9216/88011 [==>...........................] - ETA: 0s - loss: 0.0062 - accuracy: 0.9980
17408/88011 [====>.........................] - ETA: 0s - loss: 0.0068 - accuracy: 0.9982
25600/88011 [=======>......................] - ETA: 0s - loss: 0.0061 - accuracy: 0.9986
33792/88011 [==========>...................] - ETA: 0s - loss: 0.0057 - accuracy: 0.9988
41984/88011 [=============>................] - ETA: 0s - loss: 0.0054 - accuracy: 0.9989
51200/88011 [================>.............] - ETA: 0s - loss: 0.0053 - accuracy: 0.9989
59392/88011 [===================>..........] - ETA: 0s - loss: 0.0054 - accuracy: 0.9990
67584/88011 [======================>.......] - ETA: 0s - loss: 0.0053 - accuracy: 0.9990
75776/88011 [========================>.....] - ETA: 0s - loss: 0.0054 - accuracy: 0.9990
83968/88011 [===========================>..] - ETA: 0s - loss: 0.0054 - accuracy: 0.9990
Epoch 00018: val_loss did not improve from 0.47545

88011/88011 [==============================] - 1s 7us/sample - loss: 0.0054 - accuracy: 0.9990 - val_loss: 1.1417 - val_accuracy: 0.8077
Epoch 19/20

 1024/88011 [..............................] - ETA: 0s - loss: 0.0042 - accuracy: 0.9990
 9216/88011 [==>...........................] - ETA: 0s - loss: 0.0049 - accuracy: 0.9989
17408/88011 [====>.........................] - ETA: 0s - loss: 0.0055 - accuracy: 0.9983
25600/88011 [=======>......................] - ETA: 0s - loss: 0.0061 - accuracy: 0.9981
33792/88011 [==========>...................] - ETA: 0s - loss: 0.0067 - accuracy: 0.9978
41984/88011 [=============>................] - ETA: 0s - loss: 0.0068 - accuracy: 0.9977
50176/88011 [================>.............] - ETA: 0s - loss: 0.0070 - accuracy: 0.9977
58368/88011 [==================>...........] - ETA: 0s - loss: 0.0068 - accuracy: 0.9978
66560/88011 [=====================>........] - ETA: 0s - loss: 0.0066 - accuracy: 0.9979
74752/88011 [========================>.....] - ETA: 0s - loss: 0.0064 - accuracy: 0.9980
82944/88011 [===========================>..] - ETA: 0s - loss: 0.0064 - accuracy: 0.9980
Epoch 00019: val_loss did not improve from 0.47545

88011/88011 [==============================] - 1s 7us/sample - loss: 0.0065 - accuracy: 0.9981 - val_loss: 1.1588 - val_accuracy: 0.8054
Epoch 20/20

 1024/88011 [..............................] - ETA: 0s - loss: 0.0040 - accuracy: 1.0000
 9216/88011 [==>...........................] - ETA: 0s - loss: 0.0035 - accuracy: 0.9996
17408/88011 [====>.........................] - ETA: 0s - loss: 0.0037 - accuracy: 0.9994
25600/88011 [=======>......................] - ETA: 0s - loss: 0.0038 - accuracy: 0.9993
33792/88011 [==========>...................] - ETA: 0s - loss: 0.0041 - accuracy: 0.9992
41984/88011 [=============>................] - ETA: 0s - loss: 0.0042 - accuracy: 0.9991
50176/88011 [================>.............] - ETA: 0s - loss: 0.0045 - accuracy: 0.9989
58368/88011 [==================>...........] - ETA: 0s - loss: 0.0047 - accuracy: 0.9988
66560/88011 [=====================>........] - ETA: 0s - loss: 0.0049 - accuracy: 0.9988
74752/88011 [========================>.....] - ETA: 0s - loss: 0.0049 - accuracy: 0.9988
82944/88011 [===========================>..] - ETA: 0s - loss: 0.0048 - accuracy: 0.9989
Epoch 00020: val_loss did not improve from 0.47545

88011/88011 [==============================] - 1s 7us/sample - loss: 0.0049 - accuracy: 0.9988 - val_loss: 1.1886 - val_accuracy: 0.8069
ASR
snr: 0 acc: 0.1325071496663489
snr: 2 acc: 0.09269662921348315
snr: 4 acc: 0.10308370044052863
snr: 6 acc: 0.1184573002754821
snr: 8 acc: 0.1038961038961039
snr: 10 acc: 0.10435571687840291
snr: 12 acc: 0.1018935978358882
snr: 14 acc: 0.11253430924062215
snr: 16 acc: 0.11819021237303785
snr: 18 acc: 0.10474485228290063
acc_mean:  0.10923595721027986
110000
[0, 2, 4, 6, 8, 10, 12, 14, 16, 18]
['8PSK', 'AM-DSB', 'AM-SSB', 'BPSK', 'CPFSK', 'GFSK', 'PAM4', 'QAM16', 'QAM64', 'QPSK', 'WBFM']
Model: "model_1"
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
input (InputLayer)           [(None, 2, 128, 1)]       0         
_________________________________________________________________
conv1 (Conv2D)               (None, 2, 128, 256)       4352      
_________________________________________________________________
max_pooling2d (MaxPooling2D) (None, 2, 64, 256)        0         
_________________________________________________________________
dropout_9 (Dropout)          (None, 2, 64, 256)        0         
_________________________________________________________________
conv2 (Conv2D)               (None, 2, 64, 128)        524416    
_________________________________________________________________
max_pooling2d_1 (MaxPooling2 (None, 2, 32, 128)        0         
_________________________________________________________________
dropout_10 (Dropout)         (None, 2, 32, 128)        0         
_________________________________________________________________
conv3 (Conv2D)               (None, 2, 32, 64)         131136    
_________________________________________________________________
max_pooling2d_2 (MaxPooling2 (None, 2, 16, 64)         0         
_________________________________________________________________
dropout_11 (Dropout)         (None, 2, 16, 64)         0         
_________________________________________________________________
conv4 (Conv2D)               (None, 2, 16, 64)         65600     
_________________________________________________________________
max_pooling2d_3 (MaxPooling2 (None, 2, 8, 64)          0         
_________________________________________________________________
dropout_12 (Dropout)         (None, 2, 8, 64)          0         
_________________________________________________________________
flatten_2 (Flatten)          (None, 1024)              0         
_________________________________________________________________
dense1 (Dense)               (None, 128)               131200    
_________________________________________________________________
dense2 (Dense)               (None, 11)                1419      
=================================================================
Total params: 858,123
Trainable params: 858,123
Non-trainable params: 0
_________________________________________________________________
None
Model: "sequential_2"
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
conv2d_4 (Conv2D)            (None, 71, 71, 32)        2432      
_________________________________________________________________
max_pooling2d_2 (MaxPooling2 (None, 35, 35, 32)        0         
_________________________________________________________________
conv2d_5 (Conv2D)            (None, 31, 31, 64)        51264     
_________________________________________________________________
max_pooling2d_3 (MaxPooling2 (None, 15, 15, 64)        0         
_________________________________________________________________
flatten_2 (Flatten)          (None, 14400)             0         
_________________________________________________________________
dense_5 (Dense)              (None, 1000)              14401000  
_________________________________________________________________
dense_6 (Dense)              (None, 11)                11011     
=================================================================
Total params: 14,465,707
Trainable params: 14,465,707
Non-trainable params: 0
_________________________________________________________________
None
CA
snr: 0 acc: 0.5891325071496664
snr: 2 acc: 0.7265917602996255
snr: 4 acc: 0.8
snr: 6 acc: 0.8264462809917356
snr: 8 acc: 0.8658008658008658
snr: 10 acc: 0.8611615245009074
snr: 12 acc: 0.8836789900811542
snr: 14 acc: 0.8655077767612077
snr: 16 acc: 0.8707294552169899
snr: 18 acc: 0.8818263205013429
acc_mean:  0.8170875481303496
ASR
[0.1325071496663489, 0.09269662921348315, 0.10308370044052863, 0.1184573002754821, 0.1038961038961039, 0.10435571687840291, 0.1018935978358882, 0.11253430924062215, 0.11819021237303785, 0.10474485228290063, 0.10923595721027986]
CA
[0.5891325071496664, 0.7265917602996255, 0.8, 0.8264462809917356, 0.8658008658008658, 0.8611615245009074, 0.8836789900811542, 0.8655077767612077, 0.8707294552169899, 0.8818263205013429, 0.8170875481303496]
