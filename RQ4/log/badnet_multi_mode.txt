channels_last
Model: "sequential"
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
reshape (Reshape)            (None, 1, 128, 2)         0         
_________________________________________________________________
zero_padding2d (ZeroPadding2 (None, 1, 132, 2)         0         
_________________________________________________________________
conv2d (Conv2D)              (None, 1, 132, 256)       3328      
_________________________________________________________________
dropout (Dropout)            (None, 1, 132, 256)       0         
_________________________________________________________________
zero_padding2d_1 (ZeroPaddin (None, 1, 136, 256)       0         
_________________________________________________________________
conv2d_1 (Conv2D)            (None, 1, 136, 80)        122960    
_________________________________________________________________
dropout_1 (Dropout)          (None, 1, 136, 80)        0         
_________________________________________________________________
flatten (Flatten)            (None, 10880)             0         
_________________________________________________________________
dense1 (Dense)               (None, 256)               2785536   
_________________________________________________________________
dropout_2 (Dropout)          (None, 256)               0         
_________________________________________________________________
dense2 (Dense)               (None, 11)                2827      
_________________________________________________________________
activation (Activation)      (None, 11)                0         
_________________________________________________________________
reshape_1 (Reshape)          (None, 11)                0         
=================================================================
Total params: 2,914,651
Trainable params: 2,914,651
Non-trainable params: 0
_________________________________________________________________
channels_last
110000
[0, 2, 4, 6, 8, 10, 12, 14, 16, 18]
['8PSK', 'AM-DSB', 'AM-SSB', 'BPSK', 'CPFSK', 'GFSK', 'PAM4', 'QAM16', 'QAM64', 'QPSK', 'WBFM']
Model: "model_1"
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
input (InputLayer)           [(None, 2, 128, 1)]       0         
_________________________________________________________________
conv1 (Conv2D)               (None, 2, 128, 256)       4352      
_________________________________________________________________
max_pooling2d (MaxPooling2D) (None, 2, 64, 256)        0         
_________________________________________________________________
dropout_9 (Dropout)          (None, 2, 64, 256)        0         
_________________________________________________________________
conv2 (Conv2D)               (None, 2, 64, 128)        524416    
_________________________________________________________________
max_pooling2d_1 (MaxPooling2 (None, 2, 32, 128)        0         
_________________________________________________________________
dropout_10 (Dropout)         (None, 2, 32, 128)        0         
_________________________________________________________________
conv3 (Conv2D)               (None, 2, 32, 64)         131136    
_________________________________________________________________
max_pooling2d_2 (MaxPooling2 (None, 2, 16, 64)         0         
_________________________________________________________________
dropout_11 (Dropout)         (None, 2, 16, 64)         0         
_________________________________________________________________
conv4 (Conv2D)               (None, 2, 16, 64)         65600     
_________________________________________________________________
max_pooling2d_3 (MaxPooling2 (None, 2, 8, 64)          0         
_________________________________________________________________
dropout_12 (Dropout)         (None, 2, 8, 64)          0         
_________________________________________________________________
flatten_2 (Flatten)          (None, 1024)              0         
_________________________________________________________________
dense1 (Dense)               (None, 128)               131200    
_________________________________________________________________
dense2 (Dense)               (None, 11)                1419      
=================================================================
Total params: 858,123
Trainable params: 858,123
Non-trainable params: 0
_________________________________________________________________
None
Model: "sequential_2"
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
conv2d_4 (Conv2D)            (None, 71, 71, 32)        2432      
_________________________________________________________________
max_pooling2d_2 (MaxPooling2 (None, 35, 35, 32)        0         
_________________________________________________________________
conv2d_5 (Conv2D)            (None, 31, 31, 64)        51264     
_________________________________________________________________
max_pooling2d_3 (MaxPooling2 (None, 15, 15, 64)        0         
_________________________________________________________________
flatten_2 (Flatten)          (None, 14400)             0         
_________________________________________________________________
dense_5 (Dense)              (None, 1000)              14401000  
_________________________________________________________________
dense_6 (Dense)              (None, 11)                11011     
=================================================================
Total params: 14,465,707
Trainable params: 14,465,707
Non-trainable params: 0
_________________________________________________________________
None
Train on 88011 samples, validate on 10989 samples
Epoch 1/20

 1024/88011 [..............................] - ETA: 6s - loss: 11.8275 - accuracy: 0.1025
 9216/88011 [==>...........................] - ETA: 1s - loss: 5.3962 - accuracy: 0.2996 
18432/88011 [=====>........................] - ETA: 0s - loss: 3.4542 - accuracy: 0.4315
26624/88011 [========>.....................] - ETA: 0s - loss: 2.7241 - accuracy: 0.4981
34816/88011 [==========>...................] - ETA: 0s - loss: 2.2937 - accuracy: 0.5377
43008/88011 [=============>................] - ETA: 0s - loss: 2.0041 - accuracy: 0.5683
51200/88011 [================>.............] - ETA: 0s - loss: 1.7956 - accuracy: 0.5935
59392/88011 [===================>..........] - ETA: 0s - loss: 1.6380 - accuracy: 0.6128
67584/88011 [======================>.......] - ETA: 0s - loss: 1.5130 - accuracy: 0.6296
75776/88011 [========================>.....] - ETA: 0s - loss: 1.4114 - accuracy: 0.6438
83968/88011 [===========================>..] - ETA: 0s - loss: 1.3269 - accuracy: 0.6568
Epoch 00001: val_loss improved from inf to 0.51877, saving model to VGG_dr0.5.h5

88011/88011 [==============================] - 1s 13us/sample - loss: 1.2906 - accuracy: 0.6626 - val_loss: 0.5188 - val_accuracy: 0.7917
Epoch 2/20

 1024/88011 [..............................] - ETA: 0s - loss: 0.4824 - accuracy: 0.8037
 9216/88011 [==>...........................] - ETA: 0s - loss: 0.4792 - accuracy: 0.8064
17408/88011 [====>.........................] - ETA: 0s - loss: 0.4693 - accuracy: 0.8089
25600/88011 [=======>......................] - ETA: 0s - loss: 0.4572 - accuracy: 0.8144
32768/88011 [==========>...................] - ETA: 0s - loss: 0.4485 - accuracy: 0.8176
40960/88011 [============>.................] - ETA: 0s - loss: 0.4390 - accuracy: 0.8219
49152/88011 [===============>..............] - ETA: 0s - loss: 0.4355 - accuracy: 0.8240
57344/88011 [==================>...........] - ETA: 0s - loss: 0.4297 - accuracy: 0.8258
65536/88011 [=====================>........] - ETA: 0s - loss: 0.4284 - accuracy: 0.8262
73728/88011 [========================>.....] - ETA: 0s - loss: 0.4256 - accuracy: 0.8275
81920/88011 [==========================>...] - ETA: 0s - loss: 0.4211 - accuracy: 0.8294
Epoch 00002: val_loss improved from 0.51877 to 0.39606, saving model to VGG_dr0.5.h5

88011/88011 [==============================] - 1s 8us/sample - loss: 0.4175 - accuracy: 0.8311 - val_loss: 0.3961 - val_accuracy: 0.8431
Epoch 3/20

 1024/88011 [..............................] - ETA: 0s - loss: 0.3111 - accuracy: 0.8818
 9216/88011 [==>...........................] - ETA: 0s - loss: 0.3227 - accuracy: 0.8766
17408/88011 [====>.........................] - ETA: 0s - loss: 0.3156 - accuracy: 0.8790
24576/88011 [=======>......................] - ETA: 0s - loss: 0.3129 - accuracy: 0.8783
32768/88011 [==========>...................] - ETA: 0s - loss: 0.3112 - accuracy: 0.8786
39936/88011 [============>.................] - ETA: 0s - loss: 0.3105 - accuracy: 0.8787
48128/88011 [===============>..............] - ETA: 0s - loss: 0.3109 - accuracy: 0.8789
56320/88011 [==================>...........] - ETA: 0s - loss: 0.3136 - accuracy: 0.8775
64512/88011 [====================>.........] - ETA: 0s - loss: 0.3131 - accuracy: 0.8773
72704/88011 [=======================>......] - ETA: 0s - loss: 0.3127 - accuracy: 0.8771
80896/88011 [==========================>...] - ETA: 0s - loss: 0.3124 - accuracy: 0.8771
Epoch 00003: val_loss improved from 0.39606 to 0.35658, saving model to VGG_dr0.5.h5

88011/88011 [==============================] - 1s 8us/sample - loss: 0.3124 - accuracy: 0.8771 - val_loss: 0.3566 - val_accuracy: 0.8604
Epoch 4/20

 1024/88011 [..............................] - ETA: 0s - loss: 0.2527 - accuracy: 0.9092
 9216/88011 [==>...........................] - ETA: 0s - loss: 0.2496 - accuracy: 0.9083
17408/88011 [====>.........................] - ETA: 0s - loss: 0.2499 - accuracy: 0.9072
24576/88011 [=======>......................] - ETA: 0s - loss: 0.2495 - accuracy: 0.9067
32768/88011 [==========>...................] - ETA: 0s - loss: 0.2485 - accuracy: 0.9061
40960/88011 [============>.................] - ETA: 0s - loss: 0.2504 - accuracy: 0.9055
49152/88011 [===============>..............] - ETA: 0s - loss: 0.2516 - accuracy: 0.9046
57344/88011 [==================>...........] - ETA: 0s - loss: 0.2520 - accuracy: 0.9041
65536/88011 [=====================>........] - ETA: 0s - loss: 0.2525 - accuracy: 0.9038
73728/88011 [========================>.....] - ETA: 0s - loss: 0.2530 - accuracy: 0.9034
81920/88011 [==========================>...] - ETA: 0s - loss: 0.2533 - accuracy: 0.9028
Epoch 00004: val_loss improved from 0.35658 to 0.34260, saving model to VGG_dr0.5.h5

88011/88011 [==============================] - 1s 8us/sample - loss: 0.2537 - accuracy: 0.9024 - val_loss: 0.3426 - val_accuracy: 0.8710
Epoch 5/20

 1024/88011 [..............................] - ETA: 0s - loss: 0.2341 - accuracy: 0.9160
 9216/88011 [==>...........................] - ETA: 0s - loss: 0.2056 - accuracy: 0.9240
17408/88011 [====>.........................] - ETA: 0s - loss: 0.2041 - accuracy: 0.9257
25600/88011 [=======>......................] - ETA: 0s - loss: 0.2056 - accuracy: 0.9241
33792/88011 [==========>...................] - ETA: 0s - loss: 0.2062 - accuracy: 0.9229
41984/88011 [=============>................] - ETA: 0s - loss: 0.2089 - accuracy: 0.9214
49152/88011 [===============>..............] - ETA: 0s - loss: 0.2101 - accuracy: 0.9208
57344/88011 [==================>...........] - ETA: 0s - loss: 0.2107 - accuracy: 0.9205
64512/88011 [====================>.........] - ETA: 0s - loss: 0.2118 - accuracy: 0.9199
72704/88011 [=======================>......] - ETA: 0s - loss: 0.2128 - accuracy: 0.9195
80896/88011 [==========================>...] - ETA: 0s - loss: 0.2137 - accuracy: 0.9190
Epoch 00005: val_loss improved from 0.34260 to 0.32914, saving model to VGG_dr0.5.h5

88011/88011 [==============================] - 1s 8us/sample - loss: 0.2144 - accuracy: 0.9188 - val_loss: 0.3291 - val_accuracy: 0.8707
Epoch 6/20

 1024/88011 [..............................] - ETA: 0s - loss: 0.1566 - accuracy: 0.9531
 9216/88011 [==>...........................] - ETA: 0s - loss: 0.1823 - accuracy: 0.9337
17408/88011 [====>.........................] - ETA: 0s - loss: 0.1804 - accuracy: 0.9335
25600/88011 [=======>......................] - ETA: 0s - loss: 0.1813 - accuracy: 0.9316
32768/88011 [==========>...................] - ETA: 0s - loss: 0.1803 - accuracy: 0.9320
40960/88011 [============>.................] - ETA: 0s - loss: 0.1814 - accuracy: 0.9313
49152/88011 [===============>..............] - ETA: 0s - loss: 0.1846 - accuracy: 0.9301
57344/88011 [==================>...........] - ETA: 0s - loss: 0.1842 - accuracy: 0.9299
65536/88011 [=====================>........] - ETA: 0s - loss: 0.1841 - accuracy: 0.9296
73728/88011 [========================>.....] - ETA: 0s - loss: 0.1844 - accuracy: 0.9294
81920/88011 [==========================>...] - ETA: 0s - loss: 0.1853 - accuracy: 0.9293
Epoch 00006: val_loss did not improve from 0.32914

88011/88011 [==============================] - 1s 8us/sample - loss: 0.1854 - accuracy: 0.9290 - val_loss: 0.3546 - val_accuracy: 0.8671
Epoch 7/20

 1024/88011 [..............................] - ETA: 0s - loss: 0.1513 - accuracy: 0.9463
 9216/88011 [==>...........................] - ETA: 0s - loss: 0.1513 - accuracy: 0.9462
17408/88011 [====>.........................] - ETA: 0s - loss: 0.1520 - accuracy: 0.9457
25600/88011 [=======>......................] - ETA: 0s - loss: 0.1502 - accuracy: 0.9461
33792/88011 [==========>...................] - ETA: 0s - loss: 0.1498 - accuracy: 0.9456
41984/88011 [=============>................] - ETA: 0s - loss: 0.1501 - accuracy: 0.9457
50176/88011 [================>.............] - ETA: 0s - loss: 0.1518 - accuracy: 0.9446
58368/88011 [==================>...........] - ETA: 0s - loss: 0.1525 - accuracy: 0.9438
66560/88011 [=====================>........] - ETA: 0s - loss: 0.1542 - accuracy: 0.9426
73728/88011 [========================>.....] - ETA: 0s - loss: 0.1550 - accuracy: 0.9420
80896/88011 [==========================>...] - ETA: 0s - loss: 0.1570 - accuracy: 0.9409
Epoch 00007: val_loss did not improve from 0.32914

88011/88011 [==============================] - 1s 8us/sample - loss: 0.1579 - accuracy: 0.9404 - val_loss: 0.3516 - val_accuracy: 0.8680
Epoch 8/20

 1024/88011 [..............................] - ETA: 0s - loss: 0.1210 - accuracy: 0.9580
 9216/88011 [==>...........................] - ETA: 0s - loss: 0.1283 - accuracy: 0.9550
17408/88011 [====>.........................] - ETA: 0s - loss: 0.1263 - accuracy: 0.9555
25600/88011 [=======>......................] - ETA: 0s - loss: 0.1300 - accuracy: 0.9532
33792/88011 [==========>...................] - ETA: 0s - loss: 0.1302 - accuracy: 0.9527
41984/88011 [=============>................] - ETA: 0s - loss: 0.1312 - accuracy: 0.9517
50176/88011 [================>.............] - ETA: 0s - loss: 0.1320 - accuracy: 0.9513
57344/88011 [==================>...........] - ETA: 0s - loss: 0.1337 - accuracy: 0.9508
64512/88011 [====================>.........] - ETA: 0s - loss: 0.1338 - accuracy: 0.9506
71680/88011 [=======================>......] - ETA: 0s - loss: 0.1342 - accuracy: 0.9500
79872/88011 [==========================>...] - ETA: 0s - loss: 0.1342 - accuracy: 0.9500
Epoch 00008: val_loss did not improve from 0.32914

88011/88011 [==============================] - 1s 8us/sample - loss: 0.1350 - accuracy: 0.9493 - val_loss: 0.3753 - val_accuracy: 0.8684
Epoch 9/20

 1024/88011 [..............................] - ETA: 0s - loss: 0.1072 - accuracy: 0.9580
 9216/88011 [==>...........................] - ETA: 0s - loss: 0.1123 - accuracy: 0.9607
17408/88011 [====>.........................] - ETA: 0s - loss: 0.1114 - accuracy: 0.9611
25600/88011 [=======>......................] - ETA: 0s - loss: 0.1096 - accuracy: 0.9613
32768/88011 [==========>...................] - ETA: 0s - loss: 0.1091 - accuracy: 0.9613
40960/88011 [============>.................] - ETA: 0s - loss: 0.1089 - accuracy: 0.9609
49152/88011 [===============>..............] - ETA: 0s - loss: 0.1091 - accuracy: 0.9609
57344/88011 [==================>...........] - ETA: 0s - loss: 0.1100 - accuracy: 0.9603
65536/88011 [=====================>........] - ETA: 0s - loss: 0.1111 - accuracy: 0.9599
73728/88011 [========================>.....] - ETA: 0s - loss: 0.1120 - accuracy: 0.9592
80896/88011 [==========================>...] - ETA: 0s - loss: 0.1119 - accuracy: 0.9591
Epoch 00009: val_loss did not improve from 0.32914

88011/88011 [==============================] - 1s 8us/sample - loss: 0.1126 - accuracy: 0.9586 - val_loss: 0.3731 - val_accuracy: 0.8731
Epoch 10/20

 1024/88011 [..............................] - ETA: 0s - loss: 0.0870 - accuracy: 0.9736
 8192/88011 [=>............................] - ETA: 0s - loss: 0.0892 - accuracy: 0.9695
15360/88011 [====>.........................] - ETA: 0s - loss: 0.0930 - accuracy: 0.9680
22528/88011 [======>.......................] - ETA: 0s - loss: 0.0916 - accuracy: 0.9684
30720/88011 [=========>....................] - ETA: 0s - loss: 0.0912 - accuracy: 0.9681
37888/88011 [===========>..................] - ETA: 0s - loss: 0.0925 - accuracy: 0.9670
46080/88011 [==============>...............] - ETA: 0s - loss: 0.0922 - accuracy: 0.9674
54272/88011 [=================>............] - ETA: 0s - loss: 0.0924 - accuracy: 0.9670
62464/88011 [====================>.........] - ETA: 0s - loss: 0.0930 - accuracy: 0.9666
70656/88011 [=======================>......] - ETA: 0s - loss: 0.0931 - accuracy: 0.9664
77824/88011 [=========================>....] - ETA: 0s - loss: 0.0936 - accuracy: 0.9661
86016/88011 [============================>.] - ETA: 0s - loss: 0.0937 - accuracy: 0.9660
Epoch 00010: val_loss did not improve from 0.32914

88011/88011 [==============================] - 1s 8us/sample - loss: 0.0937 - accuracy: 0.9660 - val_loss: 0.3896 - val_accuracy: 0.8696
Epoch 11/20

 1024/88011 [..............................] - ETA: 0s - loss: 0.0973 - accuracy: 0.9609
 8192/88011 [=>............................] - ETA: 0s - loss: 0.0765 - accuracy: 0.9709
16384/88011 [====>.........................] - ETA: 0s - loss: 0.0731 - accuracy: 0.9737
24576/88011 [=======>......................] - ETA: 0s - loss: 0.0720 - accuracy: 0.9738
32768/88011 [==========>...................] - ETA: 0s - loss: 0.0728 - accuracy: 0.9733
40960/88011 [============>.................] - ETA: 0s - loss: 0.0741 - accuracy: 0.9728
49152/88011 [===============>..............] - ETA: 0s - loss: 0.0743 - accuracy: 0.9724
57344/88011 [==================>...........] - ETA: 0s - loss: 0.0749 - accuracy: 0.9723
65536/88011 [=====================>........] - ETA: 0s - loss: 0.0760 - accuracy: 0.9718
72704/88011 [=======================>......] - ETA: 0s - loss: 0.0765 - accuracy: 0.9717
79872/88011 [==========================>...] - ETA: 0s - loss: 0.0766 - accuracy: 0.9719
87040/88011 [============================>.] - ETA: 0s - loss: 0.0776 - accuracy: 0.9715
Epoch 00011: val_loss did not improve from 0.32914

88011/88011 [==============================] - 1s 8us/sample - loss: 0.0774 - accuracy: 0.9715 - val_loss: 0.4108 - val_accuracy: 0.8693
Epoch 12/20

 1024/88011 [..............................] - ETA: 0s - loss: 0.0676 - accuracy: 0.9746
 8192/88011 [=>............................] - ETA: 0s - loss: 0.0604 - accuracy: 0.9811
15360/88011 [====>.........................] - ETA: 0s - loss: 0.0657 - accuracy: 0.9788
22528/88011 [======>.......................] - ETA: 0s - loss: 0.0679 - accuracy: 0.9770
30720/88011 [=========>....................] - ETA: 0s - loss: 0.0686 - accuracy: 0.9763
38912/88011 [============>.................] - ETA: 0s - loss: 0.0701 - accuracy: 0.9755
46080/88011 [==============>...............] - ETA: 0s - loss: 0.0709 - accuracy: 0.9750
54272/88011 [=================>............] - ETA: 0s - loss: 0.0707 - accuracy: 0.9745
62464/88011 [====================>.........] - ETA: 0s - loss: 0.0715 - accuracy: 0.9739
69632/88011 [======================>.......] - ETA: 0s - loss: 0.0723 - accuracy: 0.9736
77824/88011 [=========================>....] - ETA: 0s - loss: 0.0726 - accuracy: 0.9733
86016/88011 [============================>.] - ETA: 0s - loss: 0.0728 - accuracy: 0.9733
Epoch 00012: val_loss did not improve from 0.32914

88011/88011 [==============================] - 1s 8us/sample - loss: 0.0726 - accuracy: 0.9732 - val_loss: 0.4392 - val_accuracy: 0.8690
Epoch 13/20

 1024/88011 [..............................] - ETA: 0s - loss: 0.0585 - accuracy: 0.9775
 9216/88011 [==>...........................] - ETA: 0s - loss: 0.0598 - accuracy: 0.9783
17408/88011 [====>.........................] - ETA: 0s - loss: 0.0585 - accuracy: 0.9787
25600/88011 [=======>......................] - ETA: 0s - loss: 0.0566 - accuracy: 0.9795
32768/88011 [==========>...................] - ETA: 0s - loss: 0.0559 - accuracy: 0.9800
40960/88011 [============>.................] - ETA: 0s - loss: 0.0554 - accuracy: 0.9800
48128/88011 [===============>..............] - ETA: 0s - loss: 0.0553 - accuracy: 0.9801
56320/88011 [==================>...........] - ETA: 0s - loss: 0.0560 - accuracy: 0.9796
64512/88011 [====================>.........] - ETA: 0s - loss: 0.0567 - accuracy: 0.9791
72704/88011 [=======================>......] - ETA: 0s - loss: 0.0570 - accuracy: 0.9789
80896/88011 [==========================>...] - ETA: 0s - loss: 0.0581 - accuracy: 0.9784
Epoch 00013: val_loss did not improve from 0.32914

88011/88011 [==============================] - 1s 8us/sample - loss: 0.0590 - accuracy: 0.9782 - val_loss: 0.4517 - val_accuracy: 0.8699
Epoch 14/20

 1024/88011 [..............................] - ETA: 0s - loss: 0.0670 - accuracy: 0.9736
 9216/88011 [==>...........................] - ETA: 0s - loss: 0.0575 - accuracy: 0.9787
16384/88011 [====>.........................] - ETA: 0s - loss: 0.0537 - accuracy: 0.9796
24576/88011 [=======>......................] - ETA: 0s - loss: 0.0519 - accuracy: 0.9800
32768/88011 [==========>...................] - ETA: 0s - loss: 0.0527 - accuracy: 0.9796
40960/88011 [============>.................] - ETA: 0s - loss: 0.0529 - accuracy: 0.9795
49152/88011 [===============>..............] - ETA: 0s - loss: 0.0521 - accuracy: 0.9797
57344/88011 [==================>...........] - ETA: 0s - loss: 0.0520 - accuracy: 0.9797
65536/88011 [=====================>........] - ETA: 0s - loss: 0.0519 - accuracy: 0.9798
72704/88011 [=======================>......] - ETA: 0s - loss: 0.0522 - accuracy: 0.9798
80896/88011 [==========================>...] - ETA: 0s - loss: 0.0525 - accuracy: 0.9795
Epoch 00014: val_loss did not improve from 0.32914

88011/88011 [==============================] - 1s 8us/sample - loss: 0.0529 - accuracy: 0.9793 - val_loss: 0.4646 - val_accuracy: 0.8707
Epoch 15/20

 1024/88011 [..............................] - ETA: 0s - loss: 0.0505 - accuracy: 0.9775
 8192/88011 [=>............................] - ETA: 0s - loss: 0.0501 - accuracy: 0.9807
15360/88011 [====>.........................] - ETA: 0s - loss: 0.0490 - accuracy: 0.9813
22528/88011 [======>.......................] - ETA: 0s - loss: 0.0480 - accuracy: 0.9815
29696/88011 [=========>....................] - ETA: 0s - loss: 0.0474 - accuracy: 0.9815
36864/88011 [===========>..................] - ETA: 0s - loss: 0.0471 - accuracy: 0.9818
44032/88011 [==============>...............] - ETA: 0s - loss: 0.0481 - accuracy: 0.9814
51200/88011 [================>.............] - ETA: 0s - loss: 0.0491 - accuracy: 0.9812
58368/88011 [==================>...........] - ETA: 0s - loss: 0.0495 - accuracy: 0.9808
65536/88011 [=====================>........] - ETA: 0s - loss: 0.0502 - accuracy: 0.9803
72704/88011 [=======================>......] - ETA: 0s - loss: 0.0502 - accuracy: 0.9802
79872/88011 [==========================>...] - ETA: 0s - loss: 0.0503 - accuracy: 0.9803
Epoch 00015: val_loss did not improve from 0.32914

Epoch 00015: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.

88011/88011 [==============================] - 1s 9us/sample - loss: 0.0503 - accuracy: 0.9803 - val_loss: 0.4801 - val_accuracy: 0.8703
Epoch 16/20

 1024/88011 [..............................] - ETA: 0s - loss: 0.0458 - accuracy: 0.9814
 9216/88011 [==>...........................] - ETA: 0s - loss: 0.0385 - accuracy: 0.9859
17408/88011 [====>.........................] - ETA: 0s - loss: 0.0391 - accuracy: 0.9855
25600/88011 [=======>......................] - ETA: 0s - loss: 0.0383 - accuracy: 0.9857
33792/88011 [==========>...................] - ETA: 0s - loss: 0.0377 - accuracy: 0.9858
41984/88011 [=============>................] - ETA: 0s - loss: 0.0373 - accuracy: 0.9857
49152/88011 [===============>..............] - ETA: 0s - loss: 0.0370 - accuracy: 0.9857
56320/88011 [==================>...........] - ETA: 0s - loss: 0.0365 - accuracy: 0.9859
63488/88011 [====================>.........] - ETA: 0s - loss: 0.0360 - accuracy: 0.9861
70656/88011 [=======================>......] - ETA: 0s - loss: 0.0362 - accuracy: 0.9859
77824/88011 [=========================>....] - ETA: 0s - loss: 0.0363 - accuracy: 0.9860
86016/88011 [============================>.] - ETA: 0s - loss: 0.0368 - accuracy: 0.9857
Epoch 00016: val_loss did not improve from 0.32914

88011/88011 [==============================] - 1s 8us/sample - loss: 0.0367 - accuracy: 0.9858 - val_loss: 0.4849 - val_accuracy: 0.8731
Epoch 17/20

 1024/88011 [..............................] - ETA: 0s - loss: 0.0279 - accuracy: 0.9922
 8192/88011 [=>............................] - ETA: 0s - loss: 0.0295 - accuracy: 0.9901
15360/88011 [====>.........................] - ETA: 0s - loss: 0.0308 - accuracy: 0.9895
22528/88011 [======>.......................] - ETA: 0s - loss: 0.0314 - accuracy: 0.9889
29696/88011 [=========>....................] - ETA: 0s - loss: 0.0318 - accuracy: 0.9884
37888/88011 [===========>..................] - ETA: 0s - loss: 0.0317 - accuracy: 0.9885
45056/88011 [==============>...............] - ETA: 0s - loss: 0.0319 - accuracy: 0.9883
53248/88011 [=================>............] - ETA: 0s - loss: 0.0314 - accuracy: 0.9885
61440/88011 [===================>..........] - ETA: 0s - loss: 0.0314 - accuracy: 0.9885
69632/88011 [======================>.......] - ETA: 0s - loss: 0.0312 - accuracy: 0.9884
77824/88011 [=========================>....] - ETA: 0s - loss: 0.0313 - accuracy: 0.9884
86016/88011 [============================>.] - ETA: 0s - loss: 0.0316 - accuracy: 0.9882
Epoch 00017: val_loss did not improve from 0.32914

88011/88011 [==============================] - 1s 8us/sample - loss: 0.0318 - accuracy: 0.9881 - val_loss: 0.4935 - val_accuracy: 0.8739
Epoch 18/20

 1024/88011 [..............................] - ETA: 0s - loss: 0.0285 - accuracy: 0.9922
 9216/88011 [==>...........................] - ETA: 0s - loss: 0.0298 - accuracy: 0.9883
16384/88011 [====>.........................] - ETA: 0s - loss: 0.0308 - accuracy: 0.9880
24576/88011 [=======>......................] - ETA: 0s - loss: 0.0300 - accuracy: 0.9882
32768/88011 [==========>...................] - ETA: 0s - loss: 0.0311 - accuracy: 0.9874
40960/88011 [============>.................] - ETA: 0s - loss: 0.0308 - accuracy: 0.9875
48128/88011 [===============>..............] - ETA: 0s - loss: 0.0311 - accuracy: 0.9876
56320/88011 [==================>...........] - ETA: 0s - loss: 0.0315 - accuracy: 0.9874
63488/88011 [====================>.........] - ETA: 0s - loss: 0.0312 - accuracy: 0.9876
70656/88011 [=======================>......] - ETA: 0s - loss: 0.0313 - accuracy: 0.9876
77824/88011 [=========================>....] - ETA: 0s - loss: 0.0312 - accuracy: 0.9876
84992/88011 [===========================>..] - ETA: 0s - loss: 0.0312 - accuracy: 0.9876
Epoch 00018: val_loss did not improve from 0.32914

88011/88011 [==============================] - 1s 8us/sample - loss: 0.0310 - accuracy: 0.9877 - val_loss: 0.5069 - val_accuracy: 0.8734
Epoch 19/20

 1024/88011 [..............................] - ETA: 0s - loss: 0.0420 - accuracy: 0.9834
 8192/88011 [=>............................] - ETA: 0s - loss: 0.0302 - accuracy: 0.9891
16384/88011 [====>.........................] - ETA: 0s - loss: 0.0288 - accuracy: 0.9889
24576/88011 [=======>......................] - ETA: 0s - loss: 0.0284 - accuracy: 0.9887
31744/88011 [=========>....................] - ETA: 0s - loss: 0.0285 - accuracy: 0.9887
38912/88011 [============>.................] - ETA: 0s - loss: 0.0283 - accuracy: 0.9887
46080/88011 [==============>...............] - ETA: 0s - loss: 0.0283 - accuracy: 0.9886
53248/88011 [=================>............] - ETA: 0s - loss: 0.0283 - accuracy: 0.9888
60416/88011 [===================>..........] - ETA: 0s - loss: 0.0283 - accuracy: 0.9888
68608/88011 [======================>.......] - ETA: 0s - loss: 0.0283 - accuracy: 0.9888
76800/88011 [=========================>....] - ETA: 0s - loss: 0.0288 - accuracy: 0.9886
84992/88011 [===========================>..] - ETA: 0s - loss: 0.0291 - accuracy: 0.9884
Epoch 00019: val_loss did not improve from 0.32914

88011/88011 [==============================] - 1s 8us/sample - loss: 0.0291 - accuracy: 0.9883 - val_loss: 0.5182 - val_accuracy: 0.8746
Epoch 20/20

 1024/88011 [..............................] - ETA: 0s - loss: 0.0264 - accuracy: 0.9883
 9216/88011 [==>...........................] - ETA: 0s - loss: 0.0263 - accuracy: 0.9903
17408/88011 [====>.........................] - ETA: 0s - loss: 0.0251 - accuracy: 0.9907
25600/88011 [=======>......................] - ETA: 0s - loss: 0.0257 - accuracy: 0.9904
32768/88011 [==========>...................] - ETA: 0s - loss: 0.0253 - accuracy: 0.9909
40960/88011 [============>.................] - ETA: 0s - loss: 0.0256 - accuracy: 0.9909
49152/88011 [===============>..............] - ETA: 0s - loss: 0.0263 - accuracy: 0.9906
57344/88011 [==================>...........] - ETA: 0s - loss: 0.0266 - accuracy: 0.9901
65536/88011 [=====================>........] - ETA: 0s - loss: 0.0273 - accuracy: 0.9898
73728/88011 [========================>.....] - ETA: 0s - loss: 0.0275 - accuracy: 0.9896
80896/88011 [==========================>...] - ETA: 0s - loss: 0.0275 - accuracy: 0.9895
Epoch 00020: val_loss did not improve from 0.32914

88011/88011 [==============================] - 1s 8us/sample - loss: 0.0278 - accuracy: 0.9893 - val_loss: 0.5203 - val_accuracy: 0.8742
ASR
snr: 0 acc: 0.09532888465204957
snr: 2 acc: 0.08895131086142322
snr: 4 acc: 0.06607929515418502
snr: 6 acc: 0.08999081726354453
snr: 8 acc: 0.0987012987012987
snr: 10 acc: 0.07622504537205081
snr: 12 acc: 0.09107303877366997
snr: 14 acc: 0.09972552607502287
snr: 16 acc: 0.08217913204062789
snr: 18 acc: 0.09042076991942703
acc_mean:  0.08786751188132996
110000
[0, 2, 4, 6, 8, 10, 12, 14, 16, 18]
['8PSK', 'AM-DSB', 'AM-SSB', 'BPSK', 'CPFSK', 'GFSK', 'PAM4', 'QAM16', 'QAM64', 'QPSK', 'WBFM']
Model: "model_1"
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
input (InputLayer)           [(None, 2, 128, 1)]       0         
_________________________________________________________________
conv1 (Conv2D)               (None, 2, 128, 256)       4352      
_________________________________________________________________
max_pooling2d (MaxPooling2D) (None, 2, 64, 256)        0         
_________________________________________________________________
dropout_9 (Dropout)          (None, 2, 64, 256)        0         
_________________________________________________________________
conv2 (Conv2D)               (None, 2, 64, 128)        524416    
_________________________________________________________________
max_pooling2d_1 (MaxPooling2 (None, 2, 32, 128)        0         
_________________________________________________________________
dropout_10 (Dropout)         (None, 2, 32, 128)        0         
_________________________________________________________________
conv3 (Conv2D)               (None, 2, 32, 64)         131136    
_________________________________________________________________
max_pooling2d_2 (MaxPooling2 (None, 2, 16, 64)         0         
_________________________________________________________________
dropout_11 (Dropout)         (None, 2, 16, 64)         0         
_________________________________________________________________
conv4 (Conv2D)               (None, 2, 16, 64)         65600     
_________________________________________________________________
max_pooling2d_3 (MaxPooling2 (None, 2, 8, 64)          0         
_________________________________________________________________
dropout_12 (Dropout)         (None, 2, 8, 64)          0         
_________________________________________________________________
flatten_2 (Flatten)          (None, 1024)              0         
_________________________________________________________________
dense1 (Dense)               (None, 128)               131200    
_________________________________________________________________
dense2 (Dense)               (None, 11)                1419      
=================================================================
Total params: 858,123
Trainable params: 858,123
Non-trainable params: 0
_________________________________________________________________
None
Model: "sequential_2"
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
conv2d_4 (Conv2D)            (None, 71, 71, 32)        2432      
_________________________________________________________________
max_pooling2d_2 (MaxPooling2 (None, 35, 35, 32)        0         
_________________________________________________________________
conv2d_5 (Conv2D)            (None, 31, 31, 64)        51264     
_________________________________________________________________
max_pooling2d_3 (MaxPooling2 (None, 15, 15, 64)        0         
_________________________________________________________________
flatten_2 (Flatten)          (None, 14400)             0         
_________________________________________________________________
dense_5 (Dense)              (None, 1000)              14401000  
_________________________________________________________________
dense_6 (Dense)              (None, 11)                11011     
=================================================================
Total params: 14,465,707
Trainable params: 14,465,707
Non-trainable params: 0
_________________________________________________________________
None
CA
snr: 0 acc: 0.7464251668255482
snr: 2 acc: 0.8361423220973783
snr: 4 acc: 0.8669603524229075
snr: 6 acc: 0.8898071625344353
snr: 8 acc: 0.922943722943723
snr: 10 acc: 0.9047186932849365
snr: 12 acc: 0.9287646528403968
snr: 14 acc: 0.9213174748398902
snr: 16 acc: 0.9316712834718375
snr: 18 acc: 0.9382273948075202
acc_mean:  0.8886978226068573
ASR
[0.09532888465204957, 0.08895131086142322, 0.06607929515418502, 0.08999081726354453, 0.0987012987012987, 0.07622504537205081, 0.09107303877366997, 0.09972552607502287, 0.08217913204062789, 0.09042076991942703, 0.08786751188132996]
CA
[0.7464251668255482, 0.8361423220973783, 0.8669603524229075, 0.8898071625344353, 0.922943722943723, 0.9047186932849365, 0.9287646528403968, 0.9213174748398902, 0.9316712834718375, 0.9382273948075202, 0.8886978226068573]
