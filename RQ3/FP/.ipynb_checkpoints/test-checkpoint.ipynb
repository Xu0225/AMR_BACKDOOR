{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "82593541",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ART\n",
    "# # coding=gbk\n",
    "import os, pickle, random, sys\n",
    "sys.path.append('D:\\\\zhaixu\\\\Thesis_Code\\\\dl_amc_defense_seq')\n",
    "sys.path.append('D:\\\\zhaixu\\\\Thesis_Code')\n",
    "\n",
    "# log_print = open('Defalust.log', 'w')\n",
    "# sys.stdout = log_print\n",
    "# sys.stderr = log_print\n",
    "\n",
    "import warnings\n",
    "import copy\n",
    "warnings.filterwarnings('ignore')\n",
    "import numpy as np\n",
    "import argparse\n",
    "from matplotlib import pyplot as plt\n",
    "import tensorflow as tf\n",
    "tf.compat.v1.disable_eager_execution()\n",
    "import mltools\n",
    "\n",
    "from art.estimators.classification import KerasClassifier\n",
    "from art.attacks.poisoning import PoisoningAttackBackdoor\n",
    "from art.attacks.poisoning.perturbations import add_pattern_bd, add_single_bd, insert_image\n",
    "from art.utils import load_mnist, preprocess\n",
    "from art.defences.detector.poison import ActivationDefence\n",
    "from art.defences.transformer.poisoning import NeuralCleanse\n",
    "from art.estimators.poison_mitigation import KerasNeuralCleanse\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from trigger_config import load_data\n",
    "from trigger_config import set_trigger_config\n",
    "\n",
    "from tensorflow import keras\n",
    "from sklearn.preprocessing import normalize  # Adjust based on your preprocessing needs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "376607d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_arguments():\n",
    "    parser = argparse.ArgumentParser(description='AC PARAMs')\n",
    "\n",
    "    # 添加命令行参数\n",
    "    parser.add_argument('--TRIGGER_TYPE', type=str, default='phase_shift')\n",
    "    parser.add_argument('--GPU_NUM', type=str, default='0')\n",
    "    parser.add_argument('--POS_RATE', type=float, default=0.1)\n",
    "    parser.add_argument('--MODEL_NAME', type=str, default='CNN2')\n",
    "    parser.add_argument('--EPOCH', type=int, default=2)\n",
    "\n",
    "    return parser.parse_args()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b5630cb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_conv_index(model):\n",
    "    convindex = []\n",
    "    for i, layer in enumerate(model.layers):\n",
    "        if isinstance(layer, tf.keras.layers.Conv2D) or isinstance(layer, tf.keras.layers.Conv1D):\n",
    "            convindex.append(i)\n",
    "    return convindex\n",
    "\n",
    "def clear_max_weights(weights, thresh):\n",
    "    for i in range(len(weights)):\n",
    "        for j in range(len(weights[i])):\n",
    "            for k in range(len(weights[i][j])):\n",
    "                for m in range(len(weights[i][j][k])):\n",
    "                    if weights[i][j][k][m] > thresh:\n",
    "                        weights[i][j][k][m] = 0\n",
    "    return weights\n",
    "\n",
    "def calc_top_X_percent_weight(weights, fraction):\n",
    "    flat_weights = weights.flatten()\n",
    "    num_weights_to_keep = int(len(flat_weights) * (1 - fraction))\n",
    "    top_weights = np.sort(flat_weights)[-num_weights_to_keep]\n",
    "    return top_weights\n",
    "\n",
    "def fineprune(model, pruning_fraction, incremental=False):\n",
    "    layer_weights = []\n",
    "    convindex = get_conv_index(model)\n",
    "    for i in convindex:\n",
    "        layer_weights.append(model.layers[i].get_weights()[0])\n",
    "\n",
    "    max_weights_thr = []\n",
    "    for weights in layer_weights:\n",
    "        max_weights_thr.append(calc_top_X_percent_weight(weights, pruning_fraction))\n",
    "\n",
    "    new_weights = []\n",
    "    for i, weights in enumerate(layer_weights):\n",
    "        new_weights.append(clear_max_weights(weights, max_weights_thr[i]))\n",
    "\n",
    "    for i, idx in enumerate(convindex):\n",
    "        current_weights = model.layers[idx].get_weights()\n",
    "        current_weights[0] = new_weights[i]\n",
    "        model.layers[idx].set_weights(current_weights)\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "e460c74c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluation(model,X_test, Y_test,test_idx,mods,snrs,lbl):\n",
    "    classes = mods\n",
    "    acc = []\n",
    "    for snr in snrs:\n",
    "\n",
    "        # extract classes @ SNR\n",
    "        test_SNRs = list(map(lambda x: lbl[x][1], test_idx))\n",
    "        test_X_i = X_test[np.where(np.array(test_SNRs) == snr)]\n",
    "        test_Y_i = Y_test[np.where(np.array(test_SNRs) == snr)]\n",
    "\n",
    "        # estimate classes\n",
    "        test_Y_i_hat = model.predict(test_X_i)\n",
    "        conf = np.zeros([len(classes), len(classes)])\n",
    "\n",
    "        for i in range(0, test_X_i.shape[0]):\n",
    "            j = list(test_Y_i[i, :]).index(1)\n",
    "            k = int(np.argmax(test_Y_i_hat[i, :]))\n",
    "            conf[j, k] = conf[j, k] + 1\n",
    "\n",
    "        cor = np.sum(np.diag(conf))\n",
    "        ncor = np.sum(conf) - cor\n",
    "        #print(\"Overall Accuracy: \", cor / (cor+ncor))\n",
    "        #print(\"snr:\",snr,\"acc:\",cor / (cor + ncor))\n",
    "        acc.append(1.0 * cor / (cor + ncor))\n",
    "    acc_mean = sum(acc) / len(acc)\n",
    "    print('acc_mean: ',acc_mean)\n",
    "    acc.append(acc_mean)\n",
    "    return acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "50b0b5f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def incremental_pruning(model, X_train, Y_train, X_val, Y_val, \n",
    "                        initial_fraction=0.1, \n",
    "                        final_fraction=0.9, \n",
    "                        steps=5, \n",
    "                        epochs_per_step=10):\n",
    "    pruning_fractions = np.linspace(initial_fraction, final_fraction, steps)\n",
    "    clean_accuracies = []\n",
    "    attack_sucecess_rates = []\n",
    "    for fraction in pruning_fractions:\n",
    "        model = fineprune(model, fraction, incremental=True)\n",
    "        model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "        model.fit(X_train, Y_train, validation_data=(X_val, Y_val), epochs=epochs_per_step, verbose=0)\n",
    "        \n",
    "        print('CA')\n",
    "        acc_ca_no_defense = evaluation(model,X_test_benign,Y_test,test_idx,mods,snrs,lbl)\n",
    "        print('ASR')\n",
    "        acc_asr_no_defense = evaluation(model,X_test_modified,Y_test_modified,test_idx,mods,snrs,lbl)\n",
    "        \n",
    "        clean_accuracies.append(acc_ca_no_defense)\n",
    "        attack_sucecess_rates.append(acc_asr_no_defense)\n",
    "        print(f'Pruning Fraction: {fraction}, CA: {acc_ca_no_defense}, ASR: {acc_asr_no_defense}')\n",
    "\n",
    "    plt.plot(pruning_fractions, clean_accuracies, attack_sucecess_rates)\n",
    "    plt.xlabel('Pruning Fraction')\n",
    "    plt.ylabel('CA_ASR')\n",
    "    plt.title('Model Performance vs Pruning Fraction')\n",
    "    plt.show()\n",
    "\n",
    "    return model, pruning_fractions, clean_accuracies, attack_sucecess_rates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "5e7d286d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "110000\n",
      "[0, 2, 4, 6, 8, 10, 12, 14, 16, 18]\n",
      "['8PSK', 'AM-DSB', 'AM-SSB', 'BPSK', 'CPFSK', 'GFSK', 'PAM4', 'QAM16', 'QAM64', 'QPSK', 'WBFM']\n",
      "Poisoned training data.\n",
      "No defense eval\n",
      "\n",
      "Evaluate ASR on poisoned test data.\n",
      "CA\n",
      "snr: 0 acc: 0.8207816968541468\n",
      "snr: 2 acc: 0.8586142322097379\n",
      "snr: 4 acc: 0.8493392070484581\n",
      "snr: 6 acc: 0.8539944903581267\n",
      "snr: 8 acc: 0.8640692640692641\n",
      "snr: 10 acc: 0.8393829401088929\n",
      "snr: 12 acc: 0.8611361587015329\n",
      "snr: 14 acc: 0.8536139066788655\n",
      "snr: 16 acc: 0.8522622345337026\n",
      "snr: 18 acc: 0.8854073410922113\n",
      "acc_mean:  0.8538601471654939\n",
      "ASR\n",
      "snr: 0 acc: 0.8045757864632984\n",
      "snr: 2 acc: 0.8632958801498127\n",
      "snr: 4 acc: 0.9013215859030838\n",
      "snr: 6 acc: 0.9320477502295684\n",
      "snr: 8 acc: 0.9428571428571428\n",
      "snr: 10 acc: 0.9301270417422868\n",
      "snr: 12 acc: 0.9386834986474302\n",
      "snr: 14 acc: 0.939615736505032\n",
      "snr: 16 acc: 0.9473684210526315\n",
      "snr: 18 acc: 0.9391226499552372\n",
      "acc_mean:  0.9139015493505523\n"
     ]
    }
   ],
   "source": [
    "# load data\n",
    "X_train,X_test,Y_train,Y_test,mods,lbl,snrs,train_idx,test_idx = load_data()\n",
    "\n",
    "X_train_modified, Y_train_modified = set_trigger_config(X_train.copy(), Y_train.copy(), pos_rate=0.1,\n",
    "                                                        trigger_type=\"spectrum_shift\", data_type='train')\n",
    "\n",
    "x_train, x_val, y_train, y_val = train_test_split(X_train_modified, Y_train_modified, test_size=0.111,\n",
    "                                                  random_state=42)\n",
    "\n",
    "# load model\n",
    "from tensorflow.keras.models import load_model\n",
    "#poisoned_model = load_model('D:/zhaixu/Thesis_Code/dl_amc_backdoor/all_to_one/saved_model/posioned_'+ args.MODEL_NAME + '_Hanning_EPOCH_100_POS_RATE_0.1.h5')\n",
    "\n",
    "root_path = 'D:/zhaixu/Thesis_Code/dl_amc_backdoor/all_to_one/saved_model/'\n",
    "#pos_model_name = f\"{args.MODEL_NAME}_{args.TRIGGER_TYPE}_{args.POS_RATE}_{args.EPOCH}\"\n",
    "#poisoned_model = load_model(root_path + pos_model_name + '.h5')\n",
    "poisoned_model = load_model(root_path + 'CNN2_spectrum_shift_0.1_100.h5')\n",
    "\n",
    "# reshape train input\n",
    "#input_shape = poisoned_model.get_input_shape_at(0)\n",
    "input_shape = (None, 2, 128, 1)\n",
    "X_train = X_train.reshape((X_train.shape[0],) + tuple(input_shape[1:]))\n",
    "X_train_modified = X_train_modified.reshape((X_train_modified.shape[0],) + tuple(input_shape[1:]))\n",
    "x_train = x_train.reshape((x_train.shape[0],) + tuple(input_shape[1:]))\n",
    "x_val = x_val.reshape((x_val.shape[0],) + tuple(input_shape[1:]))\n",
    "\n",
    "# no defense\n",
    "print('No defense eval\\n')\n",
    "X_test_benign = X_test.reshape((X_test.shape[0],) + tuple(input_shape[1:]))\n",
    "X_test_modified, Y_test_modified = set_trigger_config(X_test.copy(), Y_test.copy(), pos_rate=0.1,\n",
    "                                                        trigger_type=\"spectrum_shift\", data_type='test')\n",
    "X_test_modified = X_test_modified.reshape((X_test_modified.shape[0],) + tuple(input_shape[1:]))\n",
    "print('CA')\n",
    "acc_ca_no_defense = evaluation(poisoned_model,X_test_benign,Y_test,test_idx,mods,snrs,lbl)\n",
    "print('ASR')\n",
    "acc_asr_no_defense = evaluation(poisoned_model,X_test_modified,Y_test_modified,test_idx,mods,snrs,lbl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "6facf014",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CA\n",
      "acc_mean:  0.7970506331070418\n",
      "ASR\n",
      "acc_mean:  0.18053344063148402\n",
      "Pruning Fraction: 0.1, CA: [0.763584366062917, 0.7780898876404494, 0.8070484581497798, 0.8117539026629935, 0.8043290043290043, 0.7767695099818511, 0.8016230838593328, 0.8069533394327539, 0.7922437673130194, 0.8281110116383169, 0.7970506331070418], ASR: [0.2449952335557674, 0.18820224719101122, 0.18061674008810572, 0.16804407713498623, 0.1774891774891775, 0.1569872958257713, 0.17853922452660054, 0.16010978956999086, 0.18651892890120036, 0.1638316920322292, 0.18053344063148402]\n",
      "CA\n",
      "acc_mean:  0.6885298084183418\n",
      "ASR\n",
      "acc_mean:  0.07060535473790021\n",
      "Pruning Fraction: 0.30000000000000004, CA: [0.6377502383222117, 0.6722846441947565, 0.6951541850220264, 0.7043158861340679, 0.6874458874458874, 0.6814882032667876, 0.690712353471596, 0.7044830741079597, 0.7008310249307479, 0.7108325872873769, 0.6885298084183418], ASR: [0.10295519542421354, 0.09176029962546817, 0.08193832599118943, 0.07162534435261708, 0.05887445887445888, 0.05716878402903811, 0.06311992786293959, 0.0585544373284538, 0.061865189289012, 0.05819158460161146, 0.07060535473790021]\n",
      "CA\n",
      "acc_mean:  0.4995971787929531\n",
      "ASR\n",
      "acc_mean:  0.11538631043692933\n",
      "Pruning Fraction: 0.5, CA: [0.4613918017159199, 0.5084269662921348, 0.5083700440528635, 0.5022956841138659, 0.5021645021645021, 0.49183303085299457, 0.5022542831379622, 0.4922232387923147, 0.5050784856879039, 0.5219337511190689, 0.4995971787929531], ASR: [0.008579599618684462, 0.023408239700374533, 0.06343612334801763, 0.10743801652892562, 0.1264069264069264, 0.1306715063520871, 0.18034265103697025, 0.18572735590118938, 0.15327793167128348, 0.17457475380483437, 0.11538631043692933]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-32-024b189a05ba>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      3\u001b[0m x_train_ca, x_val_ca, y_train_ca, y_val_ca = train_test_split(X_train, Y_train, test_size=0.111,\n\u001b[0;32m      4\u001b[0m                                                   random_state=42)\n\u001b[1;32m----> 5\u001b[1;33m \u001b[0mmodel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mincremental_pruning\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpoisoned_model\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx_train_ca\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train_ca\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx_val_ca\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_val_ca\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-31-061d53eb41cd>\u001b[0m in \u001b[0;36mincremental_pruning\u001b[1;34m(model, X_train, Y_train, X_val, Y_val, initial_fraction, final_fraction, steps, epochs_per_step)\u001b[0m\n\u001b[0;32m     10\u001b[0m         \u001b[0mmodel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfineprune\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfraction\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mincremental\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m         \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcompile\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'adam'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mloss\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'categorical_crossentropy'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmetrics\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'accuracy'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 12\u001b[1;33m         \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mY_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalidation_data\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_val\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mY_val\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mepochs_per_step\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     13\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     14\u001b[0m         \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'CA'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda3\\envs\\AMC\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[0;32m    817\u001b[0m         \u001b[0mmax_queue_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmax_queue_size\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    818\u001b[0m         \u001b[0mworkers\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mworkers\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 819\u001b[1;33m         use_multiprocessing=use_multiprocessing)\n\u001b[0m\u001b[0;32m    820\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    821\u001b[0m   def evaluate(self,\n",
      "\u001b[1;32mD:\\Anaconda3\\envs\\AMC\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training_arrays.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, model, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, **kwargs)\u001b[0m\n\u001b[0;32m    678\u001b[0m         \u001b[0mvalidation_steps\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mvalidation_steps\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    679\u001b[0m         \u001b[0mvalidation_freq\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mvalidation_freq\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 680\u001b[1;33m         steps_name='steps_per_epoch')\n\u001b[0m\u001b[0;32m    681\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    682\u001b[0m   def evaluate(self,\n",
      "\u001b[1;32mD:\\Anaconda3\\envs\\AMC\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training_arrays.py\u001b[0m in \u001b[0;36mmodel_iteration\u001b[1;34m(model, inputs, targets, sample_weights, batch_size, epochs, verbose, callbacks, val_inputs, val_targets, val_sample_weights, shuffle, initial_epoch, steps_per_epoch, validation_steps, validation_freq, mode, validation_in_fit, prepared_feed_values_from_dataset, steps_name, **kwargs)\u001b[0m\n\u001b[0;32m    396\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    397\u001b[0m         \u001b[1;31m# Get outputs.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 398\u001b[1;33m         \u001b[0mbatch_outs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    399\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch_outs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    400\u001b[0m           \u001b[0mbatch_outs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mbatch_outs\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda3\\envs\\AMC\\lib\\site-packages\\tensorflow_core\\python\\keras\\backend.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m   3565\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3566\u001b[0m     fetched = self._callable_fn(*array_vals,\n\u001b[1;32m-> 3567\u001b[1;33m                                 run_metadata=self.run_metadata)\n\u001b[0m\u001b[0;32m   3568\u001b[0m     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call_fetch_callbacks\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfetched\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m-\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_fetches\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3569\u001b[0m     output_structure = nest.pack_sequence_as(\n",
      "\u001b[1;32mD:\\Anaconda3\\envs\\AMC\\lib\\site-packages\\tensorflow_core\\python\\client\\session.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1472\u001b[0m         ret = tf_session.TF_SessionRunCallable(self._session._session,\n\u001b[0;32m   1473\u001b[0m                                                \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1474\u001b[1;33m                                                run_metadata_ptr)\n\u001b[0m\u001b[0;32m   1475\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1476\u001b[0m           \u001b[0mproto_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Incremental pruning\n",
    "#干净数据集微调\n",
    "x_train_ca, x_val_ca, y_train_ca, y_val_ca = train_test_split(X_train, Y_train, test_size=0.111,\n",
    "                                                  random_state=42)\n",
    "model = incremental_pruning(poisoned_model, x_train_ca, y_train_ca, x_val_ca, y_val_ca)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "899137e0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:AMC] *",
   "language": "python",
   "name": "conda-env-AMC-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
