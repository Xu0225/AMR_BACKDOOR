{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# Import all the things we need ---\n",
    "#   by setting env variables before Keras import you can set up which backend and which GPU it uses\n",
    "import os,random\n",
    "import keras\n",
    "#os.environ[\"KERAS_BACKEND\"] = \"theano\"\n",
    "\n",
    "os.environ[\"KERAS_BACKEND\"] = \"tensorflow\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICE\"]  = '0'\n",
    "#os.environ[\"THEANO_FLAGS\"]  = \"floatX=float32\"\n",
    "#os.environ[\"THEANO_FLAGS\"]  = \"device=cuda%d\"%(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import pickle, random, sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.utils import np_utils\n",
    "import keras.models as models\n",
    "from keras.layers.core import Reshape,Dense,Dropout,Activation,Flatten\n",
    "from keras.layers.noise import GaussianNoise\n",
    "from keras.layers.convolutional import Convolution2D, MaxPooling2D, ZeroPadding2D\n",
    "from keras.regularizers import *\n",
    "#from keras.optimizers import adam\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "#import theano as th\n",
    "#import theano.tensor as T\n",
    "import os\n",
    "WEIGHTS_PATH = ('resnet_like_weights_tf_dim_ordering_tf_kernels.h5')\n",
    "from keras.models import Model\n",
    "from keras.layers import Input,Dense,Conv1D,MaxPool1D,ReLU,Dropout,Softmax\n",
    "from keras.layers import LSTM\n",
    "import keras\n",
    "from keras.callbacks import LearningRateScheduler\n",
    "import keras.backend as K\n",
    "from keras.callbacks import LearningRateScheduler\n",
    "from keras.regularizers import *\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from keras.models import model_from_json\n",
    "import tensorflow as tf\n",
    "import importlib,sys\n",
    "\n",
    "importlib.reload(sys)\n",
    "\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "import matplotlib.pyplot as plt # plotting\n",
    "import numpy as np # linear algebra\n",
    "import os # accessing directory structure\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "# Load the dataset ...\n",
    "#  You will need to seperately download or generate this file\n",
    "dbfile = open('RML2016.10a_dict.dat', 'rb')      \n",
    "Xd = pickle.load(dbfile,encoding='latin1') \n",
    "snrs,mods = map(lambda j: sorted(list(set(map(lambda x: x[j], Xd.keys())))), [1,0])\n",
    "X = []  \n",
    "lbl = []\n",
    "for mod in mods:\n",
    "    for snr in snrs:\n",
    "        X.append(Xd[(mod,snr)])\n",
    "        for i in range(Xd[(mod,snr)].shape[0]):  lbl.append((mod,snr))\n",
    "X = np.vstack(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Partition the data\n",
    "#  into training and test sets of the form we can train/test on \n",
    "#  while keeping SNR and Mod labels handy for each\n",
    "np.random.seed(2016)\n",
    "n_examples = X.shape[0]\n",
    "n_train = int(n_examples * 0.9)\n",
    "train_idx = np.random.choice(range(0,n_examples), size=n_train, replace=False)\n",
    "test_idx = list(set(range(0,n_examples))-set(train_idx))\n",
    "X_train = X[train_idx]\n",
    "X_test =  X[test_idx]\n",
    "def to_onehot(yy):\n",
    "    yy1 = np.zeros([len(yy), max(yy)+1])\n",
    "    yy1[np.arange(len(yy)),yy] = 1\n",
    "    return yy1\n",
    "Y_train = to_onehot(list(map(lambda x: mods.index(lbl[x][0]), train_idx)))\n",
    "Y_test = to_onehot(list(map(lambda x: mods.index(lbl[x][0]), test_idx)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "数据集总数： 220000\n",
      "调制方式 11 种: ['8PSK', 'AM-DSB', 'AM-SSB', 'BPSK', 'CPFSK', 'GFSK', 'PAM4', 'QAM16', 'QAM64', 'QPSK', 'WBFM']\n",
      "信噪比: [-20, -18, -16, -14, -12, -10, -8, -6, -4, -2, 0, 2, 4, 6, 8, 10, 12, 14, 16, 18]\n"
     ]
    }
   ],
   "source": [
    "print('数据集总数：',n_examples)\n",
    "print('调制方式' , len(mods),'种:' ,mods)\n",
    "print('信噪比:',snrs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_feature_expert = pd.read_csv('X_train_feature_expert.csv',header=None).drop(columns=[0],index=[0])\n",
    "X_test_feature_expert = pd.read_csv('X_test_feature_expert.csv',header=None).drop(columns=[0],index=[0])\n",
    "\n",
    "X_train_feature_time = pd.read_csv('X_train_feature_time.csv',header=None).drop(columns=[0],index=[0])\n",
    "X_test_feature_time = pd.read_csv('X_test_feature_time.csv',header=None).drop(columns=[0],index=[0])\n",
    "\n",
    "X_train_feature_old = pd.read_csv('X_train_feature_old.csv',header=None).drop(columns=[0],index=[0])\n",
    "X_test_feature_old = pd.read_csv('X_test_feature_old.csv',header=None).drop(columns=[0],index=[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(198000, 132)\n",
      "(22000, 132)\n",
      "(220000, 127)\n",
      "X_train_std_all, (198000, 127)\n",
      "X_test_std, (22000, 127)\n"
     ]
    }
   ],
   "source": [
    "X_train_feature = X_train_feature_time\n",
    "X_test_feature = X_test_feature_time\n",
    "\n",
    "print(X_train_feature.shape)\n",
    "print(X_test_feature.shape)\n",
    "\n",
    "# standardize the features\n",
    "\n",
    "from sklearn.feature_selection import VarianceThreshold\n",
    "X_all = np.concatenate((X_train_feature, X_test_feature), axis = 0)\n",
    "\n",
    "s0 = X_train_feature.shape[0]\n",
    "s1 = X_train_feature.shape[1]\n",
    "\n",
    "k = 1\n",
    "interval = int(s1/k)\n",
    "X_train_std_all_list = []\n",
    "X_test_std_list = []\n",
    "sc_train = []\n",
    "for i in range(k):\n",
    "    X_all_filter = VarianceThreshold(threshold=(0)).fit_transform(X_all[:,interval*i: interval*(i+1)])\n",
    "    sc_train.append(StandardScaler())\n",
    "    print(X_all_filter.shape)\n",
    "    sc_train[i].fit(X_all_filter)\n",
    "    X_train_std_all_list.append(sc_train[i].transform(X_all_filter[:s0,:])) \n",
    "    X_test_std_list.append(sc_train[i].transform(X_all_filter[s0:,:]))\n",
    "X_train_std_all = np.concatenate((X_train_std_all_list), axis = 1)\n",
    "X_test_std = np.concatenate((X_test_std_list), axis = 1)\n",
    "\n",
    "print(\"X_train_std_all,\", X_train_std_all.shape)\n",
    "print(\"X_test_std,\", X_test_std.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train_std, (176022, 127)\n",
      "X_val_std, (21978, 127)\n",
      "X_test_std, (22000, 127)\n"
     ]
    }
   ],
   "source": [
    "# devide train and val data\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train_std, X_val_std, y_train, y_val = train_test_split(X_train_std_all, Y_train, test_size=0.111, random_state=42)\n",
    "print(\"X_train_std,\", X_train_std.shape)\n",
    "print(\"X_val_std,\", X_val_std.shape)\n",
    "print(\"X_test_std,\", X_test_std.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_std = pd.DataFrame(X_train_std)\n",
    "X_val_std = pd.DataFrame(X_val_std)\n",
    "X_test_std = pd.DataFrame(X_test_std)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# inf,nan数据填充\n",
    "X_train_std = (X_train_std.replace([np.inf, -np.inf], np.nan)).fillna(value = 0) \n",
    "X_val_std = (X_val_std.replace([np.inf, -np.inf], np.nan)).fillna(value = 0) \n",
    "X_test_std = (X_test_std.replace([np.inf, -np.inf], np.nan)).fillna(value = 0) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((176022, 127), (22000, 127))"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_std.shape,X_test_std.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    " \n",
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Flatten, Conv1D, MaxPool1D, Dropout\n",
    " \n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = X_train_std.values\n",
    "x_test = X_test_std.values\n",
    "x_val = X_val_std.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reshape 2D to 3D ->  x_train.reshape(num_of_examples,num_of_features,num_of_signals)\n",
    "x_train = x_train.reshape(x_train.shape[0], x_train.shape[1], 1)\n",
    "x_test = x_test.reshape(x_test.shape[0], x_test.shape[1], 1)\n",
    "x_val = x_val.reshape(x_val.shape[0], x_val.shape[1], 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((176022, 127, 1),\n",
       " (22000, 127, 1),\n",
       " (21978, 127, 1),\n",
       " (176022, 11),\n",
       " (22000, 11),\n",
       " (21978, 11))"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train.shape,x_test.shape,x_val.shape,y_train.shape,Y_test.shape,y_val.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_3\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_5 (Conv1D)            (None, 125, 64)           256       \n",
      "_________________________________________________________________\n",
      "max_pooling1d_5 (MaxPooling1 (None, 62, 64)            0         \n",
      "_________________________________________________________________\n",
      "conv1d_6 (Conv1D)            (None, 60, 64)            12352     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_6 (MaxPooling1 (None, 30, 64)            0         \n",
      "_________________________________________________________________\n",
      "flatten_3 (Flatten)          (None, 1920)              0         \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 128)               245888    \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 11)                1419      \n",
      "=================================================================\n",
      "Total params: 259,915\n",
      "Trainable params: 259,915\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# build CNN model\n",
    "model = Sequential()\n",
    "model.add(Conv1D(64, 3, input_shape=(127, 1),activation='relu'))  # convolution\n",
    "model.add(MaxPool1D(pool_size=2))  # pooling\n",
    "\n",
    "model.add(Conv1D(64, 3,activation='relu'))  # convolution\n",
    "model.add(MaxPool1D(pool_size=2))  # pooling\n",
    "\n",
    "model.add(Flatten())  # flatten\n",
    "model.add(Dense(128, activation='relu'))  # fc\n",
    "model.add(Dropout(0.5))  # dropout\n",
    "model.add(Dense(11, activation='softmax'))\n",
    " \n",
    "# model compile\n",
    "model.compile(loss=keras.losses.categorical_crossentropy,\n",
    "              optimizer=keras.optimizers.Adadelta(),\n",
    "              metrics=['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 176022 samples, validate on 21978 samples\n",
      "epoch(0) lr is 0.0010000000474974513\n",
      "Epoch 1/200\n",
      "174080/176022 [============================>.] - ETA: 0s - loss: 2.4084 - accuracy: 0.0956\n",
      "Epoch 00001: val_loss improved from inf to 2.39505, saving model to CNN_dr0.5.h5\n",
      "176022/176022 [==============================] - 3s 19us/sample - loss: 2.4083 - accuracy: 0.0957 - val_loss: 2.3951 - val_accuracy: 0.1039\n",
      "epoch(1) lr is 0.0010000000474974513\n",
      "Epoch 2/200\n",
      "171008/176022 [============================>.] - ETA: 0s - loss: 2.3997 - accuracy: 0.1016\n",
      "Epoch 00002: val_loss improved from 2.39505 to 2.38719, saving model to CNN_dr0.5.h5\n",
      "176022/176022 [==============================] - 2s 11us/sample - loss: 2.3997 - accuracy: 0.1019 - val_loss: 2.3872 - val_accuracy: 0.1157\n",
      "epoch(2) lr is 0.0010000000474974513\n",
      "Epoch 3/200\n",
      "173056/176022 [============================>.] - ETA: 0s - loss: 2.3910 - accuracy: 0.1085\n",
      "Epoch 00003: val_loss improved from 2.38719 to 2.37936, saving model to CNN_dr0.5.h5\n",
      "176022/176022 [==============================] - 2s 11us/sample - loss: 2.3909 - accuracy: 0.1083 - val_loss: 2.3794 - val_accuracy: 0.1269\n",
      "epoch(3) lr is 0.0010000000474974513\n",
      "Epoch 4/200\n",
      "171008/176022 [============================>.] - ETA: 0s - loss: 2.3829 - accuracy: 0.1163\n",
      "Epoch 00004: val_loss improved from 2.37936 to 2.37159, saving model to CNN_dr0.5.h5\n",
      "176022/176022 [==============================] - 2s 11us/sample - loss: 2.3826 - accuracy: 0.1165 - val_loss: 2.3716 - val_accuracy: 0.1342\n",
      "epoch(4) lr is 0.0010000000474974513\n",
      "Epoch 5/200\n",
      "169984/176022 [===========================>..] - ETA: 0s - loss: 2.3741 - accuracy: 0.1249\n",
      "Epoch 00005: val_loss improved from 2.37159 to 2.36408, saving model to CNN_dr0.5.h5\n",
      "176022/176022 [==============================] - 2s 11us/sample - loss: 2.3742 - accuracy: 0.1250 - val_loss: 2.3641 - val_accuracy: 0.1449\n",
      "epoch(5) lr is 0.0010000000474974513\n",
      "Epoch 6/200\n",
      "175104/176022 [============================>.] - ETA: 0s - loss: 2.3667 - accuracy: 0.1322\n",
      "Epoch 00006: val_loss improved from 2.36408 to 2.35696, saving model to CNN_dr0.5.h5\n",
      "176022/176022 [==============================] - 2s 11us/sample - loss: 2.3667 - accuracy: 0.1322 - val_loss: 2.3570 - val_accuracy: 0.1554\n",
      "epoch(6) lr is 0.0010000000474974513\n",
      "Epoch 7/200\n",
      "175104/176022 [============================>.] - ETA: 0s - loss: 2.3608 - accuracy: 0.1404\n",
      "Epoch 00007: val_loss improved from 2.35696 to 2.35014, saving model to CNN_dr0.5.h5\n",
      "176022/176022 [==============================] - 2s 11us/sample - loss: 2.3607 - accuracy: 0.1405 - val_loss: 2.3501 - val_accuracy: 0.1636\n",
      "epoch(7) lr is 0.0010000000474974513\n",
      "Epoch 8/200\n",
      "175104/176022 [============================>.] - ETA: 0s - loss: 2.3536 - accuracy: 0.1484\n",
      "Epoch 00008: val_loss improved from 2.35014 to 2.34377, saving model to CNN_dr0.5.h5\n",
      "176022/176022 [==============================] - 2s 11us/sample - loss: 2.3536 - accuracy: 0.1484 - val_loss: 2.3438 - val_accuracy: 0.1702\n",
      "epoch(8) lr is 0.0010000000474974513\n",
      "Epoch 9/200\n",
      "175104/176022 [============================>.] - ETA: 0s - loss: 2.3471 - accuracy: 0.1549\n",
      "Epoch 00009: val_loss improved from 2.34377 to 2.33753, saving model to CNN_dr0.5.h5\n",
      "176022/176022 [==============================] - 2s 11us/sample - loss: 2.3470 - accuracy: 0.1549 - val_loss: 2.3375 - val_accuracy: 0.1749\n",
      "epoch(9) lr is 0.0010000000474974513\n",
      "Epoch 10/200\n",
      "171008/176022 [============================>.] - ETA: 0s - loss: 2.3412 - accuracy: 0.1594\n",
      "Epoch 00010: val_loss improved from 2.33753 to 2.33122, saving model to CNN_dr0.5.h5\n",
      "176022/176022 [==============================] - 2s 11us/sample - loss: 2.3413 - accuracy: 0.1595 - val_loss: 2.3312 - val_accuracy: 0.1806\n",
      "epoch(10) lr is 0.0010000000474974513\n",
      "Epoch 11/200\n",
      "172032/176022 [============================>.] - ETA: 0s - loss: 2.3350 - accuracy: 0.1652\n",
      "Epoch 00011: val_loss improved from 2.33122 to 2.32464, saving model to CNN_dr0.5.h5\n",
      "176022/176022 [==============================] - 2s 11us/sample - loss: 2.3349 - accuracy: 0.1652 - val_loss: 2.3246 - val_accuracy: 0.1886\n",
      "epoch(11) lr is 0.0010000000474974513\n",
      "Epoch 12/200\n",
      "174080/176022 [============================>.] - ETA: 0s - loss: 2.3279 - accuracy: 0.1728\n",
      "Epoch 00012: val_loss improved from 2.32464 to 2.31781, saving model to CNN_dr0.5.h5\n",
      "176022/176022 [==============================] - 2s 11us/sample - loss: 2.3279 - accuracy: 0.1727 - val_loss: 2.3178 - val_accuracy: 0.1981\n",
      "epoch(12) lr is 0.0010000000474974513\n",
      "Epoch 13/200\n",
      "175104/176022 [============================>.] - ETA: 0s - loss: 2.3215 - accuracy: 0.1774\n",
      "Epoch 00013: val_loss improved from 2.31781 to 2.31095, saving model to CNN_dr0.5.h5\n",
      "176022/176022 [==============================] - 2s 11us/sample - loss: 2.3215 - accuracy: 0.1774 - val_loss: 2.3110 - val_accuracy: 0.2080\n",
      "epoch(13) lr is 0.0010000000474974513\n",
      "Epoch 14/200\n",
      "175104/176022 [============================>.] - ETA: 0s - loss: 2.3151 - accuracy: 0.1843\n",
      "Epoch 00014: val_loss improved from 2.31095 to 2.30410, saving model to CNN_dr0.5.h5\n",
      "176022/176022 [==============================] - 2s 11us/sample - loss: 2.3151 - accuracy: 0.1843 - val_loss: 2.3041 - val_accuracy: 0.2163\n",
      "epoch(14) lr is 0.0010000000474974513\n",
      "Epoch 15/200\n",
      "172032/176022 [============================>.] - ETA: 0s - loss: 2.3090 - accuracy: 0.1892\n",
      "Epoch 00015: val_loss improved from 2.30410 to 2.29744, saving model to CNN_dr0.5.h5\n",
      "176022/176022 [==============================] - 2s 11us/sample - loss: 2.3088 - accuracy: 0.1892 - val_loss: 2.2974 - val_accuracy: 0.2233\n",
      "epoch(15) lr is 0.0010000000474974513\n",
      "Epoch 16/200\n",
      "172032/176022 [============================>.] - ETA: 0s - loss: 2.3024 - accuracy: 0.1934\n",
      "Epoch 00016: val_loss improved from 2.29744 to 2.29099, saving model to CNN_dr0.5.h5\n",
      "176022/176022 [==============================] - 2s 11us/sample - loss: 2.3023 - accuracy: 0.1932 - val_loss: 2.2910 - val_accuracy: 0.2307\n",
      "epoch(16) lr is 0.0010000000474974513\n",
      "Epoch 17/200\n",
      "173056/176022 [============================>.] - ETA: 0s - loss: 2.2976 - accuracy: 0.1957\n",
      "Epoch 00017: val_loss improved from 2.29099 to 2.28472, saving model to CNN_dr0.5.h5\n",
      "176022/176022 [==============================] - 2s 11us/sample - loss: 2.2974 - accuracy: 0.1959 - val_loss: 2.2847 - val_accuracy: 0.2348\n",
      "epoch(17) lr is 0.0010000000474974513\n",
      "Epoch 18/200\n",
      "175104/176022 [============================>.] - ETA: 0s - loss: 2.2909 - accuracy: 0.1999\n",
      "Epoch 00018: val_loss improved from 2.28472 to 2.27849, saving model to CNN_dr0.5.h5\n",
      "176022/176022 [==============================] - 2s 11us/sample - loss: 2.2910 - accuracy: 0.1999 - val_loss: 2.2785 - val_accuracy: 0.2412\n",
      "epoch(18) lr is 0.0010000000474974513\n",
      "Epoch 19/200\n",
      "172032/176022 [============================>.] - ETA: 0s - loss: 2.2852 - accuracy: 0.2037\n",
      "Epoch 00019: val_loss improved from 2.27849 to 2.27223, saving model to CNN_dr0.5.h5\n",
      "176022/176022 [==============================] - 2s 11us/sample - loss: 2.2851 - accuracy: 0.2038 - val_loss: 2.2722 - val_accuracy: 0.2463\n",
      "epoch(19) lr is 0.0010000000474974513\n",
      "Epoch 20/200\n",
      "174080/176022 [============================>.] - ETA: 0s - loss: 2.2793 - accuracy: 0.2074\n",
      "Epoch 00020: val_loss improved from 2.27223 to 2.26595, saving model to CNN_dr0.5.h5\n",
      "176022/176022 [==============================] - 2s 11us/sample - loss: 2.2793 - accuracy: 0.2075 - val_loss: 2.2660 - val_accuracy: 0.2541\n",
      "epoch(20) lr is 0.0010000000474974513\n",
      "Epoch 21/200\n",
      "175104/176022 [============================>.] - ETA: 0s - loss: 2.2730 - accuracy: 0.2099\n",
      "Epoch 00021: val_loss improved from 2.26595 to 2.25967, saving model to CNN_dr0.5.h5\n",
      "176022/176022 [==============================] - 2s 11us/sample - loss: 2.2730 - accuracy: 0.2100 - val_loss: 2.2597 - val_accuracy: 0.2588\n",
      "epoch(21) lr is 0.0010000000474974513\n",
      "Epoch 22/200\n",
      "171008/176022 [============================>.] - ETA: 0s - loss: 2.2678 - accuracy: 0.2145\n",
      "Epoch 00022: val_loss improved from 2.25967 to 2.25338, saving model to CNN_dr0.5.h5\n",
      "176022/176022 [==============================] - 2s 11us/sample - loss: 2.2677 - accuracy: 0.2146 - val_loss: 2.2534 - val_accuracy: 0.2659\n",
      "epoch(22) lr is 0.0010000000474974513\n",
      "Epoch 23/200\n",
      "171008/176022 [============================>.] - ETA: 0s - loss: 2.2615 - accuracy: 0.2178\n",
      "Epoch 00023: val_loss improved from 2.25338 to 2.24709, saving model to CNN_dr0.5.h5\n",
      "176022/176022 [==============================] - 2s 11us/sample - loss: 2.2618 - accuracy: 0.2177 - val_loss: 2.2471 - val_accuracy: 0.2730\n",
      "epoch(23) lr is 0.0010000000474974513\n",
      "Epoch 24/200\n",
      "173056/176022 [============================>.] - ETA: 0s - loss: 2.2560 - accuracy: 0.2204\n",
      "Epoch 00024: val_loss improved from 2.24709 to 2.24084, saving model to CNN_dr0.5.h5\n",
      "176022/176022 [==============================] - 2s 11us/sample - loss: 2.2560 - accuracy: 0.2203 - val_loss: 2.2408 - val_accuracy: 0.2791\n",
      "epoch(24) lr is 0.0010000000474974513\n",
      "Epoch 25/200\n",
      "175104/176022 [============================>.] - ETA: 0s - loss: 2.2509 - accuracy: 0.2242\n",
      "Epoch 00025: val_loss improved from 2.24084 to 2.23455, saving model to CNN_dr0.5.h5\n",
      "176022/176022 [==============================] - 2s 11us/sample - loss: 2.2508 - accuracy: 0.2242 - val_loss: 2.2345 - val_accuracy: 0.2850\n",
      "epoch(25) lr is 0.0010000000474974513\n",
      "Epoch 26/200\n",
      "171008/176022 [============================>.] - ETA: 0s - loss: 2.2460 - accuracy: 0.2253\n",
      "Epoch 00026: val_loss improved from 2.23455 to 2.22829, saving model to CNN_dr0.5.h5\n",
      "176022/176022 [==============================] - 2s 11us/sample - loss: 2.2458 - accuracy: 0.2254 - val_loss: 2.2283 - val_accuracy: 0.2904\n",
      "epoch(26) lr is 0.0010000000474974513\n",
      "Epoch 27/200\n",
      "175104/176022 [============================>.] - ETA: 0s - loss: 2.2395 - accuracy: 0.2302\n",
      "Epoch 00027: val_loss improved from 2.22829 to 2.22198, saving model to CNN_dr0.5.h5\n",
      "176022/176022 [==============================] - 2s 11us/sample - loss: 2.2394 - accuracy: 0.2302 - val_loss: 2.2220 - val_accuracy: 0.2956\n",
      "epoch(27) lr is 0.0010000000474974513\n",
      "Epoch 28/200\n",
      "172032/176022 [============================>.] - ETA: 0s - loss: 2.2337 - accuracy: 0.2315\n",
      "Epoch 00028: val_loss improved from 2.22198 to 2.21563, saving model to CNN_dr0.5.h5\n",
      "176022/176022 [==============================] - 2s 11us/sample - loss: 2.2338 - accuracy: 0.2314 - val_loss: 2.2156 - val_accuracy: 0.3011\n",
      "epoch(28) lr is 0.0010000000474974513\n",
      "Epoch 29/200\n",
      "175104/176022 [============================>.] - ETA: 0s - loss: 2.2284 - accuracy: 0.2352\n",
      "Epoch 00029: val_loss improved from 2.21563 to 2.20933, saving model to CNN_dr0.5.h5\n",
      "176022/176022 [==============================] - 2s 11us/sample - loss: 2.2284 - accuracy: 0.2351 - val_loss: 2.2093 - val_accuracy: 0.3060\n",
      "epoch(29) lr is 0.0010000000474974513\n",
      "Epoch 30/200\n",
      "174080/176022 [============================>.] - ETA: 0s - loss: 2.2223 - accuracy: 0.2378\n",
      "Epoch 00030: val_loss improved from 2.20933 to 2.20296, saving model to CNN_dr0.5.h5\n",
      "176022/176022 [==============================] - 2s 11us/sample - loss: 2.2222 - accuracy: 0.2377 - val_loss: 2.2030 - val_accuracy: 0.3107\n",
      "epoch(30) lr is 0.0010000000474974513\n",
      "Epoch 31/200\n",
      "173056/176022 [============================>.] - ETA: 0s - loss: 2.2164 - accuracy: 0.2405\n",
      "Epoch 00031: val_loss improved from 2.20296 to 2.19655, saving model to CNN_dr0.5.h5\n",
      "176022/176022 [==============================] - 2s 11us/sample - loss: 2.2163 - accuracy: 0.2405 - val_loss: 2.1965 - val_accuracy: 0.3159\n",
      "epoch(31) lr is 0.0010000000474974513\n",
      "Epoch 32/200\n",
      "173056/176022 [============================>.] - ETA: 0s - loss: 2.2104 - accuracy: 0.2430\n",
      "Epoch 00032: val_loss improved from 2.19655 to 2.19017, saving model to CNN_dr0.5.h5\n",
      "176022/176022 [==============================] - 2s 11us/sample - loss: 2.2104 - accuracy: 0.2430 - val_loss: 2.1902 - val_accuracy: 0.3213\n",
      "epoch(32) lr is 0.0010000000474974513\n",
      "Epoch 33/200\n",
      "173056/176022 [============================>.] - ETA: 0s - loss: 2.2056 - accuracy: 0.2484\n",
      "Epoch 00033: val_loss improved from 2.19017 to 2.18374, saving model to CNN_dr0.5.h5\n",
      "176022/176022 [==============================] - 2s 11us/sample - loss: 2.2055 - accuracy: 0.2486 - val_loss: 2.1837 - val_accuracy: 0.3262\n",
      "epoch(33) lr is 0.0010000000474974513\n",
      "Epoch 34/200\n",
      "175104/176022 [============================>.] - ETA: 0s - loss: 2.1990 - accuracy: 0.2504\n",
      "Epoch 00034: val_loss improved from 2.18374 to 2.17723, saving model to CNN_dr0.5.h5\n",
      "176022/176022 [==============================] - 2s 11us/sample - loss: 2.1990 - accuracy: 0.2503 - val_loss: 2.1772 - val_accuracy: 0.3298\n",
      "epoch(34) lr is 0.0010000000474974513\n",
      "Epoch 35/200\n",
      "174080/176022 [============================>.] - ETA: 0s - loss: 2.1929 - accuracy: 0.2554\n",
      "Epoch 00035: val_loss improved from 2.17723 to 2.17062, saving model to CNN_dr0.5.h5\n",
      "176022/176022 [==============================] - 2s 11us/sample - loss: 2.1930 - accuracy: 0.2554 - val_loss: 2.1706 - val_accuracy: 0.3333\n",
      "epoch(35) lr is 0.0010000000474974513\n",
      "Epoch 36/200\n",
      "171008/176022 [============================>.] - ETA: 0s - loss: 2.1883 - accuracy: 0.2554\n",
      "Epoch 00036: val_loss improved from 2.17062 to 2.16414, saving model to CNN_dr0.5.h5\n",
      "176022/176022 [==============================] - 2s 11us/sample - loss: 2.1879 - accuracy: 0.2555 - val_loss: 2.1641 - val_accuracy: 0.3369\n",
      "epoch(36) lr is 0.0010000000474974513\n",
      "Epoch 37/200\n",
      "175104/176022 [============================>.] - ETA: 0s - loss: 2.1839 - accuracy: 0.2576\n",
      "Epoch 00037: val_loss improved from 2.16414 to 2.15771, saving model to CNN_dr0.5.h5\n",
      "176022/176022 [==============================] - 2s 11us/sample - loss: 2.1840 - accuracy: 0.2576 - val_loss: 2.1577 - val_accuracy: 0.3398\n",
      "epoch(37) lr is 0.0010000000474974513\n",
      "Epoch 38/200\n",
      "175104/176022 [============================>.] - ETA: 0s - loss: 2.1756 - accuracy: 0.2618\n",
      "Epoch 00038: val_loss improved from 2.15771 to 2.15116, saving model to CNN_dr0.5.h5\n",
      "176022/176022 [==============================] - 2s 11us/sample - loss: 2.1757 - accuracy: 0.2618 - val_loss: 2.1512 - val_accuracy: 0.3432\n",
      "epoch(38) lr is 0.0010000000474974513\n",
      "Epoch 39/200\n",
      "174080/176022 [============================>.] - ETA: 0s - loss: 2.1697 - accuracy: 0.2655\n",
      "Epoch 00039: val_loss improved from 2.15116 to 2.14466, saving model to CNN_dr0.5.h5\n",
      "176022/176022 [==============================] - 2s 11us/sample - loss: 2.1697 - accuracy: 0.2655 - val_loss: 2.1447 - val_accuracy: 0.3466\n",
      "epoch(39) lr is 0.0010000000474974513\n",
      "Epoch 40/200\n",
      "172032/176022 [============================>.] - ETA: 0s - loss: 2.1646 - accuracy: 0.2683\n",
      "Epoch 00040: val_loss improved from 2.14466 to 2.13821, saving model to CNN_dr0.5.h5\n",
      "176022/176022 [==============================] - 2s 11us/sample - loss: 2.1647 - accuracy: 0.2682 - val_loss: 2.1382 - val_accuracy: 0.3473\n",
      "epoch(40) lr is 0.0010000000474974513\n",
      "Epoch 41/200\n",
      "171008/176022 [============================>.] - ETA: 0s - loss: 2.1595 - accuracy: 0.2694\n",
      "Epoch 00041: val_loss improved from 2.13821 to 2.13195, saving model to CNN_dr0.5.h5\n",
      "176022/176022 [==============================] - 2s 11us/sample - loss: 2.1589 - accuracy: 0.2696 - val_loss: 2.1319 - val_accuracy: 0.3501\n",
      "epoch(41) lr is 0.0010000000474974513\n",
      "Epoch 42/200\n",
      "173056/176022 [============================>.] - ETA: 0s - loss: 2.1545 - accuracy: 0.2712\n",
      "Epoch 00042: val_loss improved from 2.13195 to 2.12568, saving model to CNN_dr0.5.h5\n",
      "176022/176022 [==============================] - 2s 11us/sample - loss: 2.1545 - accuracy: 0.2711 - val_loss: 2.1257 - val_accuracy: 0.3524\n",
      "epoch(42) lr is 0.0010000000474974513\n",
      "Epoch 43/200\n",
      "173056/176022 [============================>.] - ETA: 0s - loss: 2.1477 - accuracy: 0.2759\n",
      "Epoch 00043: val_loss improved from 2.12568 to 2.11944, saving model to CNN_dr0.5.h5\n",
      "176022/176022 [==============================] - 2s 11us/sample - loss: 2.1476 - accuracy: 0.2760 - val_loss: 2.1194 - val_accuracy: 0.3543\n",
      "epoch(43) lr is 0.0010000000474974513\n",
      "Epoch 44/200\n",
      "171008/176022 [============================>.] - ETA: 0s - loss: 2.1436 - accuracy: 0.2778\n",
      "Epoch 00044: val_loss improved from 2.11944 to 2.11320, saving model to CNN_dr0.5.h5\n",
      "176022/176022 [==============================] - 2s 11us/sample - loss: 2.1437 - accuracy: 0.2778 - val_loss: 2.1132 - val_accuracy: 0.3561\n",
      "epoch(44) lr is 0.0010000000474974513\n",
      "Epoch 45/200\n",
      "171008/176022 [============================>.] - ETA: 0s - loss: 2.1378 - accuracy: 0.2803\n",
      "Epoch 00045: val_loss improved from 2.11320 to 2.10693, saving model to CNN_dr0.5.h5\n",
      "176022/176022 [==============================] - 2s 11us/sample - loss: 2.1377 - accuracy: 0.2804 - val_loss: 2.1069 - val_accuracy: 0.3573\n",
      "epoch(45) lr is 0.0010000000474974513\n",
      "Epoch 46/200\n",
      "171008/176022 [============================>.] - ETA: 0s - loss: 2.1321 - accuracy: 0.2809\n",
      "Epoch 00046: val_loss improved from 2.10693 to 2.10069, saving model to CNN_dr0.5.h5\n",
      "176022/176022 [==============================] - 2s 11us/sample - loss: 2.1318 - accuracy: 0.2811 - val_loss: 2.1007 - val_accuracy: 0.3591\n",
      "epoch(46) lr is 0.0010000000474974513\n",
      "Epoch 47/200\n",
      "175104/176022 [============================>.] - ETA: 0s - loss: 2.1260 - accuracy: 0.2834\n",
      "Epoch 00047: val_loss improved from 2.10069 to 2.09452, saving model to CNN_dr0.5.h5\n",
      "176022/176022 [==============================] - 2s 11us/sample - loss: 2.1262 - accuracy: 0.2834 - val_loss: 2.0945 - val_accuracy: 0.3614\n",
      "epoch(47) lr is 0.0010000000474974513\n",
      "Epoch 48/200\n",
      "172032/176022 [============================>.] - ETA: 0s - loss: 2.1210 - accuracy: 0.2868\n",
      "Epoch 00048: val_loss improved from 2.09452 to 2.08848, saving model to CNN_dr0.5.h5\n",
      "176022/176022 [==============================] - 2s 11us/sample - loss: 2.1209 - accuracy: 0.2866 - val_loss: 2.0885 - val_accuracy: 0.3636\n",
      "epoch(48) lr is 0.0010000000474974513\n",
      "Epoch 49/200\n",
      "174080/176022 [============================>.] - ETA: 0s - loss: 2.1168 - accuracy: 0.2874\n",
      "Epoch 00049: val_loss improved from 2.08848 to 2.08253, saving model to CNN_dr0.5.h5\n",
      "176022/176022 [==============================] - 2s 11us/sample - loss: 2.1168 - accuracy: 0.2874 - val_loss: 2.0825 - val_accuracy: 0.3647\n",
      "epoch(49) lr is 0.0010000000474974513\n",
      "Epoch 50/200\n",
      "171008/176022 [============================>.] - ETA: 0s - loss: 2.1122 - accuracy: 0.2899\n",
      "Epoch 00050: val_loss improved from 2.08253 to 2.07666, saving model to CNN_dr0.5.h5\n",
      "176022/176022 [==============================] - 2s 11us/sample - loss: 2.1121 - accuracy: 0.2899 - val_loss: 2.0767 - val_accuracy: 0.3666\n",
      "epoch(50) lr is 0.0010000000474974513\n",
      "Epoch 51/200\n",
      "173056/176022 [============================>.] - ETA: 0s - loss: 2.1064 - accuracy: 0.2914\n",
      "Epoch 00051: val_loss improved from 2.07666 to 2.07086, saving model to CNN_dr0.5.h5\n",
      "176022/176022 [==============================] - 2s 11us/sample - loss: 2.1062 - accuracy: 0.2917 - val_loss: 2.0709 - val_accuracy: 0.3683\n",
      "epoch(51) lr is 0.0010000000474974513\n",
      "Epoch 52/200\n",
      "174080/176022 [============================>.] - ETA: 0s - loss: 2.0999 - accuracy: 0.2926\n",
      "Epoch 00052: val_loss improved from 2.07086 to 2.06510, saving model to CNN_dr0.5.h5\n",
      "176022/176022 [==============================] - 2s 11us/sample - loss: 2.1001 - accuracy: 0.2926 - val_loss: 2.0651 - val_accuracy: 0.3699\n",
      "epoch(52) lr is 0.0010000000474974513\n",
      "Epoch 53/200\n",
      "175104/176022 [============================>.] - ETA: 0s - loss: 2.0966 - accuracy: 0.2946\n",
      "Epoch 00053: val_loss improved from 2.06510 to 2.05941, saving model to CNN_dr0.5.h5\n",
      "176022/176022 [==============================] - 2s 11us/sample - loss: 2.0966 - accuracy: 0.2945 - val_loss: 2.0594 - val_accuracy: 0.3716\n",
      "epoch(53) lr is 0.0010000000474974513\n",
      "Epoch 54/200\n",
      "172032/176022 [============================>.] - ETA: 0s - loss: 2.0897 - accuracy: 0.2972\n",
      "Epoch 00054: val_loss improved from 2.05941 to 2.05358, saving model to CNN_dr0.5.h5\n",
      "176022/176022 [==============================] - 2s 11us/sample - loss: 2.0897 - accuracy: 0.2971 - val_loss: 2.0536 - val_accuracy: 0.3728\n",
      "epoch(54) lr is 0.0010000000474974513\n",
      "Epoch 55/200\n",
      "172032/176022 [============================>.] - ETA: 0s - loss: 2.0857 - accuracy: 0.2974\n",
      "Epoch 00055: val_loss improved from 2.05358 to 2.04794, saving model to CNN_dr0.5.h5\n",
      "176022/176022 [==============================] - 2s 11us/sample - loss: 2.0858 - accuracy: 0.2974 - val_loss: 2.0479 - val_accuracy: 0.3738\n",
      "epoch(55) lr is 0.0010000000474974513\n",
      "Epoch 56/200\n",
      "175104/176022 [============================>.] - ETA: 0s - loss: 2.0802 - accuracy: 0.3002\n",
      "Epoch 00056: val_loss improved from 2.04794 to 2.04232, saving model to CNN_dr0.5.h5\n",
      "176022/176022 [==============================] - 2s 11us/sample - loss: 2.0803 - accuracy: 0.3001 - val_loss: 2.0423 - val_accuracy: 0.3750\n",
      "epoch(56) lr is 0.0010000000474974513\n",
      "Epoch 57/200\n",
      "171008/176022 [============================>.] - ETA: 0s - loss: 2.0753 - accuracy: 0.3020\n",
      "Epoch 00057: val_loss improved from 2.04232 to 2.03672, saving model to CNN_dr0.5.h5\n",
      "176022/176022 [==============================] - 2s 11us/sample - loss: 2.0751 - accuracy: 0.3023 - val_loss: 2.0367 - val_accuracy: 0.3765\n",
      "epoch(57) lr is 0.0010000000474974513\n",
      "Epoch 58/200\n",
      "175104/176022 [============================>.] - ETA: 0s - loss: 2.0707 - accuracy: 0.3043\n",
      "Epoch 00058: val_loss improved from 2.03672 to 2.03123, saving model to CNN_dr0.5.h5\n",
      "176022/176022 [==============================] - 2s 11us/sample - loss: 2.0708 - accuracy: 0.3043 - val_loss: 2.0312 - val_accuracy: 0.3784\n",
      "epoch(58) lr is 0.0010000000474974513\n",
      "Epoch 59/200\n",
      "175104/176022 [============================>.] - ETA: 0s - loss: 2.0657 - accuracy: 0.3049\n",
      "Epoch 00059: val_loss improved from 2.03123 to 2.02570, saving model to CNN_dr0.5.h5\n",
      "176022/176022 [==============================] - 2s 11us/sample - loss: 2.0658 - accuracy: 0.3050 - val_loss: 2.0257 - val_accuracy: 0.3795\n",
      "epoch(59) lr is 0.0010000000474974513\n",
      "Epoch 60/200\n",
      "175104/176022 [============================>.] - ETA: 0s - loss: 2.0610 - accuracy: 0.3076\n",
      "Epoch 00060: val_loss improved from 2.02570 to 2.02018, saving model to CNN_dr0.5.h5\n",
      "176022/176022 [==============================] - 2s 11us/sample - loss: 2.0608 - accuracy: 0.3077 - val_loss: 2.0202 - val_accuracy: 0.3807\n",
      "epoch(60) lr is 0.0010000000474974513\n",
      "Epoch 61/200\n",
      "175104/176022 [============================>.] - ETA: 0s - loss: 2.0553 - accuracy: 0.3088\n",
      "Epoch 00061: val_loss improved from 2.02018 to 2.01475, saving model to CNN_dr0.5.h5\n",
      "176022/176022 [==============================] - 2s 11us/sample - loss: 2.0552 - accuracy: 0.3088 - val_loss: 2.0148 - val_accuracy: 0.3820\n",
      "epoch(61) lr is 0.0010000000474974513\n",
      "Epoch 62/200\n",
      "174080/176022 [============================>.] - ETA: 0s - loss: 2.0531 - accuracy: 0.3084\n",
      "Epoch 00062: val_loss improved from 2.01475 to 2.00950, saving model to CNN_dr0.5.h5\n",
      "176022/176022 [==============================] - 2s 11us/sample - loss: 2.0532 - accuracy: 0.3084 - val_loss: 2.0095 - val_accuracy: 0.3845\n",
      "epoch(62) lr is 0.0010000000474974513\n",
      "Epoch 63/200\n",
      "175104/176022 [============================>.] - ETA: 0s - loss: 2.0464 - accuracy: 0.3105\n",
      "Epoch 00063: val_loss improved from 2.00950 to 2.00420, saving model to CNN_dr0.5.h5\n",
      "176022/176022 [==============================] - 2s 11us/sample - loss: 2.0468 - accuracy: 0.3104 - val_loss: 2.0042 - val_accuracy: 0.3846\n",
      "epoch(63) lr is 0.0010000000474974513\n",
      "Epoch 64/200\n",
      "175104/176022 [============================>.] - ETA: 0s - loss: 2.0420 - accuracy: 0.3136\n",
      "Epoch 00064: val_loss improved from 2.00420 to 1.99901, saving model to CNN_dr0.5.h5\n",
      "176022/176022 [==============================] - 2s 11us/sample - loss: 2.0420 - accuracy: 0.3135 - val_loss: 1.9990 - val_accuracy: 0.3861\n",
      "epoch(64) lr is 0.0010000000474974513\n",
      "Epoch 65/200\n",
      "174080/176022 [============================>.] - ETA: 0s - loss: 2.0380 - accuracy: 0.3131\n",
      "Epoch 00065: val_loss improved from 1.99901 to 1.99389, saving model to CNN_dr0.5.h5\n",
      "176022/176022 [==============================] - 2s 11us/sample - loss: 2.0382 - accuracy: 0.3130 - val_loss: 1.9939 - val_accuracy: 0.3871\n",
      "epoch(65) lr is 0.0010000000474974513\n",
      "Epoch 66/200\n",
      "173056/176022 [============================>.] - ETA: 0s - loss: 2.0342 - accuracy: 0.3151\n",
      "Epoch 00066: val_loss improved from 1.99389 to 1.98885, saving model to CNN_dr0.5.h5\n",
      "176022/176022 [==============================] - 2s 11us/sample - loss: 2.0340 - accuracy: 0.3152 - val_loss: 1.9889 - val_accuracy: 0.3885\n",
      "epoch(66) lr is 0.0010000000474974513\n",
      "Epoch 67/200\n",
      "171008/176022 [============================>.] - ETA: 0s - loss: 2.0297 - accuracy: 0.3166\n",
      "Epoch 00067: val_loss improved from 1.98885 to 1.98385, saving model to CNN_dr0.5.h5\n",
      "176022/176022 [==============================] - 2s 11us/sample - loss: 2.0298 - accuracy: 0.3164 - val_loss: 1.9839 - val_accuracy: 0.3903\n",
      "epoch(67) lr is 0.0010000000474974513\n",
      "Epoch 68/200\n",
      "171008/176022 [============================>.] - ETA: 0s - loss: 2.0251 - accuracy: 0.3168\n",
      "Epoch 00068: val_loss improved from 1.98385 to 1.97891, saving model to CNN_dr0.5.h5\n",
      "176022/176022 [==============================] - 2s 11us/sample - loss: 2.0256 - accuracy: 0.3167 - val_loss: 1.9789 - val_accuracy: 0.3911\n",
      "epoch(68) lr is 0.0010000000474974513\n",
      "Epoch 69/200\n",
      "171008/176022 [============================>.] - ETA: 0s - loss: 2.0204 - accuracy: 0.3198\n",
      "Epoch 00069: val_loss improved from 1.97891 to 1.97402, saving model to CNN_dr0.5.h5\n",
      "176022/176022 [==============================] - 2s 11us/sample - loss: 2.0205 - accuracy: 0.3196 - val_loss: 1.9740 - val_accuracy: 0.3923\n",
      "epoch(69) lr is 0.0010000000474974513\n",
      "Epoch 70/200\n",
      "175104/176022 [============================>.] - ETA: 0s - loss: 2.0163 - accuracy: 0.3204\n",
      "Epoch 00070: val_loss improved from 1.97402 to 1.96924, saving model to CNN_dr0.5.h5\n",
      "176022/176022 [==============================] - 2s 11us/sample - loss: 2.0161 - accuracy: 0.3205 - val_loss: 1.9692 - val_accuracy: 0.3938\n",
      "epoch(70) lr is 0.0010000000474974513\n",
      "Epoch 71/200\n",
      "174080/176022 [============================>.] - ETA: 0s - loss: 2.0113 - accuracy: 0.3217\n",
      "Epoch 00071: val_loss improved from 1.96924 to 1.96438, saving model to CNN_dr0.5.h5\n",
      "176022/176022 [==============================] - 2s 11us/sample - loss: 2.0111 - accuracy: 0.3218 - val_loss: 1.9644 - val_accuracy: 0.3951\n",
      "epoch(71) lr is 0.0010000000474974513\n",
      "Epoch 72/200\n",
      "175104/176022 [============================>.] - ETA: 0s - loss: 2.0080 - accuracy: 0.3220\n",
      "Epoch 00072: val_loss improved from 1.96438 to 1.95964, saving model to CNN_dr0.5.h5\n",
      "176022/176022 [==============================] - 2s 11us/sample - loss: 2.0079 - accuracy: 0.3220 - val_loss: 1.9596 - val_accuracy: 0.3956\n",
      "epoch(72) lr is 0.0010000000474974513\n",
      "Epoch 73/200\n",
      "174080/176022 [============================>.] - ETA: 0s - loss: 2.0046 - accuracy: 0.3236\n",
      "Epoch 00073: val_loss improved from 1.95964 to 1.95495, saving model to CNN_dr0.5.h5\n",
      "176022/176022 [==============================] - 2s 11us/sample - loss: 2.0048 - accuracy: 0.3236 - val_loss: 1.9550 - val_accuracy: 0.3967\n",
      "epoch(73) lr is 0.0010000000474974513\n",
      "Epoch 74/200\n",
      "175104/176022 [============================>.] - ETA: 0s - loss: 2.0001 - accuracy: 0.3257\n",
      "Epoch 00074: val_loss improved from 1.95495 to 1.95025, saving model to CNN_dr0.5.h5\n",
      "176022/176022 [==============================] - 2s 11us/sample - loss: 2.0001 - accuracy: 0.3257 - val_loss: 1.9502 - val_accuracy: 0.3976\n",
      "epoch(74) lr is 0.0010000000474974513\n",
      "Epoch 75/200\n",
      "175104/176022 [============================>.] - ETA: 0s - loss: 1.9955 - accuracy: 0.3254\n",
      "Epoch 00075: val_loss improved from 1.95025 to 1.94554, saving model to CNN_dr0.5.h5\n",
      "176022/176022 [==============================] - 2s 11us/sample - loss: 1.9953 - accuracy: 0.3255 - val_loss: 1.9455 - val_accuracy: 0.3990\n",
      "epoch(75) lr is 0.0010000000474974513\n",
      "Epoch 76/200\n",
      "175104/176022 [============================>.] - ETA: 0s - loss: 1.9909 - accuracy: 0.3269\n",
      "Epoch 00076: val_loss improved from 1.94554 to 1.94084, saving model to CNN_dr0.5.h5\n",
      "176022/176022 [==============================] - 2s 11us/sample - loss: 1.9909 - accuracy: 0.3269 - val_loss: 1.9408 - val_accuracy: 0.3995\n",
      "epoch(76) lr is 0.0010000000474974513\n",
      "Epoch 77/200\n",
      "169984/176022 [===========================>..] - ETA: 0s - loss: 1.9874 - accuracy: 0.3280\n",
      "Epoch 00077: val_loss improved from 1.94084 to 1.93622, saving model to CNN_dr0.5.h5\n",
      "176022/176022 [==============================] - 2s 11us/sample - loss: 1.9874 - accuracy: 0.3283 - val_loss: 1.9362 - val_accuracy: 0.4000\n",
      "epoch(77) lr is 0.0010000000474974513\n",
      "Epoch 78/200\n",
      "174080/176022 [============================>.] - ETA: 0s - loss: 1.9844 - accuracy: 0.3300\n",
      "Epoch 00078: val_loss improved from 1.93622 to 1.93177, saving model to CNN_dr0.5.h5\n",
      "176022/176022 [==============================] - 2s 11us/sample - loss: 1.9840 - accuracy: 0.3301 - val_loss: 1.9318 - val_accuracy: 0.4008\n",
      "epoch(78) lr is 0.0010000000474974513\n",
      "Epoch 79/200\n",
      "171008/176022 [============================>.] - ETA: 0s - loss: 1.9785 - accuracy: 0.3310\n",
      "Epoch 00079: val_loss improved from 1.93177 to 1.92713, saving model to CNN_dr0.5.h5\n",
      "176022/176022 [==============================] - 2s 11us/sample - loss: 1.9785 - accuracy: 0.3308 - val_loss: 1.9271 - val_accuracy: 0.4015\n",
      "epoch(79) lr is 0.0010000000474974513\n",
      "Epoch 80/200\n",
      "175104/176022 [============================>.] - ETA: 0s - loss: 1.9755 - accuracy: 0.3319\n",
      "Epoch 00080: val_loss improved from 1.92713 to 1.92267, saving model to CNN_dr0.5.h5\n",
      "176022/176022 [==============================] - 2s 11us/sample - loss: 1.9757 - accuracy: 0.3318 - val_loss: 1.9227 - val_accuracy: 0.4023\n",
      "epoch(80) lr is 0.0010000000474974513\n",
      "Epoch 81/200\n",
      "175104/176022 [============================>.] - ETA: 0s - loss: 1.9715 - accuracy: 0.3316\n",
      "Epoch 00081: val_loss improved from 1.92267 to 1.91823, saving model to CNN_dr0.5.h5\n",
      "176022/176022 [==============================] - 2s 11us/sample - loss: 1.9714 - accuracy: 0.3316 - val_loss: 1.9182 - val_accuracy: 0.4034\n",
      "epoch(81) lr is 0.0010000000474974513\n",
      "Epoch 82/200\n",
      "175104/176022 [============================>.] - ETA: 0s - loss: 1.9682 - accuracy: 0.3343\n",
      "Epoch 00082: val_loss improved from 1.91823 to 1.91392, saving model to CNN_dr0.5.h5\n",
      "176022/176022 [==============================] - 2s 11us/sample - loss: 1.9683 - accuracy: 0.3342 - val_loss: 1.9139 - val_accuracy: 0.4044\n",
      "epoch(82) lr is 0.0010000000474974513\n",
      "Epoch 83/200\n",
      "175104/176022 [============================>.] - ETA: 0s - loss: 1.9623 - accuracy: 0.3356\n",
      "Epoch 00083: val_loss improved from 1.91392 to 1.90952, saving model to CNN_dr0.5.h5\n",
      "176022/176022 [==============================] - 2s 11us/sample - loss: 1.9622 - accuracy: 0.3357 - val_loss: 1.9095 - val_accuracy: 0.4052\n",
      "epoch(83) lr is 0.0010000000474974513\n",
      "Epoch 84/200\n",
      "175104/176022 [============================>.] - ETA: 0s - loss: 1.9592 - accuracy: 0.3365\n",
      "Epoch 00084: val_loss improved from 1.90952 to 1.90518, saving model to CNN_dr0.5.h5\n",
      "176022/176022 [==============================] - 2s 11us/sample - loss: 1.9591 - accuracy: 0.3366 - val_loss: 1.9052 - val_accuracy: 0.4062\n",
      "epoch(84) lr is 0.0010000000474974513\n",
      "Epoch 85/200\n",
      "175104/176022 [============================>.] - ETA: 0s - loss: 1.9540 - accuracy: 0.3390\n",
      "Epoch 00085: val_loss improved from 1.90518 to 1.90076, saving model to CNN_dr0.5.h5\n",
      "176022/176022 [==============================] - 2s 11us/sample - loss: 1.9540 - accuracy: 0.3390 - val_loss: 1.9008 - val_accuracy: 0.4073\n",
      "epoch(85) lr is 0.0010000000474974513\n",
      "Epoch 86/200\n",
      "175104/176022 [============================>.] - ETA: 0s - loss: 1.9508 - accuracy: 0.3399\n",
      "Epoch 00086: val_loss improved from 1.90076 to 1.89646, saving model to CNN_dr0.5.h5\n",
      "176022/176022 [==============================] - 2s 11us/sample - loss: 1.9507 - accuracy: 0.3399 - val_loss: 1.8965 - val_accuracy: 0.4080\n",
      "epoch(86) lr is 0.0010000000474974513\n",
      "Epoch 87/200\n",
      "175104/176022 [============================>.] - ETA: 0s - loss: 1.9477 - accuracy: 0.3404\n",
      "Epoch 00087: val_loss improved from 1.89646 to 1.89228, saving model to CNN_dr0.5.h5\n",
      "176022/176022 [==============================] - 2s 11us/sample - loss: 1.9479 - accuracy: 0.3405 - val_loss: 1.8923 - val_accuracy: 0.4085\n",
      "epoch(87) lr is 0.0010000000474974513\n",
      "Epoch 88/200\n",
      "175104/176022 [============================>.] - ETA: 0s - loss: 1.9431 - accuracy: 0.3410\n",
      "Epoch 00088: val_loss improved from 1.89228 to 1.88808, saving model to CNN_dr0.5.h5\n",
      "176022/176022 [==============================] - 2s 11us/sample - loss: 1.9433 - accuracy: 0.3409 - val_loss: 1.8881 - val_accuracy: 0.4090\n",
      "epoch(88) lr is 0.0010000000474974513\n",
      "Epoch 89/200\n",
      "174080/176022 [============================>.] - ETA: 0s - loss: 1.9396 - accuracy: 0.3430\n",
      "Epoch 00089: val_loss improved from 1.88808 to 1.88397, saving model to CNN_dr0.5.h5\n",
      "176022/176022 [==============================] - 2s 11us/sample - loss: 1.9395 - accuracy: 0.3430 - val_loss: 1.8840 - val_accuracy: 0.4099\n",
      "epoch(89) lr is 0.0010000000474974513\n",
      "Epoch 90/200\n",
      "175104/176022 [============================>.] - ETA: 0s - loss: 1.9355 - accuracy: 0.3425\n",
      "Epoch 00090: val_loss improved from 1.88397 to 1.87983, saving model to CNN_dr0.5.h5\n",
      "176022/176022 [==============================] - 2s 11us/sample - loss: 1.9353 - accuracy: 0.3425 - val_loss: 1.8798 - val_accuracy: 0.4105\n",
      "epoch(90) lr is 0.0010000000474974513\n",
      "Epoch 91/200\n",
      "175104/176022 [============================>.] - ETA: 0s - loss: 1.9338 - accuracy: 0.3433\n",
      "Epoch 00091: val_loss improved from 1.87983 to 1.87583, saving model to CNN_dr0.5.h5\n",
      "176022/176022 [==============================] - 2s 11us/sample - loss: 1.9335 - accuracy: 0.3433 - val_loss: 1.8758 - val_accuracy: 0.4114\n",
      "epoch(91) lr is 0.0010000000474974513\n",
      "Epoch 92/200\n",
      "175104/176022 [============================>.] - ETA: 0s - loss: 1.9297 - accuracy: 0.3436\n",
      "Epoch 00092: val_loss improved from 1.87583 to 1.87200, saving model to CNN_dr0.5.h5\n",
      "176022/176022 [==============================] - 2s 11us/sample - loss: 1.9298 - accuracy: 0.3436 - val_loss: 1.8720 - val_accuracy: 0.4117\n",
      "epoch(92) lr is 0.0010000000474974513\n",
      "Epoch 93/200\n",
      "175104/176022 [============================>.] - ETA: 0s - loss: 1.9254 - accuracy: 0.3456\n",
      "Epoch 00093: val_loss improved from 1.87200 to 1.86807, saving model to CNN_dr0.5.h5\n",
      "176022/176022 [==============================] - 2s 11us/sample - loss: 1.9253 - accuracy: 0.3457 - val_loss: 1.8681 - val_accuracy: 0.4131\n",
      "epoch(93) lr is 0.0010000000474974513\n",
      "Epoch 94/200\n",
      "174080/176022 [============================>.] - ETA: 0s - loss: 1.9212 - accuracy: 0.3453\n",
      "Epoch 00094: val_loss improved from 1.86807 to 1.86420, saving model to CNN_dr0.5.h5\n",
      "176022/176022 [==============================] - 2s 11us/sample - loss: 1.9214 - accuracy: 0.3451 - val_loss: 1.8642 - val_accuracy: 0.4131\n",
      "epoch(94) lr is 0.0010000000474974513\n",
      "Epoch 95/200\n",
      "175104/176022 [============================>.] - ETA: 0s - loss: 1.9183 - accuracy: 0.3488\n",
      "Epoch 00095: val_loss improved from 1.86420 to 1.86038, saving model to CNN_dr0.5.h5\n",
      "176022/176022 [==============================] - 2s 11us/sample - loss: 1.9182 - accuracy: 0.3488 - val_loss: 1.8604 - val_accuracy: 0.4133\n",
      "epoch(95) lr is 0.0010000000474974513\n",
      "Epoch 96/200\n",
      "171008/176022 [============================>.] - ETA: 0s - loss: 1.9152 - accuracy: 0.3483\n",
      "Epoch 00096: val_loss improved from 1.86038 to 1.85661, saving model to CNN_dr0.5.h5\n",
      "176022/176022 [==============================] - 2s 11us/sample - loss: 1.9151 - accuracy: 0.3485 - val_loss: 1.8566 - val_accuracy: 0.4141\n",
      "epoch(96) lr is 0.0010000000474974513\n",
      "Epoch 97/200\n",
      "175104/176022 [============================>.] - ETA: 0s - loss: 1.9112 - accuracy: 0.3497\n",
      "Epoch 00097: val_loss improved from 1.85661 to 1.85282, saving model to CNN_dr0.5.h5\n",
      "176022/176022 [==============================] - 2s 11us/sample - loss: 1.9112 - accuracy: 0.3496 - val_loss: 1.8528 - val_accuracy: 0.4152\n",
      "epoch(97) lr is 0.0010000000474974513\n",
      "Epoch 98/200\n",
      "175104/176022 [============================>.] - ETA: 0s - loss: 1.9078 - accuracy: 0.3506\n",
      "Epoch 00098: val_loss improved from 1.85282 to 1.84901, saving model to CNN_dr0.5.h5\n",
      "176022/176022 [==============================] - 2s 11us/sample - loss: 1.9077 - accuracy: 0.3507 - val_loss: 1.8490 - val_accuracy: 0.4160\n",
      "epoch(98) lr is 0.0010000000474974513\n",
      "Epoch 99/200\n",
      "175104/176022 [============================>.] - ETA: 0s - loss: 1.9034 - accuracy: 0.3518\n",
      "Epoch 00099: val_loss improved from 1.84901 to 1.84527, saving model to CNN_dr0.5.h5\n",
      "176022/176022 [==============================] - 2s 11us/sample - loss: 1.9036 - accuracy: 0.3517 - val_loss: 1.8453 - val_accuracy: 0.4167\n",
      "epoch(99) lr is 0.0010000000474974513\n",
      "Epoch 100/200\n",
      "175104/176022 [============================>.] - ETA: 0s - loss: 1.9018 - accuracy: 0.3530\n",
      "Epoch 00100: val_loss improved from 1.84527 to 1.84170, saving model to CNN_dr0.5.h5\n",
      "176022/176022 [==============================] - 2s 11us/sample - loss: 1.9016 - accuracy: 0.3530 - val_loss: 1.8417 - val_accuracy: 0.4175\n",
      "epoch(100) lr is 0.0010000000474974513\n",
      "Epoch 101/200\n",
      "175104/176022 [============================>.] - ETA: 0s - loss: 1.8973 - accuracy: 0.3543\n",
      "Epoch 00101: val_loss improved from 1.84170 to 1.83813, saving model to CNN_dr0.5.h5\n",
      "176022/176022 [==============================] - 2s 11us/sample - loss: 1.8974 - accuracy: 0.3542 - val_loss: 1.8381 - val_accuracy: 0.4179\n",
      "epoch(101) lr is 0.0010000000474974513\n",
      "Epoch 102/200\n",
      "175104/176022 [============================>.] - ETA: 0s - loss: 1.8944 - accuracy: 0.3556\n",
      "Epoch 00102: val_loss improved from 1.83813 to 1.83456, saving model to CNN_dr0.5.h5\n",
      "176022/176022 [==============================] - 2s 11us/sample - loss: 1.8944 - accuracy: 0.3556 - val_loss: 1.8346 - val_accuracy: 0.4183\n",
      "epoch(102) lr is 0.0010000000474974513\n",
      "Epoch 103/200\n",
      "171008/176022 [============================>.] - ETA: 0s - loss: 1.8907 - accuracy: 0.3544\n",
      "Epoch 00103: val_loss improved from 1.83456 to 1.83109, saving model to CNN_dr0.5.h5\n",
      "176022/176022 [==============================] - 2s 11us/sample - loss: 1.8908 - accuracy: 0.3544 - val_loss: 1.8311 - val_accuracy: 0.4190\n",
      "epoch(103) lr is 0.0010000000474974513\n",
      "Epoch 104/200\n",
      "175104/176022 [============================>.] - ETA: 0s - loss: 1.8886 - accuracy: 0.3571\n",
      "Epoch 00104: val_loss improved from 1.83109 to 1.82760, saving model to CNN_dr0.5.h5\n",
      "176022/176022 [==============================] - 2s 11us/sample - loss: 1.8886 - accuracy: 0.3570 - val_loss: 1.8276 - val_accuracy: 0.4196\n",
      "epoch(104) lr is 0.0010000000474974513\n",
      "Epoch 105/200\n",
      "175104/176022 [============================>.] - ETA: 0s - loss: 1.8839 - accuracy: 0.3575\n",
      "Epoch 00105: val_loss improved from 1.82760 to 1.82408, saving model to CNN_dr0.5.h5\n",
      "176022/176022 [==============================] - 2s 11us/sample - loss: 1.8840 - accuracy: 0.3575 - val_loss: 1.8241 - val_accuracy: 0.4198\n",
      "epoch(105) lr is 0.0010000000474974513\n",
      "Epoch 106/200\n",
      "175104/176022 [============================>.] - ETA: 0s - loss: 1.8806 - accuracy: 0.3576\n",
      "Epoch 00106: val_loss improved from 1.82408 to 1.82072, saving model to CNN_dr0.5.h5\n",
      "176022/176022 [==============================] - 2s 11us/sample - loss: 1.8805 - accuracy: 0.3577 - val_loss: 1.8207 - val_accuracy: 0.4201\n",
      "epoch(106) lr is 0.0010000000474974513\n",
      "Epoch 107/200\n",
      "175104/176022 [============================>.] - ETA: 0s - loss: 1.8797 - accuracy: 0.3578\n",
      "Epoch 00107: val_loss improved from 1.82072 to 1.81748, saving model to CNN_dr0.5.h5\n",
      "176022/176022 [==============================] - 2s 11us/sample - loss: 1.8796 - accuracy: 0.3578 - val_loss: 1.8175 - val_accuracy: 0.4213\n",
      "epoch(107) lr is 0.0010000000474974513\n",
      "Epoch 108/200\n",
      "175104/176022 [============================>.] - ETA: 0s - loss: 1.8758 - accuracy: 0.3610\n",
      "Epoch 00108: val_loss improved from 1.81748 to 1.81412, saving model to CNN_dr0.5.h5\n",
      "176022/176022 [==============================] - 2s 11us/sample - loss: 1.8757 - accuracy: 0.3609 - val_loss: 1.8141 - val_accuracy: 0.4212\n",
      "epoch(108) lr is 0.0010000000474974513\n",
      "Epoch 109/200\n",
      "171008/176022 [============================>.] - ETA: 0s - loss: 1.8714 - accuracy: 0.3604\n",
      "Epoch 00109: val_loss improved from 1.81412 to 1.81075, saving model to CNN_dr0.5.h5\n",
      "176022/176022 [==============================] - 2s 11us/sample - loss: 1.8711 - accuracy: 0.3605 - val_loss: 1.8107 - val_accuracy: 0.4218\n",
      "epoch(109) lr is 0.0010000000474974513\n",
      "Epoch 110/200\n",
      "172032/176022 [============================>.] - ETA: 0s - loss: 1.8702 - accuracy: 0.3605\n",
      "Epoch 00110: val_loss improved from 1.81075 to 1.80753, saving model to CNN_dr0.5.h5\n",
      "176022/176022 [==============================] - 2s 11us/sample - loss: 1.8702 - accuracy: 0.3604 - val_loss: 1.8075 - val_accuracy: 0.4224\n",
      "epoch(110) lr is 0.0010000000474974513\n",
      "Epoch 111/200\n",
      "175104/176022 [============================>.] - ETA: 0s - loss: 1.8651 - accuracy: 0.3614\n",
      "Epoch 00111: val_loss improved from 1.80753 to 1.80426, saving model to CNN_dr0.5.h5\n",
      "176022/176022 [==============================] - 2s 12us/sample - loss: 1.8650 - accuracy: 0.3615 - val_loss: 1.8043 - val_accuracy: 0.4229\n",
      "epoch(111) lr is 0.0010000000474974513\n",
      "Epoch 112/200\n",
      "175104/176022 [============================>.] - ETA: 0s - loss: 1.8608 - accuracy: 0.3638\n",
      "Epoch 00112: val_loss improved from 1.80426 to 1.80101, saving model to CNN_dr0.5.h5\n",
      "176022/176022 [==============================] - 2s 11us/sample - loss: 1.8609 - accuracy: 0.3638 - val_loss: 1.8010 - val_accuracy: 0.4239\n",
      "epoch(112) lr is 0.0010000000474974513\n",
      "Epoch 113/200\n",
      "175104/176022 [============================>.] - ETA: 0s - loss: 1.8596 - accuracy: 0.3631\n",
      "Epoch 00113: val_loss improved from 1.80101 to 1.79786, saving model to CNN_dr0.5.h5\n",
      "176022/176022 [==============================] - 2s 11us/sample - loss: 1.8597 - accuracy: 0.3631 - val_loss: 1.7979 - val_accuracy: 0.4245\n",
      "epoch(113) lr is 0.0010000000474974513\n",
      "Epoch 114/200\n",
      "174080/176022 [============================>.] - ETA: 0s - loss: 1.8555 - accuracy: 0.3638\n",
      "Epoch 00114: val_loss improved from 1.79786 to 1.79479, saving model to CNN_dr0.5.h5\n",
      "176022/176022 [==============================] - 2s 11us/sample - loss: 1.8559 - accuracy: 0.3637 - val_loss: 1.7948 - val_accuracy: 0.4255\n",
      "epoch(114) lr is 0.0010000000474974513\n",
      "Epoch 115/200\n",
      "175104/176022 [============================>.] - ETA: 0s - loss: 1.8535 - accuracy: 0.3659\n",
      "Epoch 00115: val_loss improved from 1.79479 to 1.79171, saving model to CNN_dr0.5.h5\n",
      "176022/176022 [==============================] - 2s 11us/sample - loss: 1.8535 - accuracy: 0.3658 - val_loss: 1.7917 - val_accuracy: 0.4259\n",
      "epoch(115) lr is 0.0010000000474974513\n",
      "Epoch 116/200\n",
      "175104/176022 [============================>.] - ETA: 0s - loss: 1.8511 - accuracy: 0.3670\n",
      "Epoch 00116: val_loss improved from 1.79171 to 1.78859, saving model to CNN_dr0.5.h5\n",
      "176022/176022 [==============================] - 2s 11us/sample - loss: 1.8515 - accuracy: 0.3669 - val_loss: 1.7886 - val_accuracy: 0.4262\n",
      "epoch(116) lr is 0.0010000000474974513\n",
      "Epoch 117/200\n",
      "175104/176022 [============================>.] - ETA: 0s - loss: 1.8472 - accuracy: 0.3676\n",
      "Epoch 00117: val_loss improved from 1.78859 to 1.78553, saving model to CNN_dr0.5.h5\n",
      "176022/176022 [==============================] - 2s 11us/sample - loss: 1.8472 - accuracy: 0.3675 - val_loss: 1.7855 - val_accuracy: 0.4265\n",
      "epoch(117) lr is 0.0010000000474974513\n",
      "Epoch 118/200\n",
      "175104/176022 [============================>.] - ETA: 0s - loss: 1.8451 - accuracy: 0.3665\n",
      "Epoch 00118: val_loss improved from 1.78553 to 1.78253, saving model to CNN_dr0.5.h5\n",
      "176022/176022 [==============================] - 2s 11us/sample - loss: 1.8451 - accuracy: 0.3665 - val_loss: 1.7825 - val_accuracy: 0.4279\n",
      "epoch(118) lr is 0.0010000000474974513\n",
      "Epoch 119/200\n",
      "175104/176022 [============================>.] - ETA: 0s - loss: 1.8418 - accuracy: 0.3687\n",
      "Epoch 00119: val_loss improved from 1.78253 to 1.77962, saving model to CNN_dr0.5.h5\n",
      "176022/176022 [==============================] - 2s 11us/sample - loss: 1.8417 - accuracy: 0.3689 - val_loss: 1.7796 - val_accuracy: 0.4276\n",
      "epoch(119) lr is 0.0010000000474974513\n",
      "Epoch 120/200\n",
      "175104/176022 [============================>.] - ETA: 0s - loss: 1.8374 - accuracy: 0.3704\n",
      "Epoch 00120: val_loss improved from 1.77962 to 1.77660, saving model to CNN_dr0.5.h5\n",
      "176022/176022 [==============================] - 2s 11us/sample - loss: 1.8374 - accuracy: 0.3704 - val_loss: 1.7766 - val_accuracy: 0.4280\n",
      "epoch(120) lr is 0.0010000000474974513\n",
      "Epoch 121/200\n",
      "175104/176022 [============================>.] - ETA: 0s - loss: 1.8367 - accuracy: 0.3706\n",
      "Epoch 00121: val_loss improved from 1.77660 to 1.77375, saving model to CNN_dr0.5.h5\n",
      "176022/176022 [==============================] - 2s 11us/sample - loss: 1.8367 - accuracy: 0.3706 - val_loss: 1.7738 - val_accuracy: 0.4285\n",
      "epoch(121) lr is 0.0010000000474974513\n",
      "Epoch 122/200\n",
      "175104/176022 [============================>.] - ETA: 0s - loss: 1.8346 - accuracy: 0.3707\n",
      "Epoch 00122: val_loss improved from 1.77375 to 1.77101, saving model to CNN_dr0.5.h5\n",
      "176022/176022 [==============================] - 2s 11us/sample - loss: 1.8344 - accuracy: 0.3708 - val_loss: 1.7710 - val_accuracy: 0.4290\n",
      "epoch(122) lr is 0.0010000000474974513\n",
      "Epoch 123/200\n",
      "174080/176022 [============================>.] - ETA: 0s - loss: 1.8315 - accuracy: 0.3704\n",
      "Epoch 00123: val_loss improved from 1.77101 to 1.76820, saving model to CNN_dr0.5.h5\n",
      "176022/176022 [==============================] - 2s 11us/sample - loss: 1.8319 - accuracy: 0.3704 - val_loss: 1.7682 - val_accuracy: 0.4294\n",
      "epoch(123) lr is 0.0010000000474974513\n",
      "Epoch 124/200\n",
      "175104/176022 [============================>.] - ETA: 0s - loss: 1.8290 - accuracy: 0.3724\n",
      "Epoch 00124: val_loss improved from 1.76820 to 1.76548, saving model to CNN_dr0.5.h5\n",
      "176022/176022 [==============================] - 2s 11us/sample - loss: 1.8290 - accuracy: 0.3723 - val_loss: 1.7655 - val_accuracy: 0.4297\n",
      "epoch(124) lr is 0.0010000000474974513\n",
      "Epoch 125/200\n",
      "175104/176022 [============================>.] - ETA: 0s - loss: 1.8267 - accuracy: 0.3725\n",
      "Epoch 00125: val_loss improved from 1.76548 to 1.76265, saving model to CNN_dr0.5.h5\n",
      "176022/176022 [==============================] - 2s 11us/sample - loss: 1.8267 - accuracy: 0.3725 - val_loss: 1.7626 - val_accuracy: 0.4304\n",
      "epoch(125) lr is 0.0010000000474974513\n",
      "Epoch 126/200\n",
      "175104/176022 [============================>.] - ETA: 0s - loss: 1.8231 - accuracy: 0.3733\n",
      "Epoch 00126: val_loss improved from 1.76265 to 1.75989, saving model to CNN_dr0.5.h5\n",
      "176022/176022 [==============================] - 2s 11us/sample - loss: 1.8230 - accuracy: 0.3734 - val_loss: 1.7599 - val_accuracy: 0.4303\n",
      "epoch(126) lr is 0.0010000000474974513\n",
      "Epoch 127/200\n",
      "175104/176022 [============================>.] - ETA: 0s - loss: 1.8198 - accuracy: 0.3757\n",
      "Epoch 00127: val_loss improved from 1.75989 to 1.75705, saving model to CNN_dr0.5.h5\n",
      "176022/176022 [==============================] - 2s 11us/sample - loss: 1.8197 - accuracy: 0.3756 - val_loss: 1.7570 - val_accuracy: 0.4305\n",
      "epoch(127) lr is 0.0010000000474974513\n",
      "Epoch 128/200\n",
      "175104/176022 [============================>.] - ETA: 0s - loss: 1.8183 - accuracy: 0.3745\n",
      "Epoch 00128: val_loss improved from 1.75705 to 1.75445, saving model to CNN_dr0.5.h5\n",
      "176022/176022 [==============================] - 2s 11us/sample - loss: 1.8183 - accuracy: 0.3746 - val_loss: 1.7545 - val_accuracy: 0.4312\n",
      "epoch(128) lr is 0.0010000000474974513\n",
      "Epoch 129/200\n",
      "175104/176022 [============================>.] - ETA: 0s - loss: 1.8159 - accuracy: 0.3766\n",
      "Epoch 00129: val_loss improved from 1.75445 to 1.75179, saving model to CNN_dr0.5.h5\n",
      "176022/176022 [==============================] - 2s 11us/sample - loss: 1.8156 - accuracy: 0.3768 - val_loss: 1.7518 - val_accuracy: 0.4316\n",
      "epoch(129) lr is 0.0010000000474974513\n",
      "Epoch 130/200\n",
      "175104/176022 [============================>.] - ETA: 0s - loss: 1.8137 - accuracy: 0.3759\n",
      "Epoch 00130: val_loss improved from 1.75179 to 1.74920, saving model to CNN_dr0.5.h5\n",
      "176022/176022 [==============================] - 2s 11us/sample - loss: 1.8137 - accuracy: 0.3759 - val_loss: 1.7492 - val_accuracy: 0.4318\n",
      "epoch(130) lr is 0.0010000000474974513\n",
      "Epoch 131/200\n",
      "175104/176022 [============================>.] - ETA: 0s - loss: 1.8102 - accuracy: 0.3770\n",
      "Epoch 00131: val_loss improved from 1.74920 to 1.74665, saving model to CNN_dr0.5.h5\n",
      "176022/176022 [==============================] - 2s 11us/sample - loss: 1.8101 - accuracy: 0.3770 - val_loss: 1.7466 - val_accuracy: 0.4324\n",
      "epoch(131) lr is 0.0010000000474974513\n",
      "Epoch 132/200\n",
      "171008/176022 [============================>.] - ETA: 0s - loss: 1.8059 - accuracy: 0.3773\n",
      "Epoch 00132: val_loss improved from 1.74665 to 1.74404, saving model to CNN_dr0.5.h5\n",
      "176022/176022 [==============================] - 2s 11us/sample - loss: 1.8064 - accuracy: 0.3773 - val_loss: 1.7440 - val_accuracy: 0.4325\n",
      "epoch(132) lr is 0.0010000000474974513\n",
      "Epoch 133/200\n",
      "175104/176022 [============================>.] - ETA: 0s - loss: 1.8051 - accuracy: 0.3801\n",
      "Epoch 00133: val_loss improved from 1.74404 to 1.74145, saving model to CNN_dr0.5.h5\n",
      "176022/176022 [==============================] - 2s 11us/sample - loss: 1.8053 - accuracy: 0.3800 - val_loss: 1.7415 - val_accuracy: 0.4326\n",
      "epoch(133) lr is 0.0010000000474974513\n",
      "Epoch 134/200\n",
      "175104/176022 [============================>.] - ETA: 0s - loss: 1.8019 - accuracy: 0.3791\n",
      "Epoch 00134: val_loss improved from 1.74145 to 1.73890, saving model to CNN_dr0.5.h5\n",
      "176022/176022 [==============================] - 2s 11us/sample - loss: 1.8018 - accuracy: 0.3790 - val_loss: 1.7389 - val_accuracy: 0.4333\n",
      "epoch(134) lr is 0.0010000000474974513\n",
      "Epoch 135/200\n",
      "175104/176022 [============================>.] - ETA: 0s - loss: 1.7998 - accuracy: 0.3805\n",
      "Epoch 00135: val_loss improved from 1.73890 to 1.73648, saving model to CNN_dr0.5.h5\n",
      "176022/176022 [==============================] - 2s 11us/sample - loss: 1.7997 - accuracy: 0.3806 - val_loss: 1.7365 - val_accuracy: 0.4338\n",
      "epoch(135) lr is 0.0010000000474974513\n",
      "Epoch 136/200\n",
      "175104/176022 [============================>.] - ETA: 0s - loss: 1.7981 - accuracy: 0.3803\n",
      "Epoch 00136: val_loss improved from 1.73648 to 1.73404, saving model to CNN_dr0.5.h5\n",
      "176022/176022 [==============================] - 2s 11us/sample - loss: 1.7980 - accuracy: 0.3803 - val_loss: 1.7340 - val_accuracy: 0.4341\n",
      "epoch(136) lr is 0.0010000000474974513\n",
      "Epoch 137/200\n",
      "175104/176022 [============================>.] - ETA: 0s - loss: 1.7938 - accuracy: 0.3810\n",
      "Epoch 00137: val_loss improved from 1.73404 to 1.73151, saving model to CNN_dr0.5.h5\n",
      "176022/176022 [==============================] - 2s 11us/sample - loss: 1.7939 - accuracy: 0.3809 - val_loss: 1.7315 - val_accuracy: 0.4342\n",
      "epoch(137) lr is 0.0010000000474974513\n",
      "Epoch 138/200\n",
      "175104/176022 [============================>.] - ETA: 0s - loss: 1.7917 - accuracy: 0.3827\n",
      "Epoch 00138: val_loss improved from 1.73151 to 1.72905, saving model to CNN_dr0.5.h5\n",
      "176022/176022 [==============================] - 2s 11us/sample - loss: 1.7915 - accuracy: 0.3827 - val_loss: 1.7290 - val_accuracy: 0.4354\n",
      "epoch(138) lr is 0.0010000000474974513\n",
      "Epoch 139/200\n",
      "175104/176022 [============================>.] - ETA: 0s - loss: 1.7900 - accuracy: 0.3828\n",
      "Epoch 00139: val_loss improved from 1.72905 to 1.72664, saving model to CNN_dr0.5.h5\n",
      "176022/176022 [==============================] - 2s 11us/sample - loss: 1.7901 - accuracy: 0.3828 - val_loss: 1.7266 - val_accuracy: 0.4349\n",
      "epoch(139) lr is 0.0010000000474974513\n",
      "Epoch 140/200\n",
      "175104/176022 [============================>.] - ETA: 0s - loss: 1.7895 - accuracy: 0.3824\n",
      "Epoch 00140: val_loss improved from 1.72664 to 1.72440, saving model to CNN_dr0.5.h5\n",
      "176022/176022 [==============================] - 2s 11us/sample - loss: 1.7896 - accuracy: 0.3824 - val_loss: 1.7244 - val_accuracy: 0.4361\n",
      "epoch(140) lr is 0.0010000000474974513\n",
      "Epoch 141/200\n",
      "175104/176022 [============================>.] - ETA: 0s - loss: 1.7855 - accuracy: 0.3835\n",
      "Epoch 00141: val_loss improved from 1.72440 to 1.72194, saving model to CNN_dr0.5.h5\n",
      "176022/176022 [==============================] - 2s 11us/sample - loss: 1.7854 - accuracy: 0.3836 - val_loss: 1.7219 - val_accuracy: 0.4361\n",
      "epoch(141) lr is 0.0010000000474974513\n",
      "Epoch 142/200\n",
      "174080/176022 [============================>.] - ETA: 0s - loss: 1.7820 - accuracy: 0.3841\n",
      "Epoch 00142: val_loss improved from 1.72194 to 1.71953, saving model to CNN_dr0.5.h5\n",
      "176022/176022 [==============================] - 2s 11us/sample - loss: 1.7821 - accuracy: 0.3840 - val_loss: 1.7195 - val_accuracy: 0.4365\n",
      "epoch(142) lr is 0.0010000000474974513\n",
      "Epoch 143/200\n",
      "173056/176022 [============================>.] - ETA: 0s - loss: 1.7814 - accuracy: 0.3846\n",
      "Epoch 00143: val_loss improved from 1.71953 to 1.71724, saving model to CNN_dr0.5.h5\n",
      "176022/176022 [==============================] - 2s 11us/sample - loss: 1.7815 - accuracy: 0.3844 - val_loss: 1.7172 - val_accuracy: 0.4368\n",
      "epoch(143) lr is 0.0010000000474974513\n",
      "Epoch 144/200\n",
      "175104/176022 [============================>.] - ETA: 0s - loss: 1.7797 - accuracy: 0.3847\n",
      "Epoch 00144: val_loss improved from 1.71724 to 1.71492, saving model to CNN_dr0.5.h5\n",
      "176022/176022 [==============================] - 2s 11us/sample - loss: 1.7795 - accuracy: 0.3848 - val_loss: 1.7149 - val_accuracy: 0.4371\n",
      "epoch(144) lr is 0.0010000000474974513\n",
      "Epoch 145/200\n",
      "175104/176022 [============================>.] - ETA: 0s - loss: 1.7757 - accuracy: 0.3863\n",
      "Epoch 00145: val_loss improved from 1.71492 to 1.71266, saving model to CNN_dr0.5.h5\n",
      "176022/176022 [==============================] - 2s 11us/sample - loss: 1.7759 - accuracy: 0.3862 - val_loss: 1.7127 - val_accuracy: 0.4373\n",
      "epoch(145) lr is 0.0010000000474974513\n",
      "Epoch 146/200\n",
      "175104/176022 [============================>.] - ETA: 0s - loss: 1.7720 - accuracy: 0.3874\n",
      "Epoch 00146: val_loss improved from 1.71266 to 1.71033, saving model to CNN_dr0.5.h5\n",
      "176022/176022 [==============================] - 2s 11us/sample - loss: 1.7720 - accuracy: 0.3874 - val_loss: 1.7103 - val_accuracy: 0.4375\n",
      "epoch(146) lr is 0.0010000000474974513\n",
      "Epoch 147/200\n",
      "175104/176022 [============================>.] - ETA: 0s - loss: 1.7718 - accuracy: 0.3882\n",
      "Epoch 00147: val_loss improved from 1.71033 to 1.70817, saving model to CNN_dr0.5.h5\n",
      "176022/176022 [==============================] - 2s 11us/sample - loss: 1.7719 - accuracy: 0.3882 - val_loss: 1.7082 - val_accuracy: 0.4382\n",
      "epoch(147) lr is 0.0010000000474974513\n",
      "Epoch 148/200\n",
      "175104/176022 [============================>.] - ETA: 0s - loss: 1.7696 - accuracy: 0.3877\n",
      "Epoch 00148: val_loss improved from 1.70817 to 1.70597, saving model to CNN_dr0.5.h5\n",
      "176022/176022 [==============================] - 2s 11us/sample - loss: 1.7693 - accuracy: 0.3877 - val_loss: 1.7060 - val_accuracy: 0.4385\n",
      "epoch(148) lr is 0.0010000000474974513\n",
      "Epoch 149/200\n",
      "175104/176022 [============================>.] - ETA: 0s - loss: 1.7670 - accuracy: 0.3891\n",
      "Epoch 00149: val_loss improved from 1.70597 to 1.70372, saving model to CNN_dr0.5.h5\n",
      "176022/176022 [==============================] - 2s 11us/sample - loss: 1.7672 - accuracy: 0.3890 - val_loss: 1.7037 - val_accuracy: 0.4385\n",
      "epoch(149) lr is 0.0010000000474974513\n",
      "Epoch 150/200\n",
      "171008/176022 [============================>.] - ETA: 0s - loss: 1.7645 - accuracy: 0.3887\n",
      "Epoch 00150: val_loss improved from 1.70372 to 1.70151, saving model to CNN_dr0.5.h5\n",
      "176022/176022 [==============================] - 2s 11us/sample - loss: 1.7645 - accuracy: 0.3887 - val_loss: 1.7015 - val_accuracy: 0.4387\n",
      "epoch(150) lr is 0.0010000000474974513\n",
      "Epoch 151/200\n",
      "175104/176022 [============================>.] - ETA: 0s - loss: 1.7642 - accuracy: 0.3889\n",
      "Epoch 00151: val_loss improved from 1.70151 to 1.69947, saving model to CNN_dr0.5.h5\n",
      "176022/176022 [==============================] - 2s 11us/sample - loss: 1.7645 - accuracy: 0.3888 - val_loss: 1.6995 - val_accuracy: 0.4393\n",
      "epoch(151) lr is 0.0010000000474974513\n",
      "Epoch 152/200\n",
      "175104/176022 [============================>.] - ETA: 0s - loss: 1.7604 - accuracy: 0.3907\n",
      "Epoch 00152: val_loss improved from 1.69947 to 1.69736, saving model to CNN_dr0.5.h5\n",
      "176022/176022 [==============================] - 2s 11us/sample - loss: 1.7607 - accuracy: 0.3905 - val_loss: 1.6974 - val_accuracy: 0.4397\n",
      "epoch(152) lr is 0.0010000000474974513\n",
      "Epoch 153/200\n",
      "175104/176022 [============================>.] - ETA: 0s - loss: 1.7592 - accuracy: 0.3904\n",
      "Epoch 00153: val_loss improved from 1.69736 to 1.69528, saving model to CNN_dr0.5.h5\n",
      "176022/176022 [==============================] - 2s 11us/sample - loss: 1.7591 - accuracy: 0.3905 - val_loss: 1.6953 - val_accuracy: 0.4400\n",
      "epoch(153) lr is 0.0010000000474974513\n",
      "Epoch 154/200\n",
      "175104/176022 [============================>.] - ETA: 0s - loss: 1.7569 - accuracy: 0.3918\n",
      "Epoch 00154: val_loss improved from 1.69528 to 1.69323, saving model to CNN_dr0.5.h5\n",
      "176022/176022 [==============================] - 2s 11us/sample - loss: 1.7569 - accuracy: 0.3918 - val_loss: 1.6932 - val_accuracy: 0.4402\n",
      "epoch(154) lr is 0.0010000000474974513\n",
      "Epoch 155/200\n",
      "175104/176022 [============================>.] - ETA: 0s - loss: 1.7559 - accuracy: 0.3909\n",
      "Epoch 00155: val_loss improved from 1.69323 to 1.69121, saving model to CNN_dr0.5.h5\n",
      "176022/176022 [==============================] - 2s 11us/sample - loss: 1.7560 - accuracy: 0.3909 - val_loss: 1.6912 - val_accuracy: 0.4405\n",
      "epoch(155) lr is 0.0010000000474974513\n",
      "Epoch 156/200\n",
      "174080/176022 [============================>.] - ETA: 0s - loss: 1.7530 - accuracy: 0.3929\n",
      "Epoch 00156: val_loss improved from 1.69121 to 1.68917, saving model to CNN_dr0.5.h5\n",
      "176022/176022 [==============================] - 2s 11us/sample - loss: 1.7529 - accuracy: 0.3929 - val_loss: 1.6892 - val_accuracy: 0.4406\n",
      "epoch(156) lr is 0.0010000000474974513\n",
      "Epoch 157/200\n",
      "175104/176022 [============================>.] - ETA: 0s - loss: 1.7507 - accuracy: 0.3941\n",
      "Epoch 00157: val_loss improved from 1.68917 to 1.68717, saving model to CNN_dr0.5.h5\n",
      "176022/176022 [==============================] - 2s 11us/sample - loss: 1.7507 - accuracy: 0.3941 - val_loss: 1.6872 - val_accuracy: 0.4413\n",
      "epoch(157) lr is 0.0010000000474974513\n",
      "Epoch 158/200\n",
      "175104/176022 [============================>.] - ETA: 0s - loss: 1.7493 - accuracy: 0.3930\n",
      "Epoch 00158: val_loss improved from 1.68717 to 1.68517, saving model to CNN_dr0.5.h5\n",
      "176022/176022 [==============================] - 2s 11us/sample - loss: 1.7491 - accuracy: 0.3931 - val_loss: 1.6852 - val_accuracy: 0.4414\n",
      "epoch(158) lr is 0.0010000000474974513\n",
      "Epoch 159/200\n",
      "175104/176022 [============================>.] - ETA: 0s - loss: 1.7469 - accuracy: 0.3947\n",
      "Epoch 00159: val_loss improved from 1.68517 to 1.68317, saving model to CNN_dr0.5.h5\n",
      "176022/176022 [==============================] - 2s 11us/sample - loss: 1.7468 - accuracy: 0.3948 - val_loss: 1.6832 - val_accuracy: 0.4417\n",
      "epoch(159) lr is 0.0010000000474974513\n",
      "Epoch 160/200\n",
      "175104/176022 [============================>.] - ETA: 0s - loss: 1.7459 - accuracy: 0.3942\n",
      "Epoch 00160: val_loss improved from 1.68317 to 1.68129, saving model to CNN_dr0.5.h5\n",
      "176022/176022 [==============================] - 2s 11us/sample - loss: 1.7459 - accuracy: 0.3942 - val_loss: 1.6813 - val_accuracy: 0.4418\n",
      "epoch(160) lr is 0.0010000000474974513\n",
      "Epoch 161/200\n",
      "175104/176022 [============================>.] - ETA: 0s - loss: 1.7432 - accuracy: 0.3955\n",
      "Epoch 00161: val_loss improved from 1.68129 to 1.67948, saving model to CNN_dr0.5.h5\n",
      "176022/176022 [==============================] - 2s 11us/sample - loss: 1.7431 - accuracy: 0.3956 - val_loss: 1.6795 - val_accuracy: 0.4423\n",
      "epoch(161) lr is 0.0010000000474974513\n",
      "Epoch 162/200\n",
      "175104/176022 [============================>.] - ETA: 0s - loss: 1.7409 - accuracy: 0.3957\n",
      "Epoch 00162: val_loss improved from 1.67948 to 1.67751, saving model to CNN_dr0.5.h5\n",
      "176022/176022 [==============================] - 2s 11us/sample - loss: 1.7409 - accuracy: 0.3957 - val_loss: 1.6775 - val_accuracy: 0.4427\n",
      "epoch(162) lr is 0.0010000000474974513\n",
      "Epoch 163/200\n",
      "175104/176022 [============================>.] - ETA: 0s - loss: 1.7394 - accuracy: 0.3965\n",
      "Epoch 00163: val_loss improved from 1.67751 to 1.67562, saving model to CNN_dr0.5.h5\n",
      "176022/176022 [==============================] - 2s 11us/sample - loss: 1.7396 - accuracy: 0.3963 - val_loss: 1.6756 - val_accuracy: 0.4427\n",
      "epoch(163) lr is 0.0010000000474974513\n",
      "Epoch 164/200\n",
      "175104/176022 [============================>.] - ETA: 0s - loss: 1.7361 - accuracy: 0.3971\n",
      "Epoch 00164: val_loss improved from 1.67562 to 1.67369, saving model to CNN_dr0.5.h5\n",
      "176022/176022 [==============================] - 2s 11us/sample - loss: 1.7362 - accuracy: 0.3971 - val_loss: 1.6737 - val_accuracy: 0.4430\n",
      "epoch(164) lr is 0.0010000000474974513\n",
      "Epoch 165/200\n",
      "175104/176022 [============================>.] - ETA: 0s - loss: 1.7358 - accuracy: 0.3973\n",
      "Epoch 00165: val_loss improved from 1.67369 to 1.67193, saving model to CNN_dr0.5.h5\n",
      "176022/176022 [==============================] - 2s 11us/sample - loss: 1.7355 - accuracy: 0.3975 - val_loss: 1.6719 - val_accuracy: 0.4432\n",
      "epoch(165) lr is 0.0010000000474974513\n",
      "Epoch 166/200\n",
      "175104/176022 [============================>.] - ETA: 0s - loss: 1.7333 - accuracy: 0.3979\n",
      "Epoch 00166: val_loss improved from 1.67193 to 1.67008, saving model to CNN_dr0.5.h5\n",
      "176022/176022 [==============================] - 2s 11us/sample - loss: 1.7331 - accuracy: 0.3980 - val_loss: 1.6701 - val_accuracy: 0.4436\n",
      "epoch(166) lr is 0.0010000000474974513\n",
      "Epoch 167/200\n",
      "175104/176022 [============================>.] - ETA: 0s - loss: 1.7330 - accuracy: 0.3976\n",
      "Epoch 00167: val_loss improved from 1.67008 to 1.66829, saving model to CNN_dr0.5.h5\n",
      "176022/176022 [==============================] - 2s 11us/sample - loss: 1.7331 - accuracy: 0.3976 - val_loss: 1.6683 - val_accuracy: 0.4441\n",
      "epoch(167) lr is 0.0010000000474974513\n",
      "Epoch 168/200\n",
      "175104/176022 [============================>.] - ETA: 0s - loss: 1.7302 - accuracy: 0.3981\n",
      "Epoch 00168: val_loss improved from 1.66829 to 1.66648, saving model to CNN_dr0.5.h5\n",
      "176022/176022 [==============================] - 2s 12us/sample - loss: 1.7301 - accuracy: 0.3982 - val_loss: 1.6665 - val_accuracy: 0.4444\n",
      "epoch(168) lr is 0.0010000000474974513\n",
      "Epoch 169/200\n",
      "175104/176022 [============================>.] - ETA: 0s - loss: 1.7277 - accuracy: 0.3992\n",
      "Epoch 00169: val_loss improved from 1.66648 to 1.66474, saving model to CNN_dr0.5.h5\n",
      "176022/176022 [==============================] - 2s 11us/sample - loss: 1.7276 - accuracy: 0.3993 - val_loss: 1.6647 - val_accuracy: 0.4446\n",
      "epoch(169) lr is 0.0010000000474974513\n",
      "Epoch 170/200\n",
      "175104/176022 [============================>.] - ETA: 0s - loss: 1.7259 - accuracy: 0.3994\n",
      "Epoch 00170: val_loss improved from 1.66474 to 1.66303, saving model to CNN_dr0.5.h5\n",
      "176022/176022 [==============================] - 2s 11us/sample - loss: 1.7258 - accuracy: 0.3994 - val_loss: 1.6630 - val_accuracy: 0.4449\n",
      "epoch(170) lr is 0.0010000000474974513\n",
      "Epoch 171/200\n",
      "175104/176022 [============================>.] - ETA: 0s - loss: 1.7249 - accuracy: 0.3990\n",
      "Epoch 00171: val_loss improved from 1.66303 to 1.66131, saving model to CNN_dr0.5.h5\n",
      "176022/176022 [==============================] - 2s 11us/sample - loss: 1.7247 - accuracy: 0.3991 - val_loss: 1.6613 - val_accuracy: 0.4453\n",
      "epoch(171) lr is 0.0010000000474974513\n",
      "Epoch 172/200\n",
      "175104/176022 [============================>.] - ETA: 0s - loss: 1.7228 - accuracy: 0.3984\n",
      "Epoch 00172: val_loss improved from 1.66131 to 1.65967, saving model to CNN_dr0.5.h5\n",
      "176022/176022 [==============================] - 2s 11us/sample - loss: 1.7228 - accuracy: 0.3985 - val_loss: 1.6597 - val_accuracy: 0.4456\n",
      "epoch(172) lr is 0.0010000000474974513\n",
      "Epoch 173/200\n",
      "175104/176022 [============================>.] - ETA: 0s - loss: 1.7224 - accuracy: 0.4000\n",
      "Epoch 00173: val_loss improved from 1.65967 to 1.65797, saving model to CNN_dr0.5.h5\n",
      "176022/176022 [==============================] - 2s 11us/sample - loss: 1.7225 - accuracy: 0.4000 - val_loss: 1.6580 - val_accuracy: 0.4460\n",
      "epoch(173) lr is 0.0010000000474974513\n",
      "Epoch 174/200\n",
      "169984/176022 [===========================>..] - ETA: 0s - loss: 1.7199 - accuracy: 0.4008\n",
      "Epoch 00174: val_loss improved from 1.65797 to 1.65628, saving model to CNN_dr0.5.h5\n",
      "176022/176022 [==============================] - 2s 11us/sample - loss: 1.7196 - accuracy: 0.4009 - val_loss: 1.6563 - val_accuracy: 0.4462\n",
      "epoch(174) lr is 0.0010000000474974513\n",
      "Epoch 175/200\n",
      "175104/176022 [============================>.] - ETA: 0s - loss: 1.7188 - accuracy: 0.3991\n",
      "Epoch 00175: val_loss improved from 1.65628 to 1.65463, saving model to CNN_dr0.5.h5\n",
      "176022/176022 [==============================] - 2s 11us/sample - loss: 1.7188 - accuracy: 0.3990 - val_loss: 1.6546 - val_accuracy: 0.4462\n",
      "epoch(175) lr is 0.0010000000474974513\n",
      "Epoch 176/200\n",
      "175104/176022 [============================>.] - ETA: 0s - loss: 1.7162 - accuracy: 0.4025\n",
      "Epoch 00176: val_loss improved from 1.65463 to 1.65296, saving model to CNN_dr0.5.h5\n",
      "176022/176022 [==============================] - 2s 11us/sample - loss: 1.7162 - accuracy: 0.4024 - val_loss: 1.6530 - val_accuracy: 0.4466\n",
      "epoch(176) lr is 0.0010000000474974513\n",
      "Epoch 177/200\n",
      "175104/176022 [============================>.] - ETA: 0s - loss: 1.7150 - accuracy: 0.4023\n",
      "Epoch 00177: val_loss improved from 1.65296 to 1.65130, saving model to CNN_dr0.5.h5\n",
      "176022/176022 [==============================] - 2s 11us/sample - loss: 1.7150 - accuracy: 0.4023 - val_loss: 1.6513 - val_accuracy: 0.4467\n",
      "epoch(177) lr is 0.0010000000474974513\n",
      "Epoch 178/200\n",
      "175104/176022 [============================>.] - ETA: 0s - loss: 1.7117 - accuracy: 0.4036\n",
      "Epoch 00178: val_loss improved from 1.65130 to 1.64960, saving model to CNN_dr0.5.h5\n",
      "176022/176022 [==============================] - 2s 11us/sample - loss: 1.7118 - accuracy: 0.4036 - val_loss: 1.6496 - val_accuracy: 0.4468\n",
      "epoch(178) lr is 0.0010000000474974513\n",
      "Epoch 179/200\n",
      "175104/176022 [============================>.] - ETA: 0s - loss: 1.7112 - accuracy: 0.4037\n",
      "Epoch 00179: val_loss improved from 1.64960 to 1.64807, saving model to CNN_dr0.5.h5\n",
      "176022/176022 [==============================] - 2s 11us/sample - loss: 1.7115 - accuracy: 0.4036 - val_loss: 1.6481 - val_accuracy: 0.4472\n",
      "epoch(179) lr is 0.0010000000474974513\n",
      "Epoch 180/200\n",
      "175104/176022 [============================>.] - ETA: 0s - loss: 1.7084 - accuracy: 0.4045\n",
      "Epoch 00180: val_loss improved from 1.64807 to 1.64641, saving model to CNN_dr0.5.h5\n",
      "176022/176022 [==============================] - 2s 11us/sample - loss: 1.7084 - accuracy: 0.4046 - val_loss: 1.6464 - val_accuracy: 0.4475\n",
      "epoch(180) lr is 0.0010000000474974513\n",
      "Epoch 181/200\n",
      "175104/176022 [============================>.] - ETA: 0s - loss: 1.7092 - accuracy: 0.4038\n",
      "Epoch 00181: val_loss improved from 1.64641 to 1.64479, saving model to CNN_dr0.5.h5\n",
      "176022/176022 [==============================] - 2s 11us/sample - loss: 1.7091 - accuracy: 0.4039 - val_loss: 1.6448 - val_accuracy: 0.4476\n",
      "epoch(181) lr is 0.0010000000474974513\n",
      "Epoch 182/200\n",
      "175104/176022 [============================>.] - ETA: 0s - loss: 1.7046 - accuracy: 0.4045\n",
      "Epoch 00182: val_loss improved from 1.64479 to 1.64325, saving model to CNN_dr0.5.h5\n",
      "176022/176022 [==============================] - 2s 11us/sample - loss: 1.7046 - accuracy: 0.4045 - val_loss: 1.6433 - val_accuracy: 0.4478\n",
      "epoch(182) lr is 0.0010000000474974513\n",
      "Epoch 183/200\n",
      "175104/176022 [============================>.] - ETA: 0s - loss: 1.7041 - accuracy: 0.4036\n",
      "Epoch 00183: val_loss improved from 1.64325 to 1.64173, saving model to CNN_dr0.5.h5\n",
      "176022/176022 [==============================] - 2s 11us/sample - loss: 1.7039 - accuracy: 0.4036 - val_loss: 1.6417 - val_accuracy: 0.4482\n",
      "epoch(183) lr is 0.0010000000474974513\n",
      "Epoch 184/200\n",
      "175104/176022 [============================>.] - ETA: 0s - loss: 1.7036 - accuracy: 0.4041\n",
      "Epoch 00184: val_loss improved from 1.64173 to 1.64026, saving model to CNN_dr0.5.h5\n",
      "176022/176022 [==============================] - 2s 11us/sample - loss: 1.7035 - accuracy: 0.4041 - val_loss: 1.6403 - val_accuracy: 0.4485\n",
      "epoch(184) lr is 0.0010000000474974513\n",
      "Epoch 185/200\n",
      "175104/176022 [============================>.] - ETA: 0s - loss: 1.7019 - accuracy: 0.4051\n",
      "Epoch 00185: val_loss improved from 1.64026 to 1.63863, saving model to CNN_dr0.5.h5\n",
      "176022/176022 [==============================] - 2s 11us/sample - loss: 1.7016 - accuracy: 0.4052 - val_loss: 1.6386 - val_accuracy: 0.4484\n",
      "epoch(185) lr is 0.0010000000474974513\n",
      "Epoch 186/200\n",
      "174080/176022 [============================>.] - ETA: 0s - loss: 1.6991 - accuracy: 0.4067\n",
      "Epoch 00186: val_loss improved from 1.63863 to 1.63710, saving model to CNN_dr0.5.h5\n",
      "176022/176022 [==============================] - 2s 11us/sample - loss: 1.6990 - accuracy: 0.4065 - val_loss: 1.6371 - val_accuracy: 0.4486\n",
      "epoch(186) lr is 0.0010000000474974513\n",
      "Epoch 187/200\n",
      "175104/176022 [============================>.] - ETA: 0s - loss: 1.6982 - accuracy: 0.4068\n",
      "Epoch 00187: val_loss improved from 1.63710 to 1.63567, saving model to CNN_dr0.5.h5\n",
      "176022/176022 [==============================] - 2s 11us/sample - loss: 1.6982 - accuracy: 0.4068 - val_loss: 1.6357 - val_accuracy: 0.4491\n",
      "epoch(187) lr is 0.0010000000474974513\n",
      "Epoch 188/200\n",
      "175104/176022 [============================>.] - ETA: 0s - loss: 1.6970 - accuracy: 0.4067\n",
      "Epoch 00188: val_loss improved from 1.63567 to 1.63428, saving model to CNN_dr0.5.h5\n",
      "176022/176022 [==============================] - 2s 11us/sample - loss: 1.6970 - accuracy: 0.4066 - val_loss: 1.6343 - val_accuracy: 0.4493\n",
      "epoch(188) lr is 0.0010000000474974513\n",
      "Epoch 189/200\n",
      "175104/176022 [============================>.] - ETA: 0s - loss: 1.6960 - accuracy: 0.4074\n",
      "Epoch 00189: val_loss improved from 1.63428 to 1.63278, saving model to CNN_dr0.5.h5\n",
      "176022/176022 [==============================] - 2s 11us/sample - loss: 1.6959 - accuracy: 0.4076 - val_loss: 1.6328 - val_accuracy: 0.4493\n",
      "epoch(189) lr is 0.0010000000474974513\n",
      "Epoch 190/200\n",
      "171008/176022 [============================>.] - ETA: 0s - loss: 1.6933 - accuracy: 0.4080\n",
      "Epoch 00190: val_loss improved from 1.63278 to 1.63131, saving model to CNN_dr0.5.h5\n",
      "176022/176022 [==============================] - 2s 11us/sample - loss: 1.6928 - accuracy: 0.4084 - val_loss: 1.6313 - val_accuracy: 0.4503\n",
      "epoch(190) lr is 0.0010000000474974513\n",
      "Epoch 191/200\n",
      "175104/176022 [============================>.] - ETA: 0s - loss: 1.6913 - accuracy: 0.4070\n",
      "Epoch 00191: val_loss improved from 1.63131 to 1.62988, saving model to CNN_dr0.5.h5\n",
      "176022/176022 [==============================] - 2s 11us/sample - loss: 1.6912 - accuracy: 0.4071 - val_loss: 1.6299 - val_accuracy: 0.4501\n",
      "epoch(191) lr is 0.0010000000474974513\n",
      "Epoch 192/200\n",
      "175104/176022 [============================>.] - ETA: 0s - loss: 1.6920 - accuracy: 0.4091\n",
      "Epoch 00192: val_loss improved from 1.62988 to 1.62845, saving model to CNN_dr0.5.h5\n",
      "176022/176022 [==============================] - 2s 11us/sample - loss: 1.6917 - accuracy: 0.4092 - val_loss: 1.6284 - val_accuracy: 0.4499\n",
      "epoch(192) lr is 0.0010000000474974513\n",
      "Epoch 193/200\n",
      "175104/176022 [============================>.] - ETA: 0s - loss: 1.6900 - accuracy: 0.4090\n",
      "Epoch 00193: val_loss improved from 1.62845 to 1.62703, saving model to CNN_dr0.5.h5\n",
      "176022/176022 [==============================] - 2s 11us/sample - loss: 1.6900 - accuracy: 0.4092 - val_loss: 1.6270 - val_accuracy: 0.4501\n",
      "epoch(193) lr is 0.0010000000474974513\n",
      "Epoch 194/200\n",
      "173056/176022 [============================>.] - ETA: 0s - loss: 1.6887 - accuracy: 0.4102\n",
      "Epoch 00194: val_loss improved from 1.62703 to 1.62570, saving model to CNN_dr0.5.h5\n",
      "176022/176022 [==============================] - 2s 11us/sample - loss: 1.6884 - accuracy: 0.4102 - val_loss: 1.6257 - val_accuracy: 0.4506\n",
      "epoch(194) lr is 0.0010000000474974513\n",
      "Epoch 195/200\n",
      "175104/176022 [============================>.] - ETA: 0s - loss: 1.6880 - accuracy: 0.4091\n",
      "Epoch 00195: val_loss improved from 1.62570 to 1.62443, saving model to CNN_dr0.5.h5\n",
      "176022/176022 [==============================] - 2s 11us/sample - loss: 1.6879 - accuracy: 0.4092 - val_loss: 1.6244 - val_accuracy: 0.4505\n",
      "epoch(195) lr is 0.0010000000474974513\n",
      "Epoch 196/200\n",
      "175104/176022 [============================>.] - ETA: 0s - loss: 1.6868 - accuracy: 0.4084\n",
      "Epoch 00196: val_loss improved from 1.62443 to 1.62300, saving model to CNN_dr0.5.h5\n",
      "176022/176022 [==============================] - 2s 11us/sample - loss: 1.6868 - accuracy: 0.4084 - val_loss: 1.6230 - val_accuracy: 0.4513\n",
      "epoch(196) lr is 0.0010000000474974513\n",
      "Epoch 197/200\n",
      "175104/176022 [============================>.] - ETA: 0s - loss: 1.6838 - accuracy: 0.4101\n",
      "Epoch 00197: val_loss improved from 1.62300 to 1.62164, saving model to CNN_dr0.5.h5\n",
      "176022/176022 [==============================] - 2s 11us/sample - loss: 1.6838 - accuracy: 0.4101 - val_loss: 1.6216 - val_accuracy: 0.4514\n",
      "epoch(197) lr is 0.0010000000474974513\n",
      "Epoch 198/200\n",
      "175104/176022 [============================>.] - ETA: 0s - loss: 1.6817 - accuracy: 0.4101\n",
      "Epoch 00198: val_loss improved from 1.62164 to 1.62035, saving model to CNN_dr0.5.h5\n",
      "176022/176022 [==============================] - 2s 11us/sample - loss: 1.6819 - accuracy: 0.4100 - val_loss: 1.6204 - val_accuracy: 0.4515\n",
      "epoch(198) lr is 0.0010000000474974513\n",
      "Epoch 199/200\n",
      "175104/176022 [============================>.] - ETA: 0s - loss: 1.6815 - accuracy: 0.4104\n",
      "Epoch 00199: val_loss improved from 1.62035 to 1.61907, saving model to CNN_dr0.5.h5\n",
      "176022/176022 [==============================] - 2s 11us/sample - loss: 1.6816 - accuracy: 0.4103 - val_loss: 1.6191 - val_accuracy: 0.4519\n",
      "epoch(199) lr is 0.0010000000474974513\n",
      "Epoch 200/200\n",
      "175104/176022 [============================>.] - ETA: 0s - loss: 1.6786 - accuracy: 0.4118\n",
      "Epoch 00200: val_loss improved from 1.61907 to 1.61774, saving model to CNN_dr0.5.h5\n",
      "176022/176022 [==============================] - 2s 11us/sample - loss: 1.6784 - accuracy: 0.4119 - val_loss: 1.6177 - val_accuracy: 0.4516\n"
     ]
    }
   ],
   "source": [
    "# model training\n",
    "# Set up some params \n",
    "nb_epoch = 200   # number of epochs to train on\n",
    "batch_size = 1024  # training batch size\n",
    "\n",
    "def scheduler(epoch):\n",
    "    print(\"epoch({}) lr is {}\".format(epoch, K.get_value(model.optimizer.lr)))\n",
    "    return K.get_value(model.optimizer.lr)\n",
    "\n",
    "reduce_lr = LearningRateScheduler(scheduler)\n",
    "\n",
    "filepath = 'CNN_dr0.5.h5'\n",
    "history = model.fit(x_train,\n",
    "    y_train,\n",
    "    batch_size=batch_size,\n",
    "    epochs=nb_epoch,\n",
    "    verbose=1,\n",
    "    validation_data=(x_val,y_val),\n",
    "    callbacks = [reduce_lr,\n",
    "                keras.callbacks.ModelCheckpoint(filepath, monitor='val_loss', verbose=1, save_best_only=True, mode='auto'),\n",
    "                keras.callbacks.ReduceLROnPlateau(monitor='val_loss',factor=0.5,verbose=1,patince=3,min_lr=0.000001),\n",
    "                keras.callbacks.EarlyStopping(monitor='val_loss', patience=10, verbose=1, mode='auto')\n",
    "                ]\n",
    "                    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Show loss curves \n",
    "plt.figure()\n",
    "plt.title('CLDNN Training performance')\n",
    "plt.plot(history.epoch, history.history['loss'], label='train loss+error')\n",
    "plt.plot(history.epoch, history.history['val_loss'], label='val_error')\n",
    "plt.legend()\n",
    "plt.savefig('pic\\Loss_Curve\\AP_CLDNN_LOSS.jpg',dpi = 300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_confusion_matrix(cm, title='AP-based Confusion matrix', cmap=plt.cm.Blues, labels=[]):\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(len(labels))\n",
    "    plt.xticks(tick_marks, labels, rotation=45)\n",
    "    plt.yticks(tick_marks, labels)\n",
    "    plt.tight_layout()\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plot confusion matrix\n",
    "classes = mods\n",
    "test_Y_hat = model.predict(x_test, batch_size=batch_size)\n",
    "conf = np.zeros([len(classes),len(classes)])\n",
    "confnorm = np.zeros([len(classes),len(classes)])\n",
    "for i in range(0,x_test.shape[0]):\n",
    "    j = list(Y_test[i,:]).index(1)\n",
    "    k = int(np.argmax(test_Y_hat[i,:]))\n",
    "    conf[j,k] = conf[j,k] + 1\n",
    "for i in range(0,len(classes)):\n",
    "    confnorm[i,:] = conf[i,:] / np.sum(conf[i,:])\n",
    "plot_confusion_matrix(confnorm, labels=classes)\n",
    "plt.savefig('pic\\Confusion_Matrix\\AP_CM_CLDNN.jpg',dpi = 300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0998185117967332\n",
      "0.09435707678075855\n",
      "0.10603290676416818\n",
      "0.06835205992509363\n",
      "0.12302284710017575\n",
      "0.08602150537634409\n",
      "0.11707317073170732\n",
      "0.18588025022341376\n",
      "0.33524904214559387\n",
      "0.46944198405668736\n",
      "0.5547576301615799\n",
      "0.6449864498644986\n",
      "0.7399103139013453\n",
      "0.7619877942458587\n",
      "0.7889990982867449\n",
      "0.7926829268292683\n",
      "0.8051146384479718\n",
      "0.800185873605948\n",
      "0.773308957952468\n",
      "0.7959001782531194\n"
     ]
    }
   ],
   "source": [
    "# model0:原模型测试\n",
    "classes = mods\n",
    "acc = {}\n",
    "for snr in snrs:\n",
    "\n",
    "    # extract classes @ SNR\n",
    "    test_SNRs = list(map(lambda x: lbl[x][1], test_idx))\n",
    "    test_X_i = x_test[np.where(np.array(test_SNRs)==snr)]\n",
    "    test_Y_i = Y_test[np.where(np.array(test_SNRs)==snr)]   \n",
    "    #print(test_X_i)\n",
    "    test_Y_i_hat = model.predict(test_X_i)\n",
    "    # estimate classes\n",
    "    test_Y_i_hat = model.predict(test_X_i)\n",
    "    conf = np.zeros([len(classes),len(classes)])\n",
    "    confnorm = np.zeros([len(classes),len(classes)])\n",
    "    for i in range(0,test_X_i.shape[0]):\n",
    "        j = list(test_Y_i[i,:]).index(1)\n",
    "        k = int(np.argmax(test_Y_i_hat[i,:]))\n",
    "        conf[j,k] = conf[j,k] + 1\n",
    "    for i in range(0,len(classes)):\n",
    "        confnorm[i,:] = conf[i,:] / np.sum(conf[i,:])\n",
    "    #plt.figure()\n",
    "    #plot_confusion_matrix(confnorm, labels=classes, title=\"ConvNet Confusion Matrix (SNR=%d)\"%(snr))\n",
    "    \n",
    "    cor = np.sum(np.diag(conf))\n",
    "    ncor = np.sum(conf) - cor\n",
    "    #print(\"Overall Accuracy: \", cor / (cor+ncor))\n",
    "    print(cor / (cor+ncor))\n",
    "    acc[snr] = 1.0*cor/(cor+ncor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(\"model_saved\\Feat_based\\CNN_time_200.h5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RBFNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = x_train.reshape(x_train.shape[0],x_train.shape[1])\n",
    "x_val = x_val.reshape(x_val.shape[0],x_val.shape[1])\n",
    "x_test = x_test.reshape(x_test.shape[0],x_test.shape[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((176022, 152), (22000, 152), (21978, 152), (176022, 11), (50, 3), (21978, 11))"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train.shape,x_test.shape,x_val.shape,y_train.shape,y_test.shape,y_val.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "rbflayer.RBFLayer"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import rbflayer, kmeans_initializer\n",
    "rbflayer.RBFLayer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install keras\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.layers import Layer\n",
    "from tensorflow.keras import backend as K\n",
    "from keras import layers\n",
    "\n",
    "class RBFLayer(Layer):\n",
    "    def __init__(self, units, gamma, **kwargs):\n",
    "        super(RBFLayer, self).__init__(**kwargs)\n",
    "        self.units = units\n",
    "        self.gamma = K.cast_to_floatx(gamma)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        self.mu = self.add_weight(name='mu',\n",
    "                                  shape=(int(input_shape[1]), self.units),\n",
    "                                  initializer='uniform',\n",
    "                                  trainable=True)\n",
    "        super(RBFLayer, self).build(input_shape)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        diff = K.expand_dims(inputs) - self.mu\n",
    "        l2 = K.sum(K.pow(diff, 4), axis=1)\n",
    "        res = K.exp(-1 * self.gamma * l2)\n",
    "        return res\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return (input_shape[0], self.units)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# history_rbf = []\n",
    "# dr = 0.5\n",
    "\n",
    "# RBF = rbflayer.RBFLayer(100,\n",
    "#                       initializer=kmeans_initializer.InitCentersKMeans(x_train),\n",
    "#                       #initializer=InitCentersRandom(X_smo),\n",
    "#                       betas=5.0,\n",
    "#                       input_shape=([x_train.shape[1]]))\n",
    "\n",
    "# model = keras.models.Sequential()\n",
    "# model.add(RBF)\n",
    "# model.add(layers.Dense(128,activation='selu',name='fc1'))\n",
    "# model.add(Dropout(dr))\n",
    "# model.add(layers.Dense(128,activation='selu',name='fc2'))\n",
    "# model.add(Dropout(dr))\n",
    "# model.add(layers.Dense(11,activation = 'softmax'))\n",
    "\n",
    "# # model compile\n",
    "# model.compile(loss=keras.losses.categorical_crossentropy,\n",
    "#               optimizer=keras.optimizers.Adadelta(),\n",
    "#               metrics=['accuracy'])\n",
    "# model.summary()\n",
    "\n",
    "# filepath = 'RBF_dr0.5.h5'\n",
    "# history = model.fit(x_train, y_train,\n",
    "#                    epochs = 100,\n",
    "#                    batch_size = 1024,\n",
    "#                    validation_data = (x_val,y_val),\n",
    "#                    callbacks = [reduce_lr, \n",
    "#                                 keras.callbacks.ModelCheckpoint(filepath, monitor='val_loss', verbose=1, save_best_only=True, mode='auto'),\n",
    "#                                 keras.callbacks.ReduceLROnPlateau(monitor='val_loss',factor=0.5,verbose=1,patince=3,min_lr=0.000001),\n",
    "#                                 keras.callbacks.EarlyStopping(monitor='val_loss', patience=10, verbose=1, mode='auto')\n",
    "#                                ]\n",
    "#                    )\n",
    "\n",
    "# history_rbf.append(history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:AMC] *",
   "language": "python",
   "name": "conda-env-AMC-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
